%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}

\PassOptionsToPackage{booktabs}{sphinx}
\PassOptionsToPackage{colorrows}{sphinx}

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{tgtermes}
\usepackage{tgheros}
\renewcommand{\ttdefault}{txtt}



\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=auto}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}



\title{Ragflow}
\date{Jan 13, 2026}
\release{1}
\author{Jansen Jan}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\ifdefined\shorthandoff
  \ifnum\catcode`\=\string=\active\shorthandoff{=}\fi
  \ifnum\catcode`\"=\active\shorthandoff{"}\fi
\fi

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


\sphinxstepscope


\chapter{About RAGFlow: Named Among GitHub’s Fastest\sphinxhyphen{}Growing Open Source Projects}
\label{\detokenize{about:about-ragflow-named-among-githubs-fastest-growing-open-source-projects}}\label{\detokenize{about:about-ragflow-octoverse}}\label{\detokenize{about::doc}}
\sphinxAtStartPar
\sphinxstylestrong{October 28, 2025 · 3 min read}

\sphinxAtStartPar
The release of \sphinxstylestrong{GitHub’s 2025 Octoverse report} marks a pivotal moment for the open source ecosystem—and for projects like \sphinxstylestrong{RAGFlow}, which has emerged as \sphinxstylestrong{one of the fastest\sphinxhyphen{}growing open source projects by contributors this year}.

\sphinxAtStartPar
With a \sphinxstylestrong{remarkable 2,596\% year\sphinxhyphen{}over\sphinxhyphen{}year growth in contributor engagement}, RAGFlow isn’t just gaining traction—\sphinxstylestrong{it’s defining the next wave of AI\sphinxhyphen{}powered development}.

\sphinxAtStartPar
—


\section{The Rise of Retrieval\sphinxhyphen{}Augmented Generation in Production}
\label{\detokenize{about:the-rise-of-retrieval-augmented-generation-in-production}}
\sphinxAtStartPar
As the Octoverse report highlights, \sphinxstylestrong{AI is no longer experimental—it’s foundational}.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{4.3 million+ AI\sphinxhyphen{}related repositories} on GitHub

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{1.1 million+ public repos} import LLM SDKs — a \sphinxstylestrong{178\% YoY increase}

\end{itemize}

\sphinxAtStartPar
In this context, \sphinxstylestrong{RAGFlow’s rapid adoption signals a clear shift}: developers are moving \sphinxstylestrong{beyond prototyping} and into \sphinxstylestrong{production\sphinxhyphen{}grade AI workflows}.

\sphinxAtStartPar
\sphinxstylestrong{RAGFlow}—an \sphinxstylestrong{end\sphinxhyphen{}to\sphinxhyphen{}end retrieval\sphinxhyphen{}augmented generation engine with built\sphinxhyphen{}in agent capabilities}—is perfectly positioned to meet this demand. It enables developers to build \sphinxstylestrong{scalable, context\sphinxhyphen{}aware AI applications} that are both \sphinxstylestrong{powerful and practical}.

\sphinxAtStartPar
\textgreater{} As the report notes:
\textgreater{} \sphinxstyleemphasis{“AI infrastructure is emerging as a major magnet” for open source contributions.}
\textgreater{} — \sphinxstylestrong{RAGFlow sits squarely at the intersection of AI infrastructure and real\sphinxhyphen{}world usability.}

\sphinxAtStartPar
—


\section{Why RAGFlow Resonates in the AI Era}
\label{\detokenize{about:why-ragflow-resonates-in-the-ai-era}}
\sphinxAtStartPar
Several trends highlighted in the Octoverse report \sphinxstylestrong{align closely} with RAGFlow’s design and mission:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{From Notebooks to Production}
\sphinxhyphen{} Jupyter Notebooks: \sphinxstylestrong{+75\% YoY}
\sphinxhyphen{} Python codebases: \sphinxstylestrong{surging}
\sphinxhyphen{} \sphinxstylestrong{RAGFlow supports this transition} with a \sphinxstylestrong{structured, reproducible framework} for deploying RAG systems in production.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Agentic Workflows Are Going Mainstream}
\sphinxhyphen{} GitHub Copilot coding agent launch
\sphinxhyphen{} Rise of AI\sphinxhyphen{}assisted development
\sphinxhyphen{} \sphinxstylestrong{RAGFlow’s built\sphinxhyphen{}in agent capabilities} automate \sphinxstylestrong{retrieval, reasoning, and response generation}—key components of modern AI apps.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Security and Scalability Are Top of Mind}
\sphinxhyphen{} \sphinxstylestrong{172\% YoY increase} in Broken Access Control vulnerabilities
\sphinxhyphen{} \sphinxstylestrong{RAGFlow’s enterprise\sphinxhyphen{}ready deployment} helps teams address these challenges \sphinxstylestrong{secure\sphinxhyphen{}by\sphinxhyphen{}design}

\end{enumerate}

\sphinxAtStartPar
—


\section{A Project in Active Development}
\label{\detokenize{about:a-project-in-active-development}}
\sphinxAtStartPar
RAGFlow’s evolution mirrors a \sphinxstylestrong{deliberate journey}—from solving foundational RAG challenges to \sphinxstylestrong{shaping the next generation of enterprise AI infrastructure}.

\sphinxAtStartPar
\#\#\# Phase 1: Solving Core RAG Limitations
RAGFlow first made its mark by \sphinxstylestrong{systematically addressing core RAG limitations} through integrated technological innovation:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Deep document understanding} for parsing complex formats (PDFs, tables, forms)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hybrid retrieval} blending multiple search strategies (vector, keyword, graph)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Built\sphinxhyphen{}in advanced tools}: \sphinxstylestrong{GraphRAG}, \sphinxstylestrong{RAPTOR}, and more

\item {} 
\sphinxAtStartPar
Result: \sphinxstylestrong{dramatically enhanced retrieval accuracy and reasoning performance}

\end{itemize}

\sphinxAtStartPar
\#\#\# Phase 2: The Superior Context Engine for Enterprise Agents
Now, building on this robust technical foundation, \sphinxstylestrong{RAGFlow is steering toward a bolder vision}:

\sphinxAtStartPar
\textgreater{} \sphinxstylestrong{To become the superior context engine for enterprise\sphinxhyphen{}grade Agents.}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Evolving from a \sphinxstylestrong{specialized RAG engine} into a \sphinxstylestrong{unified, resilient context layer}

\item {} 
\sphinxAtStartPar
Positioning itself as the \sphinxstylestrong{essential data foundation for LLMs in the enterprise}

\item {} 
\sphinxAtStartPar
Enabling \sphinxstylestrong{Agents of any kind} to access \sphinxstylestrong{rich, precise, and secure context}

\item {} 
\sphinxAtStartPar
Ensuring \sphinxstylestrong{reliable and effective operation across all tasks}

\end{itemize}

\sphinxAtStartPar
—


\section{Conclusion}
\label{\detokenize{about:conclusion}}
\sphinxAtStartPar
RAGFlow’s \sphinxstylestrong{explosive growth} in the 2025 Octoverse is not a coincidence.

\sphinxAtStartPar
It reflects a \sphinxstylestrong{global developer movement} toward \sphinxstylestrong{production\sphinxhyphen{}ready, agentic, secure AI systems}—and RAGFlow is \sphinxstylestrong{leading the charge}.

\sphinxAtStartPar
From \sphinxstylestrong{deep document parsing} to \sphinxstylestrong{scalable agent workflows}, RAGFlow delivers the \sphinxstylestrong{infrastructure} and \sphinxstylestrong{usability} that modern AI demands.

\sphinxAtStartPar
\sphinxstylestrong{The future of enterprise AI is context\sphinxhyphen{}aware, agent\sphinxhyphen{}driven, and open source—and RAGFlow is building it.}

\sphinxstepscope


\chapter{Security Concerns}
\label{\detokenize{security-concerns:security-concerns}}\label{\detokenize{security-concerns::doc}}
\sphinxAtStartPar
The origin of ragflow is Chinese. Infiniflow is a Chinese company.
It was open\sphinxhyphen{}sourced in 2024.

\sphinxAtStartPar
Chinese origin of RAGFlow is not a material security concern in practice — the project is one of the technically strongest deep\sphinxhyphen{}document\sphinxhyphen{}understanding RAG engines available right now.For organizations that must comply with very strict procurement rules, national security guidelines, or have board\sphinxhyphen{}level “no Chinese tech” policies \(\rightarrow\) yes, it’s frequently considered a blocking factor, even if the technical risk is low.Choose according to your actual risk appetite and compliance requirements — not just origin country.


\section{considerations:}
\label{\detokenize{security-concerns:considerations}}
\sphinxAtStartPar
to mitigate risc :
\(\rightarrow\) Self\sphinxhyphen{}host + build your own Docker image from the official source code
\(\rightarrow\) Audit the diff yourself or let your security team do it (very feasible — the codebase is not enormous)
\(\rightarrow\) Use only Western/local/open\sphinxhyphen{}source LLMs \& embedding models
\(\rightarrow\) Result: concern level drops to same as any other active open\sphinxhyphen{}source project


\section{sandbox:}
\label{\detokenize{security-concerns:sandbox}}
\sphinxAtStartPar
RAGFlow is an excellent open\sphinxhyphen{}source deep\sphinxhyphen{}document\sphinxhyphen{}understanding + Agent\sphinxhyphen{}oriented RAG engine.It contains a very powerful feature: Code component inside Agents. Users can write Python or JavaScript code directly in the workflow/agent \(\rightarrow\) this code can:Process retrieved chunks
Call external APIs
Do calculations / data cleaning
Transform data
Call other tools dynamically basically anything Python/JS can do

\sphinxAtStartPar
\(\rightarrow\) This is extremely powerful, but also extremely dangerous if you let random users (or even semi\sphinxhyphen{}trusted internal users) write code.

\sphinxAtStartPar
That’s where RAGFlow Sandbox + gVisor comes in

\sphinxAtStartPar
This can be enabled in “.env”.
\begin{description}
\sphinxlineitem{User \(\rightarrow\) Agent workflow \(\rightarrow\) Code component (Python/JS)}\begin{quote}
\begin{quote}
\begin{quote}

\sphinxAtStartPar
↓
\end{quote}
\begin{description}
\sphinxlineitem{RAGFlow Sandbox Executor Manager}
\sphinxAtStartPar
↓

\end{description}
\end{quote}
\begin{description}
\sphinxlineitem{Spawns short\sphinxhyphen{}lived container(s) using}\begin{description}
\sphinxlineitem{runtime: runsc (gVisor)}
\sphinxAtStartPar
↓

\end{description}

\end{description}
\end{quote}

\sphinxAtStartPar
Code runs inside very strong syscall sandbox

\end{description}


\section{elasticsearch:}
\label{\detokenize{security-concerns:elasticsearch}}\begin{itemize}
\item {} 
\sphinxAtStartPar
xpack.security.enabled: true

\item {} 
\sphinxAtStartPar
Transport TLS enabled + working (verification\_mode: certificate is ok)

\item {} 
\sphinxAtStartPar
HTTP TLS enabled (HTTPS) on all nodes that receive traffic

\item {} 
\sphinxAtStartPar
Strong passwords set for all built\sphinxhyphen{}in users (elastic, kibana\_system, logstash\_system, beats\_system, apm\_system, …)

\item {} 
\sphinxAtStartPar
Dedicated roles + users for each service (never use elastic superuser in production apps)

\item {} 
\sphinxAtStartPar
Keystore used for all passwords (not plaintext in yml!)

\item {} 
\sphinxAtStartPar
Firewall: 9300 only between nodes, 9200 only from trusted sources

\item {} 
\sphinxAtStartPar
Regular certificate rotation plan (at least yearly, better automated)

\item {} 
\sphinxAtStartPar
Monitoring of certificate expiration

\item {} 
\sphinxAtStartPar
Audit logging enabled (at least for authentication/security events)

\end{itemize}

\sphinxstepscope


\chapter{RAGFlow System Architecture}
\label{\detokenize{architecture:ragflow-system-architecture}}\label{\detokenize{architecture::doc}}

\section{Overview}
\label{\detokenize{architecture:overview}}
\sphinxAtStartPar
RAGFlow is a \sphinxstylestrong{fully containerized}, micro\sphinxhyphen{}service\sphinxhyphen{}based RAG (Retrieval\sphinxhyphen{}Augmented Generation) engine.
When you run the official \sphinxcode{\sphinxupquote{docker\sphinxhyphen{}compose.yml}} (or \sphinxcode{\sphinxupquote{docker\sphinxhyphen{}compose\sphinxhyphen{}gpu.yml}}), the following Docker containers are launched automatically:


\section{Core Infrastructure Containers}
\label{\detokenize{architecture:core-infrastructure-containers}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabular}[t]{\X{30}{100}\X{70}{100}}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Container
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Responsibility
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
\sphinxstylestrong{mysql}
&
\sphinxAtStartPar
Persistent storage for users, knowledge bases, datasets, conversation history, API keys, etc.
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{redis}
&
\sphinxAtStartPar
Cache layer + task queue (Celery) for asynchronous jobs (document parsing, chunking, embedding)
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{elasticsearch}
&
\sphinxAtStartPar
Hybrid search engine: stores text chunks, keyword indices, and (optionally) dense vectors
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{minio}
&
\sphinxAtStartPar
S3\sphinxhyphen{}compatible object storage for original uploaded files (PDFs, Word, images, etc.)
\\
\sphinxbottomrule
\end{tabular}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}


\section{RAGFlow Application Containers}
\label{\detokenize{architecture:ragflow-application-containers}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabular}[t]{\X{30}{100}\X{70}{100}}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Container
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Responsibility
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
\sphinxstylestrong{ragflow\sphinxhyphen{}server} (a.k.a. ragflow\sphinxhyphen{}api)
&
\sphinxAtStartPar
Main FastAPI backend. Exposes all REST endpoints (/api/v1/…)
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{ragflow\sphinxhyphen{}web}
&
\sphinxAtStartPar
Frontend (React + Ant Design) \textendash{} the chat \& knowledge\sphinxhyphen{}base management UI
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{celery\sphinxhyphen{}worker} (often named ragflow\sphinxhyphen{}worker)
&
\sphinxAtStartPar
Background workers that execute long\sphinxhyphen{}running tasks (DeepDoc parsing, embedding generation, indexing)
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{nginx} (optional in some setups)
&
\sphinxAtStartPar
Reverse proxy / static file server (used in production deployments)
\\
\sphinxbottomrule
\end{tabular}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}


\section{High\sphinxhyphen{}Level Architecture Diagram}
\label{\detokenize{architecture:high-level-architecture-diagram}}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=1.000\linewidth]{{system-architecture}.png}
\caption{RAGFlow complete system architecture (as of v0.19+, 2025)}\label{\detokenize{architecture:id1}}\end{figure}


\section{Data \& Execution Flow}
\label{\detokenize{architecture:data-execution-flow}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{User uploads a document} \(\rightarrow\) stored in MinIO \(\rightarrow\) metadata saved in MySQL \(\rightarrow\) parsing task queued in Redis.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Celery worker} picks the task \(\rightarrow\) runs DeepDoc (layout detection, OCR, table extraction) \(\rightarrow\) splits into chunks.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Chunks} are sent to the configured embedding model (Ollama, vLLM, OpenAI, etc.) \(\rightarrow\) vectors returned.

\end{enumerate}

\sphinxstepscope


\chapter{From RAG to Context \sphinxhyphen{} A 2025 Year\sphinxhyphen{}End Review of RAG}
\label{\detokenize{blog-dec-2025:from-rag-to-context-a-2025-year-end-review-of-rag}}\label{\detokenize{blog-dec-2025::doc}}
\sphinxAtStartPar
\sphinxurl{https://ragflow.io/blog}. On the ragflow website appears frequently a very insightful blog!

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Date:} December 22, 2025
\item[] \sphinxstylestrong{Source:} RAGFlow Blog
\end{DUlineblock}

\sphinxAtStartPar
As 2025 draws to a close, the field of Retrieval\sphinxhyphen{}Augmented Generation
(RAG) has undergone profound reflection, vigorous debate, and marked
evolution. Far from fading into obsolescence as some bold predictions
foresaw—amid lingering scepticism over its supposedly transient role—RAG
has solidified its indispensability as a cornerstone of data
infrastructure in the demanding arena of enterprise AI adoption.

\sphinxAtStartPar
Looking back, RAG’s trajectory this year has been complex. On one hand,
its practical effectiveness faced significant skepticism, partly due to
the “easy to use, hard to master” tuning challenges inherent to RAG
systems. On the other hand, its share of public attention seemed to be
overshadowed by the undisputed focus of 2025’s LLM applications: \sphinxstylestrong{AI
Agents}.

\sphinxAtStartPar
However, an intriguing trend emerged. Despite the controversies and not
being in the spotlight, enterprises genuinely committed to building core
AI competencies—especially mid\sphinxhyphen{}to\sphinxhyphen{}large\sphinxhyphen{}sized organizations—deepened and
systematized their investments in RAG. Rather than being marginalized,
RAG has solidified its core role in enterprise AI architecture. Its
position as critical infrastructure remains unshaken, forming the robust
foundation for enterprise intelligence.

\sphinxAtStartPar
Therefore, we must first move beyond surface\sphinxhyphen{}level debates to examine
the intrinsic vitality of RAG technology. Is it merely a transitional
“band\sphinxhyphen{}aid” to patch LLM knowledge gaps, or is it an architecture capable
of continuous evolution into a cornerstone for next\sphinxhyphen{}generation AI
applications? To answer this, we must systematically review its
technical improvements, architectural evolution, and new role in the age
of Agents.


\section{Can RAG Still Be Improved?}
\label{\detokenize{blog-dec-2025:can-rag-still-be-improved}}

\subsection{The Debate About Long Context and RAG}
\label{\detokenize{blog-dec-2025:the-debate-about-long-context-and-rag}}
\sphinxAtStartPar
In 2025, the core of many RAG debates stems from a widely acknowledged
contradiction: enterprises feel they “cannot live without RAG, yet
remain unsatisfied.” RAG lowers the barrier to accessing private
knowledge, but achieving stable and accurate results—especially for
complex queries—often requires extensive, fine\sphinxhyphen{}tuned optimization,
complicating total cost of ownership assessments.

\sphinxAtStartPar
Consequently, the theoretical question heatedly discussed in 2024—“Can
Long Context replace RAG?”—rapidly entered practical testing in 2025.
Some scenarios less sensitive to latency and cost, with relatively fixed
query patterns (e.g., certain contract reviews, fixed\sphinxhyphen{}format report
analysis), began experimenting with directly using long\sphinxhyphen{}context windows.
They feed entire or large batches of relevant documents into the model
at once, hoping to bypass potential information loss or noise from RAG
retrieval and directly address inconsistent conversational quality.

\sphinxAtStartPar
However, research since 2024 offers a clearer picture of the technical
comparison. Mechanically stuffing lengthy text into an LLM’s context
window is essentially a “brute\sphinxhyphen{}force” strategy. It inevitably scatters
the model’s attention, significantly degrading answer quality through
the \sphinxstylestrong{“Lost in the Middle”} or \sphinxstylestrong{“information flooding”} effect. More
importantly, this approach incurs high costs—computational overhead for
processing long context grows non\sphinxhyphen{}linearly.

\sphinxAtStartPar
Thus, for enterprises, the practical question is not engaging in
simplistic debates like “RAG is dead,” but returning to the core
challenge: how to incorporate the most relevant and effective
information into the model’s context processing system with the best
cost\sphinxhyphen{}performance ratio. This is precisely the original design goal of
RAG technology.

\sphinxAtStartPar
Improved long\sphinxhyphen{}context capabilities have not signaled RAG’s demise.
Instead, they prompt deeper thinking about how the two can collaborate.
For example, RAG systems can use long\sphinxhyphen{}context windows to hold more
complete, semantically coherent retrieved chunks or to aggregate
intermediate results for multi\sphinxhyphen{}step retrieval and reflection. This
“retrieval\sphinxhyphen{}first, long\sphinxhyphen{}context containment” synergy is a key driver
behind the emerging field of \sphinxstylestrong{“Context Engineering.”} It marks a shift
from optimizing single “retrieval algorithms” to the systematic design
of the end\sphinxhyphen{}to\sphinxhyphen{}end “retrieval\sphinxhyphen{}context assembly\sphinxhyphen{}model reasoning” pipeline.

\sphinxAtStartPar
Currently, paradigms for providing external knowledge to LLMs mainly
fall into four categories:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Relying solely on LLM’s long\sphinxhyphen{}context capability.

\item {} 
\sphinxAtStartPar
Utilizing KV Cache.

\item {} 
\sphinxAtStartPar
Using simple search methods like Grep.

\item {} 
\sphinxAtStartPar
Employing a full RAG architecture.

\end{enumerate}

\sphinxAtStartPar
Cost\sphinxhyphen{}wise, there is roughly a \sphinxstylestrong{two\sphinxhyphen{}order\sphinxhyphen{}of\sphinxhyphen{}magnitude gap} between
option 1 and option 4. Option 2 (KV Cache based) remains at least an
order of magnitude more expensive than full RAG, while facing serious
limitations in scalability, real\sphinxhyphen{}time updates, and complex enterprise
scenarios.

\sphinxAtStartPar
Option 3 (index\sphinxhyphen{}free / Grep\sphinxhyphen{}style) works in very narrow, highly
structured domains (e.g. clean codebases or logs) but fails completely
for the majority of enterprise unstructured/multi\sphinxhyphen{}modal data.


\subsection{Optimizations for RAG Conversational Quality}
\label{\detokenize{blog-dec-2025:optimizations-for-rag-conversational-quality}}
\sphinxAtStartPar
A common source of inaccurate or unstable answers lies in a structural
conflict within the traditional “chunk\sphinxhyphen{}embed\sphinxhyphen{}retrieve” pipeline: using a
\sphinxstylestrong{single\sphinxhyphen{}granularity, fixed\sphinxhyphen{}size} text chunk to perform two inherently
conflicting tasks:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Semantic matching (recall)}: Smaller chunks (100\textendash{}256 tokens) \(\rightarrow\)
better precision

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Context understanding (utilization)}: Larger chunks (1024+ tokens)
\(\rightarrow\) better coherence

\end{itemize}

\sphinxAtStartPar
This forces a difficult trade\sphinxhyphen{}off.

\sphinxAtStartPar
A fundamental improvement is to \sphinxstylestrong{decouple} the process into two
stages:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Search} (scanning/locating): Use small, precise units for high
recall

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Retrieve} (reading/understanding): Dynamically assemble larger,
coherent context

\end{itemize}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{tree-rag}.png}
\end{figure}

\sphinxAtStartPar
\sphinxstylestrong{RAGFlow’s TreeRAG} technology embodies this:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Offline}: LLM builds hierarchical tree summaries (Chapter \(\rightarrow\)
Section \(\rightarrow\) Subsection \(\rightarrow\) Key Paragraph)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Online}: Precise small\sphinxhyphen{}chunk search \(\rightarrow\) use tree as navigation map \(\rightarrow\)
auto\sphinxhyphen{}expand to complete logical fragments

\end{itemize}

\sphinxAtStartPar
This mitigates \sphinxstylestrong{“Lost in the Middle”} and context fragmentation.

\sphinxAtStartPar
For even more complex queries (scattered info, cross\sphinxhyphen{}document reasoning)
\(\rightarrow\) \sphinxstylestrong{GraphRAG} (entity\sphinxhyphen{}relationship graphs) becomes relevant, though it
has challenges:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Massive token consumption during graph building

\item {} 
\sphinxAtStartPar
Noisy auto\sphinxhyphen{}extracted entities/relations

\item {} 
\sphinxAtStartPar
Fragmented knowledge output

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Hybrid TreeRAG + GraphRAG} (“Long\sphinxhyphen{}Context RAG”) approaches appear
most promising.

\sphinxAtStartPar
Modern RAG philosophy: Leverage LLMs during \sphinxstylestrong{ingestion} for deep
semantic enhancement (summaries, entities, metadata, potential
questions) \(\rightarrow\) use this as intelligent “navigation map” during
\sphinxstylestrong{retrieval} \(\rightarrow\) achieve optimal balance of effectiveness, performance
and cost.


\subsection{From Knowledge Base to Data Foundation}
\label{\detokenize{blog-dec-2025:from-knowledge-base-to-data-foundation}}
\sphinxAtStartPar
RAG is an architectural paradigm, not just a Q\&A tool.

\sphinxAtStartPar
With the rise of \sphinxstylestrong{AI Agents}, enterprise RAG is evolving into a
\sphinxstylestrong{general\sphinxhyphen{}purpose data foundation} for unstructured data — serving as
unified, efficient, secure access layer for all types of Agents.

\sphinxAtStartPar
A robust, scalable, configurable \sphinxstylestrong{Ingestion Pipeline} has become the
core of modern RAG engines, handling the full lifecycle from raw
documents to structured, semantically rich, query\sphinxhyphen{}ready knowledge.

\sphinxstepscope


\chapter{example:}
\label{\detokenize{rerank:example}}\label{\detokenize{rerank:ragflow-models}}\label{\detokenize{rerank::doc}}\begin{description}
\sphinxlineitem{curl \sphinxhyphen{}X POST \sphinxurl{http://localhost:8000/v1/rerank} }
\sphinxAtStartPar
\sphinxhyphen{}H “Content\sphinxhyphen{}Type: application/json” \sphinxhyphen{}d ‘\{
\begin{quote}

\sphinxAtStartPar
“model”: “uw\sphinxhyphen{}modelnaam.gguf”,
“query”: “Wat is het weer in Lokeren?”,
“documents”: {[}
\begin{quote}

\sphinxAtStartPar
“Het is vandaag zonnig in Brussel.”,
“Lokeren ligt in Oost\sphinxhyphen{}Vlaanderen.”,
“Het KNMI voorspelt regen voor morgen.”
\end{quote}

\sphinxAtStartPar
{]}
\end{quote}

\sphinxAtStartPar
\}’

\end{description}


\section{Why Infiniflow RAGFlow Uses a Reranker, an Embedding Model, and a Chat Model}
\label{\detokenize{rerank:why-infiniflow-ragflow-uses-a-reranker-an-embedding-model-and-a-chat-model}}
\sphinxAtStartPar
Infiniflow RAGFlow is a Retrieval\sphinxhyphen{}Augmented Generation (RAG) framework designed to build high\sphinxhyphen{}quality, traceable question\sphinxhyphen{}answering systems over complex data sources. To achieve accurate and contextually relevant responses, RAGFlow employs three distinct models that work in concert:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Embedding Model}
\sphinxhyphen{} \sphinxstylestrong{Purpose}: Converts both the user query and the chunks of retrieved documents into dense vector representations in the same semantic space.
\sphinxhyphen{} \sphinxstylestrong{Role in Pipeline}: Enables semantic similarity search during the retrieval phase. By computing cosine similarity (or other distance metrics) between the query embedding and document chunk embeddings, RAGFlow retrieves the most semantically relevant passages from a large corpus—far beyond keyword matching.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Reranker}
\sphinxhyphen{} \sphinxstylestrong{Purpose}: Refines the initial retrieval results by re\sphinxhyphen{}scoring the top\sphinxhyphen{}\sphinxstyleemphasis{k} candidate chunks using a cross\sphinxhyphen{}encoder architecture.
\sphinxhyphen{} \sphinxstylestrong{Role in Pipeline}: While the embedding model provides efficient approximate retrieval, the reranker applies a more computationally intensive but accurate relevance scoring. This step significantly improves precision by pushing the most contextually appropriate chunks to the top, reducing noise before generation.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{rerank}.png}
\caption{\sphinxstylestrong{Figure 1}: The reranker evaluates query\sphinxhyphen{}chunk pairs to produce fine\sphinxhyphen{}grained relevance scores.}\label{\detokenize{rerank:id1}}\end{figure}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Chat Model (LLM)}
\sphinxhyphen{} \sphinxstylestrong{Purpose}: Generates the final natural language response grounded in the refined retrieved context.
\sphinxhyphen{} \sphinxstylestrong{Role in Pipeline}: Takes the top reranked chunks as context and synthesizes a coherent, accurate, and fluent answer. The chat model (typically a large language model fine\sphinxhyphen{}tuned for instruction following) ensures the output is not only factually aligned with the source material but also conversational and user\sphinxhyphen{}friendly.

\end{enumerate}


\chapter{Synergy of the Three Models}
\label{\detokenize{rerank:synergy-of-the-three-models}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Embedding Model} \(\rightarrow\) Broad, fast, semantic retrieval

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Reranker} \(\rightarrow\) Precise, fine\sphinxhyphen{}grained reordering

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Chat Model} \(\rightarrow\) Coherent, grounded generation

\end{itemize}

\sphinxAtStartPar
This modular design allows RAGFlow to balance \sphinxstylestrong{speed}, \sphinxstylestrong{accuracy}, and \sphinxstylestrong{interpretability}, making it suitable for enterprise\sphinxhyphen{}grade RAG applications where both performance and trustworthiness are critical.

\sphinxstepscope


\chapter{Why vLLM is Used to Serve the Reranker Model}
\label{\detokenize{vllm:why-vllm-is-used-to-serve-the-reranker-model}}\label{\detokenize{vllm:vllm-reranker}}\label{\detokenize{vllm::doc}}
\sphinxAtStartPar
vLLM is a high\sphinxhyphen{}throughput, memory\sphinxhyphen{}efficient inference engine specifically designed for serving large language models (LLMs). In \sphinxstylestrong{Infiniflow RAGFlow}, the \sphinxstylestrong{reranker model}—responsible for fine\sphinxhyphen{}grained relevance scoring of retrieved document chunks—is served using \sphinxstylestrong{vLLM} to ensure low\sphinxhyphen{}latency, scalable, and production\sphinxhyphen{}ready performance.


\section{Key Reasons for Using vLLM to Serve the Reranker}
\label{\detokenize{vllm:key-reasons-for-using-vllm-to-serve-the-reranker}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{PagedAttention for Memory Efficiency}
\sphinxhyphen{} vLLM uses \sphinxstylestrong{PagedAttention}, a novel attention mechanism that manages KV cache in non\sphinxhyphen{}contiguous memory pages.
\sphinxhyphen{} This dramatically reduces memory fragmentation and enables \sphinxstylestrong{higher batch sizes} and \sphinxstylestrong{longer sequence lengths} (up to 8192 tokens in this case), critical for processing query\sphinxhyphen{}chunk pairs during reranking.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{High Throughput \& Low Latency}
\sphinxhyphen{} Supports \sphinxstylestrong{continuous batching}, allowing dynamic batch formation as requests arrive.
\sphinxhyphen{} Eliminates head\sphinxhyphen{}of\sphinxhyphen{}line blocking and maximizes GPU utilization—ideal for real\sphinxhyphen{}time reranking in interactive RAG pipelines.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{OpenAI\sphinxhyphen{}Compatible API}
\sphinxhyphen{} Exposes a clean, standardized REST API compatible with OpenAI’s format.
\sphinxhyphen{} Enables seamless integration with RAGFlow’s orchestration layer without custom inference code.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Support for Cross\sphinxhyphen{}Encoder Rerankers}
\sphinxhyphen{} Models like \sphinxstylestrong{Qwen3\sphinxhyphen{}Reranker\sphinxhyphen{}0.6B} are cross\sphinxhyphen{}encoders that take \sphinxcode{\sphinxupquote{{[}query, passage{]}}} pairs as input.
\sphinxhyphen{} vLLM efficiently handles the bidirectional attention required, delivering relevance scores via \sphinxcode{\sphinxupquote{logits{[}0{]}}} (typically for binary classification: relevant/irrelevant).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Ollama Does Not Support Reranker Models (Yet)}
\sphinxhyphen{} \sphinxstylestrong{Ollama} is excellent for local LLM inference and chat models, but \sphinxstylestrong{currently lacks native support for reranker (cross\sphinxhyphen{}encoder) models}.
\sphinxhyphen{} Rerankers require structured input formatting and logit extraction that Ollama’s current API and model loading system do not accommodate.
\sphinxhyphen{} vLLM, in contrast, supports any Hugging Face transformer model—including rerankers—with full access to outputs and fine\sphinxhyphen{}grained control.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Scalability Advantage Over Ollama}
\sphinxhyphen{} When scaling to \sphinxstylestrong{multiple concurrent users} or \sphinxstylestrong{high\sphinxhyphen{}throughput workloads}, vLLM is significantly more robust than Ollama.
\sphinxhyphen{} vLLM supports \sphinxstylestrong{distributed serving}, \sphinxstylestrong{tensor parallelism}, \sphinxstylestrong{GPU clustering}, and \sphinxstylestrong{dynamic batching at scale}.
\sphinxhyphen{} Ollama is primarily designed for \sphinxstylestrong{single\sphinxhyphen{}user, local development}, and does not scale efficiently in production environments.

\end{enumerate}


\section{Serving the Reranker Locally with vLLM}
\label{\detokenize{vllm:serving-the-reranker-locally-with-vllm}}
\sphinxAtStartPar
You can run the reranker model locally using vLLM with the following command:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
vllm\PYG{+w}{ }serve\PYG{+w}{ }/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}port\PYG{+w}{ }\PYG{l+m}{8123}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}max\PYGZhy{}model\PYGZhy{}len\PYG{+w}{ }\PYG{l+m}{8192}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}dtype\PYG{+w}{ }auto\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}trust\PYGZhy{}remote\PYGZhy{}code
\end{sphinxVerbatim}

\sphinxAtStartPar
Once running, the model is accessible via the OpenAI\sphinxhyphen{}compatible endpoint:

\sphinxAtStartPar
\sphinxstylestrong{GET} \sphinxcode{\sphinxupquote{http://localhost:8123/v1/models}}

\sphinxAtStartPar
\sphinxstylestrong{Example Response}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}object\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}list\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}data\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{[}
\PYG{+w}{    }\PYG{p}{\PYGZob{}}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}id\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}object\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}model\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}created\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{1762258164}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}owned\PYGZus{}by\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}vllm\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}root\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}parent\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{null}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}max\PYGZus{}model\PYGZus{}len\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{8192}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}permission\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{[}
\PYG{+w}{        }\PYG{p}{\PYGZob{}}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}id\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}modelperm\PYGZhy{}1a0d5938e30b4eeebb53d9e5c7d9599e\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}object\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}model\PYGZus{}permission\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}created\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{1762258164}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}allow\PYGZus{}create\PYGZus{}engine\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{false}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}allow\PYGZus{}sampling\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{true}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}allow\PYGZus{}logprobs\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{true}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}allow\PYGZus{}search\PYGZus{}indices\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{false}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}allow\PYGZus{}view\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{true}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}allow\PYGZus{}fine\PYGZus{}tuning\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{false}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}organization\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}*\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}group\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{null}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}is\PYGZus{}blocking\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{false}
\PYG{+w}{        }\PYG{p}{\PYGZcb{}}
\PYG{+w}{      }\PYG{p}{]}
\PYG{+w}{    }\PYG{p}{\PYGZcb{}}
\PYG{+w}{  }\PYG{p}{]}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\section{RAGFlow Integration}
\label{\detokenize{vllm:ragflow-integration}}
\sphinxAtStartPar
RAGFlow configures the reranker endpoint in its settings:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nt}{reranker}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{provider}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{vllm}
\PYG{+w}{  }\PYG{n+nt}{api\PYGZus{}base}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{http://localhost:8123/v1}
\PYG{+w}{  }\PYG{n+nt}{model}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b}
\end{sphinxVerbatim}

\sphinxAtStartPar
During inference, RAGFlow sends batched \sphinxcode{\sphinxupquote{{[}query, passage{]}}} pairs to the vLLM server, receives relevance scores, and reorders chunks before passing them to the chat model.

\sphinxAtStartPar
\sphinxstylestrong{Result}: Fast, accurate, and scalable reranking powered by optimized LLM inference—\sphinxstylestrong{where Ollama cannot currently follow, and where vLLM excels in both development and production.}

\sphinxstepscope


\chapter{Serving vLLM Reranker Using Docker (CPU\sphinxhyphen{}Only)}
\label{\detokenize{vllm-cpu:serving-vllm-reranker-using-docker-cpu-only}}\label{\detokenize{vllm-cpu:vllm-docker}}\label{\detokenize{vllm-cpu::doc}}
\sphinxAtStartPar
To ensure \sphinxstylestrong{reproducibility}, \sphinxstylestrong{portability}, and \sphinxstylestrong{isolation}, \sphinxstylestrong{vLLM} can be deployed using \sphinxstylestrong{Docker}. This is especially useful in environments with restricted internet access (e.g., corporate networks behind proxies or firewalls), where \sphinxstylestrong{Hugging Face Hub} may be blocked or rate\sphinxhyphen{}limited.

\sphinxAtStartPar
In this setup, \sphinxstylestrong{vLLM runs on CPU only} because:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Laptop has no GPU}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Home server has an old NVIDIA GPU} (not supported by vLLM’s CUDA requirements)

\end{itemize}

\sphinxAtStartPar
Thus, we use the \sphinxstylestrong{official CPU\sphinxhyphen{}optimized vLLM image} built from:
\sphinxurl{https://github.com/vllm-project/vllm/blob/main/docker/Dockerfile.cpu}

\sphinxAtStartPar
—


\section{Docker Compose Configuration (CPU Mode)}
\label{\detokenize{vllm-cpu:docker-compose-configuration-cpu-mode}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nt}{version}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{3.8}\PYG{l+s}{\PYGZsq{}}
\PYG{n+nt}{services}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{qwen\PYGZhy{}reranker}\PYG{p}{:}
\PYG{+w}{    }\PYG{n+nt}{image}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{vllm\PYGZhy{}cpu:latest}
\PYG{+w}{    }\PYG{n+nt}{ports}\PYG{p}{:}\PYG{+w}{ }\PYG{p+pIndicator}{[}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{8123:8000}\PYG{l+s}{\PYGZdq{}}\PYG{p+pIndicator}{]}
\PYG{+w}{    }\PYG{n+nt}{volumes}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{/home/naj/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b:/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b:ro}
\PYG{+w}{    }\PYG{n+nt}{environment}\PYG{p}{:}
\PYG{+w}{      }\PYG{n+nt}{VLLM\PYGZus{}HF\PYGZus{}OVERRIDES}\PYG{p}{:}\PYG{+w}{ }\PYG{p+pIndicator}{|}
\PYG{+w}{        }\PYG{n+no}{\PYGZob{}}
\PYG{+w}{          }\PYG{n+no}{\PYGZdq{}architectures\PYGZdq{}: [\PYGZdq{}Qwen3ForSequenceClassification\PYGZdq{}],}
\PYG{+w}{          }\PYG{n+no}{\PYGZdq{}classifier\PYGZus{}from\PYGZus{}token\PYGZdq{}: [\PYGZdq{}no\PYGZdq{}, \PYGZdq{}yes\PYGZdq{}],}
\PYG{+w}{          }\PYG{n+no}{\PYGZdq{}is\PYGZus{}original\PYGZus{}qwen3\PYGZus{}reranker\PYGZdq{}: true}
\PYG{+w}{        }\PYG{n+no}{\PYGZcb{}}
\PYG{+w}{    }\PYG{n+nt}{command}\PYG{p}{:}\PYG{+w}{ }\PYG{p+pIndicator}{\PYGZgt{}}
\PYG{+w}{      }\PYG{n+no}{/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}task score}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}dtype float32}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}port 8000}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}trust\PYGZhy{}remote\PYGZhy{}code}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}max\PYGZhy{}model\PYGZhy{}len 8192}
\PYG{+w}{    }\PYG{n+nt}{deploy}\PYG{p}{:}
\PYG{+w}{      }\PYG{n+nt}{resources}\PYG{p}{:}
\PYG{+w}{        }\PYG{n+nt}{limits}\PYG{p}{:}
\PYG{+w}{          }\PYG{n+nt}{cpus}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{10}\PYG{l+s}{\PYGZsq{}}
\PYG{+w}{          }\PYG{n+nt}{memory}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{16G}
\PYG{+w}{    }\PYG{n+nt}{shm\PYGZus{}size}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{4g}
\PYG{+w}{    }\PYG{n+nt}{restart}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{unless\PYGZhy{}stopped}
\end{sphinxVerbatim}

\sphinxAtStartPar
—


\section{Key Components Explained}
\label{\detokenize{vllm-cpu:key-components-explained}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{\textasciigrave{}\textasciigrave{}image: vllm\sphinxhyphen{}cpu:latest\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Official vLLM CPU image (no CUDA dependencies).
\sphinxhyphen{} Built from: \sphinxhref{https://github.com/vllm-project/vllm/blob/main/docker/Dockerfile.cpu}{vllm\sphinxhyphen{}project/vllm/docker/Dockerfile.cpu}
\sphinxhyphen{} Uses PyTorch CPU backend with optimized inference kernels.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{\textasciigrave{}\textasciigrave{}ports: {[}“8123:8000”{]}\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Host port \sphinxstylestrong{8123} \(\rightarrow\) container port \sphinxstylestrong{8000} (vLLM default).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{\textasciigrave{}\textasciigrave{}volumes\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Mounts \sphinxstylestrong{locally pre\sphinxhyphen{}downloaded model} in \sphinxstylestrong{read\sphinxhyphen{}only} mode.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{\textasciigrave{}\textasciigrave{}VLLM\_HF\_OVERRIDES\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Required for \sphinxstylestrong{Qwen3\sphinxhyphen{}Reranker} due to custom classification head and token handling.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{\textasciigrave{}\textasciigrave{}command\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} \sphinxcode{\sphinxupquote{\sphinxhyphen{}\sphinxhyphen{}task score}}: Enables reranker scoring (outputs relevance logits).
\sphinxhyphen{} \sphinxcode{\sphinxupquote{\sphinxhyphen{}\sphinxhyphen{}dtype float32}}: Mandatory on CPU (no half\sphinxhyphen{}precision support).
\sphinxhyphen{} \sphinxcode{\sphinxupquote{\sphinxhyphen{}\sphinxhyphen{}max\sphinxhyphen{}model\sphinxhyphen{}len 8192}}: Supports long query+passage pairs.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Resource Limits}
\sphinxhyphen{} \sphinxcode{\sphinxupquote{cpus: \textquotesingle{}10\textquotesingle{}}} and \sphinxcode{\sphinxupquote{memory: 16G}} prevent system overload.
\sphinxhyphen{} \sphinxcode{\sphinxupquote{shm\_size: 4g}} ensures sufficient shared memory for batched inference.

\end{itemize}

\sphinxAtStartPar
—


\section{Why the Model Must Be Pre\sphinxhyphen{}Downloaded Locally}
\label{\detokenize{vllm-cpu:why-the-model-must-be-pre-downloaded-locally}}
\sphinxAtStartPar
The container \sphinxstylestrong{cannot download the model at runtime} due to:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Corporate Proxy / Firewall}
\sphinxhyphen{} Outbound traffic to \sphinxcode{\sphinxupquote{huggingface.co}} is blocked or requires authentication.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hugging Face Hub Blocked}
\sphinxhyphen{} Git LFS and model downloads fail in restricted networks.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{vLLM Auto\sphinxhyphen{}Download Fails Offline}
\sphinxhyphen{} vLLM uses \sphinxcode{\sphinxupquote{transformers.AutoModel}} \(\rightarrow\) attempts online download if model not found.

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Solution: Download via mirror}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nv}{HF\PYGZus{}ENDPOINT}\PYG{o}{=}https://hf\PYGZhy{}mirror.com\PYG{+w}{ }huggingface\PYGZhy{}cli\PYG{+w}{ }download\PYG{+w}{ }Qwen/Qwen3\PYGZhy{}Reranker\PYGZhy{}0.6B\PYG{+w}{ }\PYGZhy{}\PYGZhy{}local\PYGZhy{}dir\PYG{+w}{ }./qwen3\PYGZhy{}reranker\PYGZhy{}0.6b
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Remark : for some models you need a token HF\_TOKEN=xxxxxxxx (you have to specify the model in the token definition!)

\item {} 
\sphinxAtStartPar
Remark2 : use “sudo” if non\sphinxhyphen{}root!!!

\item {} 
\sphinxAtStartPar
Uses \sphinxstylestrong{accessible mirror} (\sphinxcode{\sphinxupquote{hf\sphinxhyphen{}mirror.com}}).

\item {} 
\sphinxAtStartPar
Saves model locally for volume mounting.

\end{itemize}

\sphinxAtStartPar
—


\section{Why CPU\sphinxhyphen{}Only (No GPU)?}
\label{\detokenize{vllm-cpu:why-cpu-only-no-gpu}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Laptop}: Integrated graphics only (no discrete GPU).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Home Server}: NVIDIA GPU too old (e.g., pre\sphinxhyphen{}Ampere) \(\rightarrow\) \sphinxstylestrong{not supported} by vLLM’s CUDA 11.8+ / FlashAttention requirements.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{vLLM CPU image} enables full functionality without GPU.

\end{itemize}

\sphinxAtStartPar
\textgreater{} \sphinxstylestrong{Performance Note}: CPU inference is slower (\textasciitilde{}1\textendash{}3 sec per batch), but sufficient for \sphinxstylestrong{development}, \sphinxstylestrong{prototyping}, or \sphinxstylestrong{low\sphinxhyphen{}throughput} use cases.

\sphinxAtStartPar
—


\section{Start the Service}
\label{\detokenize{vllm-cpu:start-the-service}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker\PYGZhy{}compose\PYG{+w}{ }up\PYG{+w}{ }\PYGZhy{}d
\end{sphinxVerbatim}


\section{Verify Availability}
\label{\detokenize{vllm-cpu:verify-availability}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
curl\PYG{+w}{ }http://localhost:8123/v1/models
\end{sphinxVerbatim}

\sphinxAtStartPar
Expected output confirms the model is loaded and ready.

\sphinxAtStartPar
—


\section{Integration with RAGFlow}
\label{\detokenize{vllm-cpu:integration-with-ragflow}}
\sphinxAtStartPar
Update RAGFlow config:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nt}{reranker}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{provider}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{vllm}
\PYG{+w}{  }\PYG{n+nt}{api\PYGZus{}base}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{http://localhost:8123/v1}
\PYG{+w}{  }\PYG{n+nt}{model}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b}
\end{sphinxVerbatim}

\sphinxAtStartPar
—


\section{Benefits of This CPU + Docker Setup}
\label{\detokenize{vllm-cpu:benefits-of-this-cpu-docker-setup}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Works on any machine} (laptop, old server, air\sphinxhyphen{}gapped systems)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{No GPU required}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Offline\sphinxhyphen{}first} with pre\sphinxhyphen{}downloaded model

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Consistent environment} via Docker

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Secure}: read\sphinxhyphen{}only model, isolated container

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Scalable later}: switch to GPU image when hardware upgrades

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Ideal for local RAGFlow development and constrained production environments.}

\sphinxstepscope


\chapter{Integrating vLLM with RAGFlow via Docker Network}
\label{\detokenize{vllm-network:integrating-vllm-with-ragflow-via-docker-network}}\label{\detokenize{vllm-network:ragflow-vllm-network}}\label{\detokenize{vllm-network::doc}}
\sphinxAtStartPar
To enable \sphinxstylestrong{Infiniflow RAGFlow} (running in Docker) to communicate with a \sphinxstylestrong{vLLM reranker container}, both services must be on the \sphinxstylestrong{same Docker network}. By default, containers are isolated and cannot resolve each other by service name unless explicitly networked.

\sphinxAtStartPar
In this setup, we ensure seamless internal communication between:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{RAGFlow} (web + backend containers)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{vLLM reranker} (serving \sphinxtitleref{Qwen3\sphinxhyphen{}Reranker\sphinxhyphen{}0.6B})

\end{itemize}

\sphinxAtStartPar
—


\section{Why Network Configuration is Required}
\label{\detokenize{vllm-network:why-network-configuration-is-required}}\begin{itemize}
\item {} 
\sphinxAtStartPar
RAGFlow runs inside Docker (typically via \sphinxtitleref{docker\sphinxhyphen{}compose}).

\item {} 
\sphinxAtStartPar
vLLM reranker runs in a \sphinxstylestrong{separate container} (e.g., CPU\sphinxhyphen{}only).

\item {} 
\sphinxAtStartPar
RAGFlow needs to call: \sphinxtitleref{http://\textless{}vllm\sphinxhyphen{}service\sphinxhyphen{}name\textgreater{}:8000/v1} internally.

\item {} 
\sphinxAtStartPar
Without shared network \(\rightarrow\) \sphinxtitleref{Connection refused} or DNS lookup failure.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Solution}: Attach both services to a \sphinxstylestrong{custom Docker bridge network} (e.g., \sphinxtitleref{docker\sphinxhyphen{}ragflow}).

\sphinxAtStartPar
—


\section{Step\sphinxhyphen{}by\sphinxhyphen{}Step: Configure Docker Network}
\label{\detokenize{vllm-network:step-by-step-configure-docker-network}}
\sphinxAtStartPar
\#\#\# 1. Create a Custom Network

\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker\PYG{+w}{ }network\PYG{+w}{ }create\PYG{+w}{ }docker\PYGZhy{}ragflow
\end{sphinxVerbatim}

\sphinxAtStartPar
—

\sphinxAtStartPar
\#\#\# 2. Update \sphinxtitleref{docker\sphinxhyphen{}compose.yaml} for vLLM Reranker

\sphinxAtStartPar
Ensure the vLLM service uses the network:
\sphinxSetupCaptionForVerbatim{docker\sphinxhyphen{}compose.yaml (vLLM)}
\def\sphinxLiteralBlockLabel{\label{\detokenize{vllm-network:id1}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nt}{version}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{3.8}\PYG{l+s}{\PYGZsq{}}
\PYG{n+nt}{services}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{qwen\PYGZhy{}reranker}\PYG{p}{:}
\PYG{+w}{    }\PYG{n+nt}{image}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{vllm\PYGZhy{}cpu:latest}
\PYG{+w}{    }\PYG{n+nt}{container\PYGZus{}name}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{ragflow\PYGZhy{}vllm\PYGZhy{}reranker}
\PYG{+w}{    }\PYG{n+nt}{ports}\PYG{p}{:}\PYG{+w}{ }\PYG{p+pIndicator}{[}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{8123:8000}\PYG{l+s}{\PYGZdq{}}\PYG{p+pIndicator}{]}
\PYG{+w}{    }\PYG{n+nt}{volumes}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{/home/naj/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b:/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b:ro}
\PYG{+w}{    }\PYG{n+nt}{environment}\PYG{p}{:}
\PYG{+w}{      }\PYG{n+nt}{VLLM\PYGZus{}HF\PYGZus{}OVERRIDES}\PYG{p}{:}\PYG{+w}{ }\PYG{p+pIndicator}{|}
\PYG{+w}{        }\PYG{n+no}{\PYGZob{}}
\PYG{+w}{          }\PYG{n+no}{\PYGZdq{}architectures\PYGZdq{}: [\PYGZdq{}Qwen3ForSequenceClassification\PYGZdq{}],}
\PYG{+w}{          }\PYG{n+no}{\PYGZdq{}classifier\PYGZus{}from\PYGZus{}token\PYGZdq{}: [\PYGZdq{}no\PYGZdq{}, \PYGZdq{}yes\PYGZdq{}],}
\PYG{+w}{          }\PYG{n+no}{\PYGZdq{}is\PYGZus{}original\PYGZus{}qwen3\PYGZus{}reranker\PYGZdq{}: true}
\PYG{+w}{        }\PYG{n+no}{\PYGZcb{}}
\PYG{+w}{    }\PYG{n+nt}{command}\PYG{p}{:}\PYG{+w}{ }\PYG{p+pIndicator}{\PYGZgt{}}
\PYG{+w}{      }\PYG{n+no}{/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}task score}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}dtype float32}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}port 8000}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}trust\PYGZhy{}remote\PYGZhy{}code}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}max\PYGZhy{}model\PYGZhy{}len 8192}
\PYG{+w}{    }\PYG{n+nt}{deploy}\PYG{p}{:}
\PYG{+w}{      }\PYG{n+nt}{resources}\PYG{p}{:}
\PYG{+w}{        }\PYG{n+nt}{limits}\PYG{p}{:}
\PYG{+w}{          }\PYG{n+nt}{cpus}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{10}\PYG{l+s}{\PYGZsq{}}
\PYG{+w}{          }\PYG{n+nt}{memory}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{16G}
\PYG{+w}{    }\PYG{n+nt}{shm\PYGZus{}size}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{4g}
\PYG{+w}{    }\PYG{n+nt}{restart}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{unless\PYGZhy{}stopped}
\PYG{+w}{    }\PYG{n+nt}{networks}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{docker\PYGZhy{}ragflow}

\PYG{n+nt}{networks}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{docker\PYGZhy{}ragflow}\PYG{p}{:}
\PYG{+w}{    }\PYG{n+nt}{external}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{true}
\end{sphinxVerbatim}

\sphinxAtStartPar
—

\sphinxAtStartPar
\#\#\# 3. Connect RAGFlow Containers to the Same Network

\sphinxAtStartPar
If RAGFlow is already running via its own \sphinxtitleref{docker\sphinxhyphen{}compose}, \sphinxstylestrong{attach} it:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker\PYG{+w}{ }network\PYG{+w}{ }connect\PYG{+w}{ }docker\PYGZhy{}ragflow\PYG{+w}{ }ragflow\PYGZhy{}web
docker\PYG{+w}{ }network\PYG{+w}{ }connect\PYG{+w}{ }docker\PYGZhy{}ragflow\PYG{+w}{ }ragflow\PYGZhy{}server
\end{sphinxVerbatim}

\sphinxAtStartPar
\textgreater{} Replace \sphinxtitleref{ragflow\sphinxhyphen{}web}, \sphinxtitleref{ragflow\sphinxhyphen{}server} with actual container names (check with \sphinxtitleref{docker ps}).

\sphinxAtStartPar
—

\sphinxAtStartPar
\#\#\# 4. Configure RAGFlow to Use Internal vLLM Endpoint

\sphinxAtStartPar
In RAGFlow settings (UI or config file), set:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nt}{reranker}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{provider}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{vllm}
\PYG{+w}{  }\PYG{n+nt}{api\PYGZus{}base}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{http://ragflow\PYGZhy{}vllm\PYGZhy{}reranker:8000/v1}
\PYG{+w}{  }\PYG{n+nt}{model}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Key}: Use \sphinxstylestrong{container name} (\sphinxtitleref{ragflow\sphinxhyphen{}vllm\sphinxhyphen{}reranker}) — Docker DNS resolves it automatically within the network.

\sphinxAtStartPar
—


\section{Architecture Diagram}
\label{\detokenize{vllm-network:architecture-diagram}}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=1.000\linewidth]{{ragflow-vllm-def}.png}
\caption{\sphinxstylestrong{Figure 1}: RAGFlow containers communicate with vLLM reranker via internal Docker network \sphinxtitleref{docker\sphinxhyphen{}ragflow}. External access (optional) via port \sphinxtitleref{8123}.}\label{\detokenize{vllm-network:id2}}\end{figure}

\sphinxAtStartPar
—


\section{Verification}
\label{\detokenize{vllm-network:verification}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{From RAGFlow container}, test connectivity:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker\PYG{+w}{ }\PYG{n+nb}{exec}\PYG{+w}{ }\PYGZhy{}it\PYG{+w}{ }ragflow\PYGZhy{}server\PYG{+w}{ }curl\PYG{+w}{ }http://ragflow\PYGZhy{}vllm\PYGZhy{}reranker:8000/v1/models
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Expected output}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}object\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}list\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}data\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{[}
\PYG{+w}{    }\PYG{p}{\PYGZob{}}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}id\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{      }\PYG{err}{.}\PYG{err}{.}\PYG{err}{.}
\PYG{+w}{    }\PYG{p}{\PYGZcb{}}
\PYG{+w}{  }\PYG{p}{]}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\end{enumerate}

\sphinxAtStartPar
—


\section{Benefits of This Setup}
\label{\detokenize{vllm-network:benefits-of-this-setup}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Zero external exposure} (optional): vLLM accessible \sphinxstylestrong{only} within \sphinxtitleref{docker\sphinxhyphen{}ragflow} network.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Secure \& fast} internal communication.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Scalable}: Add more rerankers, LLMs, or vector DBs on same network.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Portable}: Works across dev, staging, production with same config.

\end{itemize}

\sphinxAtStartPar
—


\section{Troubleshooting Tips}
\label{\detokenize{vllm-network:troubleshooting-tips}}
\begin{DUlineblock}{0em}
\item[] Issue | Solution |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}———\sphinxhyphen{}|
| \sphinxtitleref{Connection refused} | Check network: \sphinxtitleref{docker network inspect docker\sphinxhyphen{}ragflow} |
| \sphinxtitleref{Unknown host} | Use \sphinxstylestrong{container name}, not \sphinxtitleref{localhost} |
| Port conflict | Ensure no other service uses \sphinxtitleref{8000} inside network |
| Model not loading | Verify volume mount and \sphinxtitleref{trust\sphinxhyphen{}remote\sphinxhyphen{}code} |

\sphinxAtStartPar
—


\section{Summary}
\label{\detokenize{vllm-network:summary}}
\sphinxAtStartPar
To use \sphinxstylestrong{vLLM inside RAGFlow Docker environment}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Create network: \sphinxtitleref{docker network create docker\sphinxhyphen{}ragflow}

\item {} 
\sphinxAtStartPar
Connect both RAGFlow and vLLM containers

\item {} 
\sphinxAtStartPar
Use \sphinxstylestrong{container name} in \sphinxtitleref{api\_base}

\item {} 
\sphinxAtStartPar
Enjoy \sphinxstylestrong{fast, secure, internal reranking}

\end{enumerate}

\sphinxAtStartPar
\textgreater{} \sphinxstylestrong{No need for public IPs, reverse proxies, or complex routing} — Docker handles it all.

\sphinxstepscope


\chapter{Batch Processing and Metadata Management in Infiniflow RAGFlow}
\label{\detokenize{api:batch-processing-and-metadata-management-in-infiniflow-ragflow}}\label{\detokenize{api:ragflow-batch-api}}\label{\detokenize{api::doc}}
\sphinxAtStartPar
\sphinxstylestrong{Infiniflow RAGFlow} provides a \sphinxstylestrong{RESTful API} (\sphinxtitleref{/api/v1}) that enables \sphinxstylestrong{programmatic control} over datasets and documents, making it ideal for \sphinxstylestrong{batch processing large volumes of documents}, \sphinxstylestrong{automated ingestion pipelines}, and \sphinxstylestrong{metadata enrichment}.

\sphinxAtStartPar
This is essential in enterprise settings where thousands of PDFs, reports, or web pages need to be:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Ingested in bulk

\item {} 
\sphinxAtStartPar
Tagged with structured metadata (author, date, source, category, etc.)

\item {} 
\sphinxAtStartPar
Updated post\sphinxhyphen{}ingestion

\item {} 
\sphinxAtStartPar
Queried or filtered later via the RAG system

\end{itemize}

\sphinxAtStartPar
—


\section{API Base URL}
\label{\detokenize{api:api-base-url}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
http://\PYGZlt{}RAGFLOW\PYGZus{}HOST\PYGZgt{}/api/v1
\end{sphinxVerbatim}


\section{Authentication}
\label{\detokenize{api:authentication}}
\sphinxAtStartPar
All requests require a \sphinxstylestrong{Bearer token}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Authorization: Bearer ragflow\PYGZhy{}\PYGZlt{}your\PYGZhy{}token\PYGZgt{}
\end{sphinxVerbatim}

\sphinxAtStartPar
\textgreater{} \sphinxstylestrong{Tip}: Obtain token via login or API key management in the RAGFlow UI.

\sphinxAtStartPar
—


\section{Step 1: Retrieve Dataset and Document IDs}
\label{\detokenize{api:step-1-retrieve-dataset-and-document-ids}}
\sphinxAtStartPar
Before updating, you \sphinxstylestrong{must know} the target:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Dataset ID} (e.g., \sphinxtitleref{f388c05e9df711f0a0fe0242ac170003})

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Document ID} (e.g., \sphinxtitleref{4920227c9eb711f0bff40242ac170003})

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{List all datasets}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
curl\PYG{+w}{ }\PYGZhy{}H\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}Authorization: Bearer ragflow\PYGZhy{}...\PYGZdq{}}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{     }http://192.168.0.213/api/v1/datasets
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{List documents in a dataset}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
curl\PYG{+w}{ }\PYGZhy{}H\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}Authorization: Bearer ragflow\PYGZhy{}...\PYGZdq{}}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{     }http://192.168.0.213/api/v1/datasets/\PYGZlt{}dataset\PYGZus{}id\PYGZgt{}/documents
\end{sphinxVerbatim}

\sphinxAtStartPar
—


\section{Step 2: Add Metadata to a Document (via PUT)}
\label{\detokenize{api:step-2-add-metadata-to-a-document-via-put}}
\sphinxAtStartPar
Use the \sphinxstylestrong{PUT} endpoint to \sphinxstylestrong{update metadata} of an existing document:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
curl\PYG{+w}{ }\PYGZhy{}\PYGZhy{}request\PYG{+w}{ }PUT\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{     }\PYGZhy{}\PYGZhy{}url\PYG{+w}{ }http://192.168.0.213/api/v1/datasets/f388c05e9df711f0a0fe0242ac170003/documents/4920227c9eb711f0bff40242ac170003\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{     }\PYGZhy{}\PYGZhy{}header\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}Content\PYGZhy{}Type: multipart/form\PYGZhy{}data\PYGZsq{}}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{     }\PYGZhy{}\PYGZhy{}header\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}Authorization: Bearer ragflow\PYGZhy{}QxNWIzMGNlOWRmMzExZjBhZjljMDI0Mm\PYGZsq{}}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{     }\PYGZhy{}\PYGZhy{}data\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}\PYGZob{}}
\PYG{l+s+s1}{       \PYGZdq{}meta\PYGZus{}fields\PYGZdq{}: \PYGZob{}}
\PYG{l+s+s1}{         \PYGZdq{}author\PYGZdq{}: \PYGZdq{}Example Author\PYGZdq{},}
\PYG{l+s+s1}{         \PYGZdq{}publish\PYGZus{}date\PYGZdq{}: \PYGZdq{}2025\PYGZhy{}01\PYGZhy{}01\PYGZdq{},}
\PYG{l+s+s1}{         \PYGZdq{}category\PYGZdq{}: \PYGZdq{}AI Business Report\PYGZdq{},}
\PYG{l+s+s1}{         \PYGZdq{}url\PYGZdq{}: \PYGZdq{}https://example.com/report.pdf\PYGZdq{}}
\PYG{l+s+s1}{       \PYGZcb{}}
\PYG{l+s+s1}{     \PYGZcb{}\PYGZsq{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Request Breakdown}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Method}: \sphinxtitleref{PUT}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Path}: \sphinxtitleref{/api/v1/datasets/\textless{}dataset\_id\textgreater{}/documents/\textless{}document\_id\textgreater{}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Content\sphinxhyphen{}Type}: \sphinxtitleref{multipart/form\sphinxhyphen{}data} (required even for JSON payload)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Body}: JSON string with \sphinxtitleref{“meta\_fields”} object

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Response (on success)}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}code\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}message\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}Success\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}data\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{\PYGZob{}}\PYG{+w}{ }\PYG{n+nt}{\PYGZdq{}document\PYGZus{}id\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}4920227c9eb711f0bff40242ac170003\PYGZdq{}}\PYG{+w}{ }\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
—


\section{Use Case: Batch Metadata Enrichment}
\label{\detokenize{api:use-case-batch-metadata-enrichment}}
\sphinxAtStartPar
You can \sphinxstylestrong{automate metadata tagging} for \sphinxstylestrong{1000s of documents} using a script:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{requests}
\PYG{k+kn}{import} \PYG{n+nn}{json}

\PYG{n}{BASE\PYGZus{}URL} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{http://192.168.0.213/api/v1}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{TOKEN} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ragflow\PYGZhy{}QxNWIzMGNlOWRmMzExZjBhZjljMDI0Mm}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{HEADERS} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Authorization}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Bearer }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{TOKEN}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Content\PYGZhy{}Type}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{multipart/form\PYGZhy{}data}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{p}{\PYGZcb{}}

\PYG{c+c1}{\PYGZsh{} Example: Load CSV with doc\PYGZus{}id, author, date, url...}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{documents\PYGZus{}metadata.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{\PYGZus{}}\PYG{p}{,} \PYG{n}{row} \PYG{o+ow}{in} \PYG{n}{df}\PYG{o}{.}\PYG{n}{iterrows}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{dataset\PYGZus{}id} \PYG{o}{=} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dataset\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
    \PYG{n}{doc\PYGZus{}id} \PYG{o}{=} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{document\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
    \PYG{n}{payload} \PYG{o}{=} \PYG{p}{\PYGZob{}}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{meta\PYGZus{}fields}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{author}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{author}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{publish\PYGZus{}date}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{publish\PYGZus{}date}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{category}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{category}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{url}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{source\PYGZus{}url}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
        \PYG{p}{\PYGZcb{}}
    \PYG{p}{\PYGZcb{}}

    \PYG{n}{files} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{json}\PYG{o}{.}\PYG{n}{dumps}\PYG{p}{(}\PYG{n}{payload}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{application/json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{\PYGZcb{}}
    \PYG{n}{resp} \PYG{o}{=} \PYG{n}{requests}\PYG{o}{.}\PYG{n}{put}\PYG{p}{(}
        \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{BASE\PYGZus{}URL}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{/datasets/}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{dataset\PYGZus{}id}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{/documents/}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{doc\PYGZus{}id}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{headers}\PYG{o}{=}\PYG{n}{HEADERS}\PYG{p}{,}
        \PYG{n}{files}\PYG{o}{=}\PYG{n}{files}
    \PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{doc\PYGZus{}id}\PYG{p}{,} \PYG{n}{resp}\PYG{o}{.}\PYG{n}{json}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{message}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Benefits}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Enrich RAG context with \sphinxstylestrong{structured, queryable metadata}

\item {} 
\sphinxAtStartPar
Enable \sphinxstylestrong{filtering} in UI or API (e.g., “Show reports from 2025 by Author X”)

\item {} 
\sphinxAtStartPar
Improve \sphinxstylestrong{traceability} and \sphinxstylestrong{auditability}

\end{itemize}

\sphinxAtStartPar
—


\section{Other Batch\sphinxhyphen{}Capable Endpoints}
\label{\detokenize{api:other-batch-capable-endpoints}}
\begin{DUlineblock}{0em}
\item[] Endpoint | Purpose |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}——\textendash{}|
| \sphinxtitleref{POST /api/v1/datasets} | Create new dataset |
| \sphinxtitleref{POST /api/v1/datasets/\{id\}/documents} | Upload new documents (with metadata) |
| \sphinxtitleref{DELETE /api/v1/datasets/\{id\}/documents/\{doc\_id\}} | Remove document |
| \sphinxtitleref{GET /api/v1/datasets/\{id\}/documents} | List + filter by metadata |

\sphinxAtStartPar
—


\section{Best Practices}
\label{\detokenize{api:best-practices}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Always use IDs} — never rely on filenames

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Batch in chunks} (e.g., 100 docs/sec) to avoid rate limits

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Validate metadata schema} in RAGFlow settings first

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Log responses} for retry logic

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Use dataset\sphinxhyphen{}level permissions} for access control

\end{enumerate}

\sphinxAtStartPar
—


\section{See also:}
\label{\detokenize{api:see-also}}
\sphinxAtStartPar
\sphinxurl{https://github.com/infiniflow/ragflow/blob/main/example/http/dataset\_example.sh}


\section{Summary}
\label{\detokenize{api:summary}}
\sphinxAtStartPar
RAGFlow’s \sphinxstylestrong{API\sphinxhyphen{}first design} enables:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Scalable batch ingestion}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Rich metadata attachment}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Full automation} of document lifecycle

\end{itemize}

\sphinxAtStartPar
\textgreater{} \sphinxstylestrong{Perfect for ETL pipelines, CMS integration, or enterprise knowledge base automation.}

\sphinxAtStartPar
With this API, you can manage \sphinxstylestrong{tens of thousands of documents} with full metadata — all programmatically.

\sphinxstepscope


\chapter{How the Knowledge Graph in Infiniflow/RAGFlow Works}
\label{\detokenize{graph:how-the-knowledge-graph-in-infiniflow-ragflow-works}}\label{\detokenize{graph::doc}}
\sphinxAtStartPar
RAGFlow is an open\sphinxhyphen{}source Retrieval\sphinxhyphen{}Augmented Generation (RAG) engine developed by Infiniflow, designed to enhance LLM\sphinxhyphen{}based question\sphinxhyphen{}answering by integrating deep document understanding with structured data processing. The knowledge graph (KG) component plays a pivotal role in handling complex queries, particularly multi\sphinxhyphen{}hop question\sphinxhyphen{}answering, by extracting and organizing entities and relationships from documents into a graph structure. This enables more accurate and interconnected retrieval beyond simple vector\sphinxhyphen{}based searches.


\section{Overview}
\label{\detokenize{graph:overview}}
\sphinxAtStartPar
The KG is constructed as an intermediate step in RAGFlow’s data pipeline, bridging raw document extraction and final indexing. It transforms unstructured text into a relational graph, allowing for entity\sphinxhyphen{}based reasoning and traversal during retrieval. Key benefits include:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Multi\sphinxhyphen{}Hop Query Support}: Facilitates queries requiring inference across multiple documents or concepts (e.g., “What caused the event that affected company X?”).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Dynamic Updates}: From version 0.16.0 onward, the KG is built across an entire knowledge base (multiple files) and automatically updates when new documents are uploaded and parsed.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Integration with RAG 2.0}: Part of preprocessing stages like document clustering and domain\sphinxhyphen{}specific embedding, ensuring retrieval results are contextually rich and grounded.

\end{itemize}

\sphinxAtStartPar
The KG is stored as chunks in RAGFlow’s document engine (Elasticsearch by default or Infinity for advanced vector/graph capabilities), making it queryable alongside embeddings.


\section{Construction Process}
\label{\detokenize{graph:construction-process}}
\sphinxAtStartPar
The KG construction occurs after initial document parsing but before indexing. Here’s the step\sphinxhyphen{}by\sphinxhyphen{}step workflow:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Document Ingestion and Extraction}:
\sphinxhyphen{} Users upload files (e.g., PDF, Word, Excel, TXT) to a knowledge base.
\sphinxhyphen{} RAGFlow’s Deep Document Understanding (DDU) module parses the content, extracting structured elements like text blocks, tables, and layouts using OCR and layout models.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Entity and Relation Extraction}:
\sphinxhyphen{} Using NLP models (integrated via configurable LLMs or embedding services), RAGFlow identifies entities (e.g., people, organizations, events) and relations (e.g., “causes”, “affiliated with”) from extracted chunks.
\sphinxhyphen{} This is model\sphinxhyphen{}driven: Preprocessing applies entity recognition and relation extraction to raw text, often leveraging domain\sphinxhyphen{}specific prompts for accuracy.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Graph Building}:
\sphinxhyphen{} Entities become nodes, and relations form directed/undirected edges.
\sphinxhyphen{} The graph is unified across the entire dataset (not per\sphinxhyphen{}file since v0.16.0), enabling cross\sphinxhyphen{}document connections.
\sphinxhyphen{} Acceleration features (introduced in later releases) optimize extraction speed, such as batch processing or efficient model inference.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Storage}:
\sphinxhyphen{} Graph chunks (nodes, edges, metadata) are serialized and stored in the document engine.
\sphinxhyphen{} No separate graph database is required; it’s embedded within the vector/full\sphinxhyphen{}text index for hybrid queries.

\end{enumerate}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Construction can be toggled per knowledge base and is optional, but recommended for complex domains like finance or healthcare.
\end{sphinxadmonition}


\section{Query and Retrieval Process}
\label{\detokenize{graph:query-and-retrieval-process}}
\sphinxAtStartPar
During inference, the KG enhances retrieval in the following manner:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Query Parsing}:
\sphinxhyphen{} Incoming user queries are analyzed to detect multi\sphinxhyphen{}hop intent (e.g., via LLM routing).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hybrid Retrieval}:
\sphinxhyphen{} Combine vector similarity search (for semantic relevance) with graph traversal:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Start from query entities as seed nodes.

\item {} 
\sphinxAtStartPar
Traverse edges to fetch connected nodes (e.g., 1\sphinxhyphen{}2 hops).

\item {} 
\sphinxAtStartPar
Rank results by relevance scores, incorporating graph proximity.

\end{itemize}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Infinity engine (optional) supports efficient graph\sphinxhyphen{}range filtering alongside vectors.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Augmentation and Generation}:
\sphinxhyphen{} Retrieved graph\sphinxhyphen{}derived contexts (e.g., subgraphs or paths) are fused with text chunks.
\sphinxhyphen{} Fed to the LLM for grounded generation, with citations traceable to source documents.

\end{enumerate}

\sphinxAtStartPar
This process addresses limitations of pure vector RAG, such as hallucination in interconnected scenarios, by providing explicit relational paths.


\section{Key Features and Limitations}
\label{\detokenize{graph:key-features-and-limitations}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Features}:
\sphinxhyphen{} \sphinxstylestrong{Scalability}: Handles enterprise\sphinxhyphen{}scale knowledge bases with dynamic rebuilding.
\sphinxhyphen{} \sphinxstylestrong{Customizability}: Configurable extraction models and hop limits.
\sphinxhyphen{} \sphinxstylestrong{Agent Integration}: Supports agentic workflows for iterative graph exploration.
\sphinxhyphen{} \sphinxstylestrong{Performance}: Accelerated extraction in v0.21+ releases.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Limitations}:
\sphinxhyphen{} Relies on quality of upstream extraction; noisy documents may yield incomplete graphs.
\sphinxhyphen{} Graph depth is configurable but can increase latency for deep traversals.
\sphinxhyphen{} Arm64 Linux support is limited when using Infinity.

\end{itemize}

\sphinxAtStartPar
For implementation details, refer to the official guide at \sphinxurl{https://github.com/infiniflow/ragflow/blob/main/docs/guides/dataset/construct\_knowledge\_graph.md}. To experiment, deploy RAGFlow via Docker and enable KG in your knowledge base settings.

\sphinxstepscope


\chapter{Running Llama 3.1 with llama.cpp}
\label{\detokenize{llama-cpp:running-llama-3-1-with-llama-cpp}}\label{\detokenize{llama-cpp:llama-cpp-model-format-gguf}}\label{\detokenize{llama-cpp::doc}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{Table of Contents}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{llama-cpp:id1}}{\hyperref[\detokenize{llama-cpp:model-format-gguf}]{\sphinxcrossref{1. Model Format: GGUF}}}

\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{llama-cpp:id2}}{\hyperref[\detokenize{llama-cpp:compile-llama-cpp-for-intel-i7-cpu-only}]{\sphinxcrossref{2. Compile llama.cpp for Intel i7 (CPU\sphinxhyphen{}only)}}}

\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{llama-cpp:id3}}{\hyperref[\detokenize{llama-cpp:run-the-model-with-web-interface}]{\sphinxcrossref{3. Run the Model with Web Interface}}}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{llama-cpp:id4}}{\hyperref[\detokenize{llama-cpp:features}]{\sphinxcrossref{Features}}}

\end{itemize}

\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{llama-cpp:id5}}{\hyperref[\detokenize{llama-cpp:compile-llama-cpp-with-nvidia-gpu-support-cuda}]{\sphinxcrossref{4. Compile llama.cpp with NVIDIA GPU Support (CUDA)}}}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{llama-cpp:id6}}{\hyperref[\detokenize{llama-cpp:prerequisites}]{\sphinxcrossref{Prerequisites}}}

\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{llama-cpp:id7}}{\hyperref[\detokenize{llama-cpp:build-with-cuda}]{\sphinxcrossref{Build with CUDA}}}

\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{llama-cpp:id8}}{\hyperref[\detokenize{llama-cpp:run-with-gpu-offloading}]{\sphinxcrossref{Run with GPU offloading}}}

\end{itemize}

\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{llama-cpp:id9}}{\hyperref[\detokenize{llama-cpp:summary}]{\sphinxcrossref{Summary}}}

\end{itemize}
\end{sphinxShadowBox}


\bigskip\hrule\bigskip



\section{1. Model Format: GGUF}
\label{\detokenize{llama-cpp:model-format-gguf}}\label{\detokenize{llama-cpp:llama-cpp-compile-llama-cpp-for-intel-i7-cpu-only}}
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{llama.cpp}} uses the \sphinxstylestrong{GGUF} (GPT\sphinxhyphen{}Generated Unified Format) model format.

\sphinxAtStartPar
You can download a pre\sphinxhyphen{}quantized \sphinxstylestrong{Llama 3.1 8B} model in GGUF format directly from Hugging Face:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
https://huggingface.co/QuantFactory/Meta\PYGZhy{}Llama\PYGZhy{}3\PYGZhy{}8B\PYGZhy{}GGUF
\end{sphinxVerbatim}

\sphinxAtStartPar
Example file: \sphinxcode{\sphinxupquote{Meta\sphinxhyphen{}Llama\sphinxhyphen{}3\sphinxhyphen{}8B.Q4\_K\_S.gguf}} (\textasciitilde{}4.7 GB, 4\sphinxhyphen{}bit quantization, excellent quality/size tradeoff).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{Q4\_K\_S}} variant uses \textasciitilde{}4.7 GB RAM and runs efficiently on Intel i7 CPUs.
\end{sphinxadmonition}


\bigskip\hrule\bigskip



\section{2. Compile llama.cpp for Intel i7 (CPU\sphinxhyphen{}only)}
\label{\detokenize{llama-cpp:compile-llama-cpp-for-intel-i7-cpu-only}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Step 1: Clone the repository}
git\PYG{+w}{ }clone\PYG{+w}{ }https://github.com/ggerganov/llama.cpp
\PYG{n+nb}{cd}\PYG{+w}{ }llama.cpp

\PYG{c+c1}{\PYGZsh{} Step 2: Build for CPU (Intel i7, AVX2 enabled by default)}
make\PYG{+w}{ }clean
make\PYG{+w}{ }\PYGZhy{}j\PYG{k}{\PYGZdl{}(}nproc\PYG{k}{)}\PYG{+w}{ }\PYG{n+nv}{LLAMA\PYGZus{}CPU}\PYG{o}{=}\PYG{l+m}{1}

\PYG{c+c1}{\PYGZsh{} Optional: Force AVX2 (most i7 CPUs support it)}
make\PYG{+w}{ }clean
make\PYG{+w}{ }\PYGZhy{}j\PYG{k}{\PYGZdl{}(}nproc\PYG{k}{)}\PYG{+w}{ }\PYG{n+nv}{LLAMA\PYGZus{}CPU}\PYG{o}{=}\PYG{l+m}{1}\PYG{+w}{ }\PYG{n+nv}{LLAMA\PYGZus{}AVX2}\PYG{o}{=}\PYG{l+m}{1}
\end{sphinxVerbatim}

\sphinxAtStartPar
The binaries will be in the root directory:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{./llama\sphinxhyphen{}cli}} \(\rightarrow\) interactive CLI

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{./server}}   \(\rightarrow\) web server (OpenAI\sphinxhyphen{}compatible API + full web UI)

\end{itemize}

\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
\sphinxstylestrong{Do not use vLLM} on older Xeon v2 CPUs — they \sphinxstylestrong{lack AVX\sphinxhyphen{}512}, which vLLM requires.
\sphinxstylestrong{llama.cpp is a better choice} — it runs efficiently with just AVX2 or even SSE.
\end{sphinxadmonition}


\bigskip\hrule\bigskip



\section{3. Run the Model with Web Interface}
\label{\detokenize{llama-cpp:run-the-model-with-web-interface}}\label{\detokenize{llama-cpp:llama-cpp-run-the-model-with-web-interface}}
\sphinxAtStartPar
Place the downloaded GGUF file in a \sphinxcode{\sphinxupquote{models/}} folder:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
mkdir\PYG{+w}{ }\PYGZhy{}p\PYG{+w}{ }models
\PYG{c+c1}{\PYGZsh{} Copy or symlink the model}
ln\PYG{+w}{ }\PYGZhy{}s\PYG{+w}{ }/path/to/Meta\PYGZhy{}Llama\PYGZhy{}3\PYGZhy{}8B.Q4\PYGZus{}K\PYGZus{}S.gguf\PYG{+w}{ }models/
\end{sphinxVerbatim}

\sphinxAtStartPar
Start the server on port \sphinxstylestrong{8087} using \sphinxstylestrong{12 threads}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
./server\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}model\PYG{+w}{ }models/Meta\PYGZhy{}Llama\PYGZhy{}3\PYGZhy{}8B.Q4\PYGZus{}K\PYGZus{}S.gguf\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}port\PYG{+w}{ }\PYG{l+m}{8087}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}threads\PYG{+w}{ }\PYG{l+m}{12}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}host\PYG{+w}{ }\PYG{l+m}{0}.0.0.0
\end{sphinxVerbatim}

\sphinxAtStartPar
In a corporate network : avoid contacting the proxy !!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
curl\PYG{+w}{ }\PYGZhy{}X\PYG{+w}{ }POST\PYG{+w}{ }http://127.0.0.1:8087/v1/chat/completions\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}noproxy\PYG{+w}{ }\PYG{l+m}{127}.0.0.1,localhost\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}H\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}Content\PYGZhy{}Type: application/json\PYGZdq{}}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}d\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}\PYGZob{}}
\PYG{l+s+s1}{    \PYGZdq{}model\PYGZdq{}: \PYGZdq{}Meta\PYGZhy{}Llama\PYGZhy{}3\PYGZhy{}8B\PYGZdq{},}
\PYG{l+s+s1}{    \PYGZdq{}messages\PYGZdq{}: [}
\PYG{l+s+s1}{      \PYGZob{}\PYGZdq{}role\PYGZdq{}: \PYGZdq{}system\PYGZdq{}, \PYGZdq{}content\PYGZdq{}: \PYGZdq{}You are a helpful assistant.\PYGZdq{}\PYGZcb{},}
\PYG{l+s+s1}{      \PYGZob{}\PYGZdq{}role\PYGZdq{}: \PYGZdq{}user\PYGZdq{}, \PYGZdq{}content\PYGZdq{}: \PYGZdq{}Hello! How are you?\PYGZdq{}\PYGZcb{}}
\PYG{l+s+s1}{    ]}
\PYG{l+s+s1}{  \PYGZcb{}\PYGZsq{}}
\end{sphinxVerbatim}


\subsection{Features}
\label{\detokenize{llama-cpp:features}}\label{\detokenize{llama-cpp:llama-cpp-features}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Full web UI} at: \sphinxurl{http://localhost:8087}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{OpenAI\sphinxhyphen{}compatible API} at: \sphinxurl{http://localhost:8087/v1}

\item {} 
\sphinxAtStartPar
List models:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
curl\PYG{+w}{ }http://localhost:8087/v1/models
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Response}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}object\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}list\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}data\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{[}
\PYG{+w}{    }\PYG{p}{\PYGZob{}}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}id\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}models/Meta\PYGZhy{}Llama\PYGZhy{}3\PYGZhy{}8B.Q4\PYGZus{}K\PYGZus{}S.gguf\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}object\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}model\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}created\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{1762783003}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}owned\PYGZus{}by\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}llamacpp\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}meta\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{        }\PYG{n+nt}{\PYGZdq{}vocab\PYGZus{}type\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{2}\PYG{p}{,}
\PYG{+w}{        }\PYG{n+nt}{\PYGZdq{}n\PYGZus{}vocab\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{128256}\PYG{p}{,}
\PYG{+w}{        }\PYG{n+nt}{\PYGZdq{}n\PYGZus{}ctx\PYGZus{}train\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{8192}\PYG{p}{,}
\PYG{+w}{        }\PYG{n+nt}{\PYGZdq{}n\PYGZus{}embd\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{4096}\PYG{p}{,}
\PYG{+w}{        }\PYG{n+nt}{\PYGZdq{}n\PYGZus{}params\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{8030261248}\PYG{p}{,}
\PYG{+w}{        }\PYG{n+nt}{\PYGZdq{}size\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{4684832768}
\PYG{+w}{      }\PYG{p}{\PYGZcb{}}
\PYG{+w}{    }\PYG{p}{\PYGZcb{}}
\PYG{+w}{  }\PYG{p}{]}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\bigskip\hrule\bigskip



\section{4. Compile llama.cpp with NVIDIA GPU Support (CUDA)}
\label{\detokenize{llama-cpp:compile-llama-cpp-with-nvidia-gpu-support-cuda}}\label{\detokenize{llama-cpp:llama-cpp-compile-llama-cpp-with-nvidia-gpu-support-cuda}}
\sphinxAtStartPar
If you have an \sphinxstylestrong{NVIDIA GPU} (e.g., RTX 3060, 4070, A100, etc.), enable \sphinxstylestrong{CUDA acceleration}:


\subsection{Prerequisites}
\label{\detokenize{llama-cpp:prerequisites}}\label{\detokenize{llama-cpp:llama-cpp-prerequisites}}\begin{itemize}
\item {} 
\sphinxAtStartPar
NVIDIA driver (\textbackslash{}geq\{\} 525)

\item {} 
\sphinxAtStartPar
CUDA Toolkit (\textbackslash{}geq\{\} 11.8, preferably 12.x)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{nvcc}} in \sphinxcode{\sphinxupquote{\$PATH}}

\end{itemize}


\subsection{Build with CUDA}
\label{\detokenize{llama-cpp:build-with-cuda}}\label{\detokenize{llama-cpp:llama-cpp-build-with-cuda}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Clean previous build}
make\PYG{+w}{ }clean

\PYG{c+c1}{\PYGZsh{} Build with full CUDA support}
make\PYG{+w}{ }\PYGZhy{}j\PYG{k}{\PYGZdl{}(}nproc\PYG{k}{)}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYG{n+nv}{LLAMA\PYGZus{}CUDA}\PYG{o}{=}\PYG{l+m}{1}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYG{n+nv}{LLAMA\PYGZus{}CUDA\PYGZus{}DMMV}\PYG{o}{=}\PYG{l+m}{1}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYG{n+nv}{LLAMA\PYGZus{}CUDA\PYGZus{}F16}\PYG{o}{=}\PYG{l+m}{1}

\PYG{c+c1}{\PYGZsh{} Optional: Specify compute capability (e.g., for RTX 40xx)}
\PYG{c+c1}{\PYGZsh{} make LLAMA\PYGZus{}CUDA=1 CUDA\PYGZus{}ARCH=\PYGZdq{}\PYGZhy{}gencode arch=compute\PYGZus{}89,code=sm\PYGZus{}89\PYGZdq{}}
\end{sphinxVerbatim}


\subsection{Run with GPU offloading}
\label{\detokenize{llama-cpp:run-with-gpu-offloading}}\label{\detokenize{llama-cpp:llama-cpp-run-with-gpu-offloading}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
./server\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}model\PYG{+w}{ }models/Meta\PYGZhy{}Llama\PYGZhy{}3\PYGZhy{}8B.Q4\PYGZus{}K\PYGZus{}S.gguf\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}port\PYG{+w}{ }\PYG{l+m}{8087}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}threads\PYG{+w}{ }\PYG{l+m}{8}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}n\PYGZhy{}gpu\PYGZhy{}layers\PYG{+w}{ }\PYG{l+m}{999}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{} }\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} offload ALL layers to GPU}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}host\PYG{+w}{ }\PYG{l+m}{0}.0.0.0
\end{sphinxVerbatim}

\begin{sphinxadmonition}{tip}{Tip:}
\sphinxAtStartPar
Use \sphinxcode{\sphinxupquote{nvidia\sphinxhyphen{}smi}} to monitor VRAM usage.
For 8B Q4 (\textasciitilde{}4.7 GB), even a \sphinxstylestrong{6 GB GPU} can run it fully offloaded.
\end{sphinxadmonition}


\bigskip\hrule\bigskip



\section{Summary}
\label{\detokenize{llama-cpp:summary}}\label{\detokenize{llama-cpp:llama-cpp-summary}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Feature
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Command / Note
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
Model Format
&
\sphinxAtStartPar
\sphinxstylestrong{GGUF}
\\
\sphinxhline
\sphinxAtStartPar
Download
&
\sphinxAtStartPar
\sphinxhref{https://huggingface.co/QuantFactory/...GGUF}{https://huggingface.co/QuantFactory/…GGUF}
\\
\sphinxhline
\sphinxAtStartPar
CPU Build (i7)
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{make LLAMA\_CPU=1}}
\\
\sphinxhline
\sphinxAtStartPar
GPU Build (CUDA)
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{make LLAMA\_CUDA=1}}
\\
\sphinxhline
\sphinxAtStartPar
Run Server
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{./server \sphinxhyphen{}\sphinxhyphen{}model ... \sphinxhyphen{}\sphinxhyphen{}port 8087}}
\\
\sphinxhline
\sphinxAtStartPar
Web UI
&
\sphinxAtStartPar
\sphinxurl{http://localhost:8087}
\\
\sphinxhline
\sphinxAtStartPar
API
&
\sphinxAtStartPar
\sphinxurl{http://localhost:8087/v1}
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\sphinxstylestrong{llama.cpp = lightweight, CPU/GPU flexible, no AVX\sphinxhyphen{}512 needed \(\rightarrow\) ideal replacement for vLLM on older hardware.}

\sphinxAtStartPar
—

\sphinxstepscope


\chapter{Running Multiple Models on llama.cpp Using Docker}
\label{\detokenize{multiple-models-llamacpp:running-multiple-models-on-llama-cpp-using-docker}}\label{\detokenize{multiple-models-llamacpp:running-multiple-models-llama-cpp-docker}}\label{\detokenize{multiple-models-llamacpp::doc}}
\sphinxAtStartPar
This guide demonstrates how to run multiple language models simultaneously using \sphinxcode{\sphinxupquote{llama.cpp}} in Docker via \sphinxcode{\sphinxupquote{docker\sphinxhyphen{}compose}}. The example below defines two services: a lightweight reranker model (Qwen3 0.6B) and a general\sphinxhyphen{}purpose chat model (Llama 3.1).


\section{Example \sphinxstyleliteralintitle{\sphinxupquote{docker\sphinxhyphen{}compose.yml}}}
\label{\detokenize{multiple-models-llamacpp:example-docker-compose-yml}}
\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{n+nt}{services}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{qwen\PYGZhy{}reranker}\PYG{p}{:}
\PYG{+w}{    }\PYG{n+nt}{image}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{ghcr.io/ggerganov/llama.cpp:server\PYGZhy{}cpu}
\PYG{+w}{    }\PYG{n+nt}{ports}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{8123:8080}\PYG{l+s}{\PYGZdq{}}
\PYG{+w}{    }\PYG{n+nt}{volumes}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{/home/naj/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b:/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b:ro}
\PYG{+w}{    }\PYG{n+nt}{environment}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{MODEL=/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b}
\PYG{+w}{    }\PYG{n+nt}{command}\PYG{p}{:}\PYG{+w}{ }\PYG{p+pIndicator}{\PYGZgt{}}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}model /models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b/model.gguf}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}port 8080}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}host 0.0.0.0}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}n\PYGZhy{}gpu\PYGZhy{}layers 0}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}ctx\PYGZhy{}size 8192}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}threads 6}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}temp 0.0}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}rpc}
\PYG{+w}{    }\PYG{n+nt}{deploy}\PYG{p}{:}
\PYG{+w}{      }\PYG{n+nt}{resources}\PYG{p}{:}
\PYG{+w}{        }\PYG{n+nt}{limits}\PYG{p}{:}
\PYG{+w}{          }\PYG{n+nt}{cpus}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{6}\PYG{l+s}{\PYGZsq{}}
\PYG{+w}{          }\PYG{n+nt}{memory}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{10G}
\PYG{+w}{    }\PYG{n+nt}{shm\PYGZus{}size}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{4g}
\PYG{+w}{    }\PYG{n+nt}{restart}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{unless\PYGZhy{}stopped}

\PYG{+w}{  }\PYG{n+nt}{llama3.1\PYGZhy{}chat}\PYG{p}{:}
\PYG{+w}{    }\PYG{n+nt}{image}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{ghcr.io/ggerganov/llama.cpp:server\PYGZhy{}cpu}
\PYG{+w}{    }\PYG{n+nt}{ports}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{8124:8080}\PYG{l+s}{\PYGZdq{}}
\PYG{+w}{    }\PYG{n+nt}{volumes}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{/home/naj/llama3.1:/models/llama3.1:ro}
\PYG{+w}{    }\PYG{n+nt}{environment}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{MODEL=/models/llama3.1}
\PYG{+w}{    }\PYG{n+nt}{command}\PYG{p}{:}\PYG{+w}{ }\PYG{p+pIndicator}{\PYGZgt{}}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}model /models/llama3.1/model.gguf}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}port 8080}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}host 0.0.0.0}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}n\PYGZhy{}gpu\PYGZhy{}layers 0}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}ctx\PYGZhy{}size 8192}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}threads 10}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}temp 0.7}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}rpc}
\PYG{+w}{    }\PYG{n+nt}{deploy}\PYG{p}{:}
\PYG{+w}{      }\PYG{n+nt}{resources}\PYG{p}{:}
\PYG{+w}{        }\PYG{n+nt}{limits}\PYG{p}{:}
\PYG{+w}{          }\PYG{n+nt}{cpus}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{10}\PYG{l+s}{\PYGZsq{}}
\PYG{+w}{          }\PYG{n+nt}{memory}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{20G}
\PYG{+w}{    }\PYG{n+nt}{shm\PYGZus{}size}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{8g}
\PYG{+w}{    }\PYG{n+nt}{restart}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{unless\PYGZhy{}stopped}
\end{sphinxVerbatim}


\section{Key Configuration Notes}
\label{\detokenize{multiple-models-llamacpp:key-configuration-notes}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Images}: Both services use the official CPU\sphinxhyphen{}optimized \sphinxcode{\sphinxupquote{llama.cpp}} server image.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Ports}:
\sphinxhyphen{} Reranker exposed on \sphinxcode{\sphinxupquote{8123}} \(\rightarrow\) internal \sphinxcode{\sphinxupquote{8080}}
\sphinxhyphen{} Chat model exposed on \sphinxcode{\sphinxupquote{8124}} \(\rightarrow\) internal \sphinxcode{\sphinxupquote{8080}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Volumes}: Model directories are mounted read\sphinxhyphen{}only (\sphinxcode{\sphinxupquote{:ro}}) from the host.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Environment}: \sphinxcode{\sphinxupquote{MODEL}} variable simplifies path references in commands.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Command Flags}:
\sphinxhyphen{} \sphinxcode{\sphinxupquote{\sphinxhyphen{}\sphinxhyphen{}n\sphinxhyphen{}gpu\sphinxhyphen{}layers 0}}: Forces CPU\sphinxhyphen{}only inference.
\sphinxhyphen{} \sphinxcode{\sphinxupquote{\sphinxhyphen{}\sphinxhyphen{}ctx\sphinxhyphen{}size 8192}}: Sets context length.
\sphinxhyphen{} \sphinxcode{\sphinxupquote{\sphinxhyphen{}\sphinxhyphen{}temp}}: Controls randomness (0.0 for deterministic reranking, 0.7 for chat).
\sphinxhyphen{} \sphinxcode{\sphinxupquote{\sphinxhyphen{}\sphinxhyphen{}rpc}}: Enables RPC interface for external control.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Resource Limits}: CPU and memory capped via \sphinxcode{\sphinxupquote{deploy.resources.limits}}.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Shared Memory (shm\_size)}: Increased to support larger contexts and batching.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Restart Policy}: \sphinxcode{\sphinxupquote{unless\sphinxhyphen{}stopped}} ensures containers restart on failure or reboot.

\end{itemize}


\section{Usage}
\label{\detokenize{multiple-models-llamacpp:usage}}
\sphinxAtStartPar
Start the services:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker\PYG{+w}{ }compose\PYG{+w}{ }up\PYG{+w}{ }\PYGZhy{}d
\end{sphinxVerbatim}

\sphinxAtStartPar
Access the models:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Reranker: \sphinxcode{\sphinxupquote{http://localhost:8123}}

\item {} 
\sphinxAtStartPar
Chat:     \sphinxcode{\sphinxupquote{http://localhost:8124}}

\end{itemize}

\sphinxAtStartPar
Send requests using the OpenAI\sphinxhyphen{}compatible API or \sphinxcode{\sphinxupquote{llama.cpp}} client tools.

\sphinxstepscope

\sphinxAtStartPar
{\color{red}\bfseries{}\textasciigrave{}\textasciigrave{}}{\color{red}\bfseries{}\textasciigrave{}}rst
.. \_deployment\sphinxhyphen{}considerations:


\chapter{Deploying LLMs in Hybrid Cloud: Why llama.cpp Wins for Us}
\label{\detokenize{hybrid:deploying-llms-in-hybrid-cloud-why-llama-cpp-wins-for-us}}\label{\detokenize{hybrid::doc}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{Table of Contents}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{hybrid:id5}}{\hyperref[\detokenize{hybrid:current-setup-ollama-in-testing}]{\sphinxcrossref{1. Current Setup: Ollama in Testing}}}

\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{hybrid:id6}}{\hyperref[\detokenize{hybrid:production-requirements-hybrid-cloud-multi-user-access}]{\sphinxcrossref{2. Production Requirements: Hybrid Cloud \& Multi\sphinxhyphen{}User Access}}}

\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{hybrid:id7}}{\hyperref[\detokenize{hybrid:evaluation-vllm-vs-llama-cpp}]{\sphinxcrossref{3. Evaluation: vLLM vs llama.cpp}}}

\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{hybrid:id8}}{\hyperref[\detokenize{hybrid:why-llama-cpp-is-our-production-choice}]{\sphinxcrossref{4. Why llama.cpp Is Our Production Choice}}}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{hybrid:id9}}{\hyperref[\detokenize{hybrid:example-production-ready-server}]{\sphinxcrossref{Example: Production\sphinxhyphen{}Ready Server}}}

\end{itemize}

\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{hybrid:id10}}{\hyperref[\detokenize{hybrid:migration-path-from-ollama-llama-cpp}]{\sphinxcrossref{5. Migration Path: From Ollama \(\rightarrow\) llama.cpp}}}

\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{hybrid:id11}}{\hyperref[\detokenize{hybrid:summary}]{\sphinxcrossref{Summary}}}

\end{itemize}
\end{sphinxShadowBox}


\bigskip\hrule\bigskip



\section{1. Current Setup: Ollama in Testing}
\label{\detokenize{hybrid:current-setup-ollama-in-testing}}
\sphinxAtStartPar
We have been using \sphinxstylestrong{Ollama} in a \sphinxstylestrong{test environment} with excellent results:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Easy to use} — \sphinxtitleref{ollama run llama3.1} just works

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Docker support} is first\sphinxhyphen{}class:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{FROM}\PYG{+w}{ }\PYG{l+s}{ollama/ollama}
\PYG{k}{COPY}\PYG{+w}{ }Modelfile\PYG{+w}{ }/root/.ollama/
\PYG{k}{RUN}\PYG{+w}{ }ollama\PYG{+w}{ }create\PYG{+w}{ }my\PYGZhy{}llama3.1\PYG{+w}{ }\PYGZhy{}f\PYG{+w}{ }Modelfile
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
Models are pulled, versioned, and cached automatically

\item {} 
\sphinxAtStartPar
Web UI and OpenAI\sphinxhyphen{}compatible API available out of the box

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Verdict}: Perfect for \sphinxstylestrong{prototyping}, \sphinxstylestrong{local dev}, and \sphinxstylestrong{small\sphinxhyphen{}scale testing}.


\bigskip\hrule\bigskip



\section{2. Production Requirements: Hybrid Cloud \& Multi\sphinxhyphen{}User Access}
\label{\detokenize{hybrid:production-requirements-hybrid-cloud-multi-user-access}}
\sphinxAtStartPar
When moving to \sphinxstylestrong{production in a hybrid cloud}, new constraints emerge:

\sphinxAtStartPar
We need a \sphinxstylestrong{lightweight, portable, hardware\sphinxhyphen{}agnostic} inference engine.


\bigskip\hrule\bigskip



\section{3. Evaluation: vLLM vs llama.cpp}
\label{\detokenize{hybrid:evaluation-vllm-vs-llama-cpp}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabular}[t]{\X{30}{100}\X{35}{100}\X{35}{100}}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Criteria
&\sphinxstyletheadfamily 
\sphinxAtStartPar
vLLM
&\sphinxstyletheadfamily 
\sphinxAtStartPar
llama.cpp
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
\sphinxstylestrong{Hardware Requirements}
&
\sphinxAtStartPar
Requires \sphinxstylestrong{AVX\sphinxhyphen{}512} (fails on Xeon v2, older i7)
&
\sphinxAtStartPar
Runs on \sphinxstylestrong{SSE2+}, AVX2 optional
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{GPU Support}
&
\sphinxAtStartPar
Excellent (PagedAttention, high throughput)
&
\sphinxAtStartPar
CUDA, Metal, Vulkan — full offloading
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{CPU Performance}
&
\sphinxAtStartPar
Poor without AVX\sphinxhyphen{}512
&
\sphinxAtStartPar
\sphinxstylestrong{Best\sphinxhyphen{}in\sphinxhyphen{}class} quantized inference
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{Binary Size}
&
\sphinxAtStartPar
\textasciitilde{}200 MB + Python deps
&
\sphinxAtStartPar
\sphinxstylestrong{\textless{} 10 MB} (statically linked)
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{Deployment}
&
\sphinxAtStartPar
Python server, complex deps
&
\sphinxAtStartPar
Single binary, \sphinxtitleref{scp} and run
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{Multi\sphinxhyphen{}user / API}
&
\sphinxAtStartPar
Built\sphinxhyphen{}in OpenAI API
&
\sphinxAtStartPar
\sphinxtitleref{server} binary with full OpenAI compat + web UI
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{Quantization Support}
&
\sphinxAtStartPar
FP16/BF16 only
&
\sphinxAtStartPar
Q4\_K, Q5\_K, Q8\_0, etc. — \sphinxstylestrong{4\textendash{}8 GB models fit in RAM}
\\
\sphinxbottomrule
\end{tabular}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\sphinxstylestrong{Key Finding}:

\sphinxAtStartPar
\textgreater{} \sphinxstylestrong{We cannot use vLLM} on our legacy Xeon v2 fleet due to missing \sphinxstylestrong{AVX\sphinxhyphen{}512}.
\textgreater{} \sphinxstylestrong{llama.cpp runs efficiently} on the \sphinxstylestrong{same hardware} with \sphinxstylestrong{Q4\_K\_M} models.


\bigskip\hrule\bigskip



\section{4. Why llama.cpp Is Our Production Choice}
\label{\detokenize{hybrid:why-llama-cpp-is-our-production-choice}}
\begin{sphinxadmonition}{note}{Decision}

\sphinxAtStartPar
\sphinxstylestrong{llama.cpp} is selected for \sphinxstylestrong{hybrid cloud LLM deployment} because:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Runs everywhere}: Old CPUs, new GPUs, laptops, edge

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Single static binary}: No Python, no CUDA runtime hell

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{GGUF format}: Share models with Ollama, local files, S3

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Built\sphinxhyphen{}in server}: OpenAI API + full web UI

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Thread \& context control}: \sphinxtitleref{\textendash{}threads}, \sphinxtitleref{\textendash{}ctx\sphinxhyphen{}size}, \sphinxtitleref{\textendash{}n\sphinxhyphen{}gpu\sphinxhyphen{}layers}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Kubernetes\sphinxhyphen{}ready}: Tiny image, fast startup

\end{itemize}
\end{sphinxadmonition}


\subsection{Example: Production\sphinxhyphen{}Ready Server}
\label{\detokenize{hybrid:example-production-ready-server}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
./llama.cpp/server\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}model\PYG{+w}{ }/models/llama3.1\PYGZhy{}8b\PYGZhy{}instruct.Q4\PYGZus{}K\PYGZus{}M.gguf\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}port\PYG{+w}{ }\PYG{l+m}{8080}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}host\PYG{+w}{ }\PYG{l+m}{0}.0.0.0\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}threads\PYG{+w}{ }\PYG{l+m}{16}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}ctx\PYGZhy{}size\PYG{+w}{ }\PYG{l+m}{8192}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}n\PYGZhy{}gpu\PYGZhy{}layers\PYG{+w}{ }\PYG{l+m}{0}\PYG{+w}{    }\PYG{c+c1}{\PYGZsh{} CPU\PYGZhy{}only on older nodes}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}log\PYGZhy{}disable
\end{sphinxVerbatim}

\sphinxAtStartPar
Deploy via Docker:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{FROM}\PYG{+w}{ }\PYG{l+s}{alpine:latest}
\PYG{k}{COPY}\PYG{+w}{ }llama.cpp/server\PYG{+w}{ }/usr/bin/
\PYG{k}{COPY}\PYG{+w}{ }models/*.gguf\PYG{+w}{ }/models/
\PYG{k}{EXPOSE}\PYG{+w}{ }\PYG{l+s}{8080}
\PYG{k}{CMD}\PYG{+w}{ }\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}server\PYGZdq{}}\PYG{p}{,}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}\PYGZhy{}\PYGZhy{}model\PYGZdq{}}\PYG{p}{,}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}/models/llama3.1\PYGZhy{}8b\PYGZhy{}instruct.Q4\PYGZus{}K\PYGZus{}M.gguf\PYGZdq{}}\PYG{p}{,}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}\PYGZhy{}\PYGZhy{}port\PYGZdq{}}\PYG{p}{,}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}8080\PYGZdq{}}\PYG{p}{]}
\end{sphinxVerbatim}


\bigskip\hrule\bigskip



\section{5. Migration Path: From Ollama \(\rightarrow\) llama.cpp}
\label{\detokenize{hybrid:migration-path-from-ollama-llama-cpp}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} 1. Reuse Ollama\PYGZsq{}s GGUF}
cp\PYG{+w}{ }\PYGZti{}/.ollama/models/blobs/sha256\PYGZhy{}*\PYG{+w}{ }/production/models/

\PYG{c+c1}{\PYGZsh{} 2. Deploy llama.cpp server}
kubectl\PYG{+w}{ }apply\PYG{+w}{ }\PYGZhy{}f\PYG{+w}{ }llama\PYGZhy{}cpp\PYGZhy{}deployment.yaml

\PYG{c+c1}{\PYGZsh{} 3. Point clients to new endpoint}
\PYG{n+nb}{export}\PYG{+w}{ }\PYG{n+nv}{OPENAI\PYGZus{}API\PYGZus{}BASE}\PYG{o}{=}http://llama\PYGZhy{}cpp\PYGZhy{}prod:8080/v1
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Zero model reconversion. Zero downtime.}


\bigskip\hrule\bigskip



\section{Summary}
\label{\detokenize{hybrid:summary}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Use Case
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Recommended Tool
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
Local dev / prototyping
&
\sphinxAtStartPar
\sphinxstylestrong{Ollama}
\\
\sphinxhline
\sphinxAtStartPar
Hybrid cloud, old hardware, scale
&
\sphinxAtStartPar
\sphinxstylestrong{llama.cpp}
\\
\sphinxhline
\sphinxAtStartPar
High\sphinxhyphen{}throughput GPU cluster
&
\sphinxAtStartPar
vLLM (if AVX\sphinxhyphen{}512 available)
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\textgreater{} \sphinxstylestrong{llama.cpp = the Swiss Army knife of LLM inference.}

\sphinxAtStartPar
—

\sphinxstepscope


\chapter{How InfiniFlow RAGFlow Uses gVisor}
\label{\detokenize{gvisor:how-infiniflow-ragflow-uses-gvisor}}\label{\detokenize{gvisor::doc}}
\sphinxAtStartPar
InfiniFlow RAGFlow is an open\sphinxhyphen{}source RAG (Retrieval\sphinxhyphen{}Augmented Generation) engine that supports document understanding and LLM orchestration. To enhance security when executing untrusted or user\sphinxhyphen{}provided code (e.g., Python agents, dynamic data processing scripts, or custom tool functions), RAGFlow runs certain components inside isolated sandboxes.


\section{gVisor Integration}
\label{\detokenize{gvisor:gvisor-integration}}
\sphinxAtStartPar
RAGFlow leverages \sphinxstylestrong{gVisor} as its primary sandboxing technology in containerized (Docker/Kubernetes) deployments.


\subsection{What is gVisor?}
\label{\detokenize{gvisor:what-is-gvisor}}
\sphinxAtStartPar
gVisor is an open\sphinxhyphen{}source project by Google that provides a user\sphinxhyphen{}space kernel (the Sentry component) and a lightweight virtual machine (runsc runtime). It intercepts and emulates Linux system calls instead of letting the container directly access the host kernel.

\sphinxAtStartPar
Key benefits of gVisor in a containerized environment:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Strong syscall isolation}: Only a limited, explicitly allowed set of syscalls reaches the host kernel. Unknown or dangerous syscalls are blocked or emulated in user space.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Reduced kernel attack surface}: Even if a process escapes the container’s namespaces/cgroups/seccomp filters, it still operates through gVisor’s restricted interface.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Compatibility with standard Docker}: gVisor integrates as an OCI\sphinxhyphen{}compatible runtime (\sphinxcode{\sphinxupquote{runsc}}), so no changes to Dockerfile or Kubernetes pod spec syntax are required beyond specifying the runtime.

\end{itemize}


\section{How RAGFlow Uses gVisor Sandboxes}
\label{\detokenize{gvisor:how-ragflow-uses-gvisor-sandboxes}}
\sphinxAtStartPar
When the sandbox feature is enabled (default in recent RAGFlow releases), the following happens:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Agent/Tool Execution Sandbox}
\sphinxhyphen{} Python\sphinxhyphen{}based agents and custom tools are executed inside a dedicated gVisor\sphinxhyphen{}powered container.
\sphinxhyphen{} The sandbox container has extremely limited privileges:
\begin{itemize}
\item {} 
\sphinxAtStartPar
No direct access to the host filesystem (only a small tmpfs and necessary code mounts as read\sphinxhyphen{}only).

\item {} 
\sphinxAtStartPar
Restricted network access (usually completely disabled or limited to internal RAGFlow services).

\item {} 
\sphinxAtStartPar
Dropped capabilities and a strict seccomp profile enforced by gVisor.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Runtime Selection}
\sphinxhyphen{} RAGFlow’s Docker Compose and Helm charts include a \sphinxcode{\sphinxupquote{ragflow\sphinxhyphen{}sandbox}} service that uses the \sphinxcode{\sphinxupquote{runsc}} runtime:
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nt}{runtime}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{runsc}
\PYG{n+nt}{runtime\PYGZhy{}config}\PYG{p}{:}
\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} Optional gVisor flags}
\PYG{+w}{  }\PYG{n+nt}{platform}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{ptrace}\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} or kvm on supported hardware}
\end{sphinxVerbatim}
\end{quote}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Communication}
\sphinxhyphen{} The main RAGFlow application communicates with the sandbox via gRPC or stdin/stdout (depending on version).
\sphinxhyphen{} Code and data are injected securely at container startup; no runtime file writes from inside the sandbox are allowed.

\end{enumerate}


\section{Security Advantages in Practice}
\label{\detokenize{gvisor:security-advantages-in-practice}}

\section{Enabling/Disabling the Sandbox}
\label{\detokenize{gvisor:enabling-disabling-the-sandbox}}
\sphinxAtStartPar
In \sphinxcode{\sphinxupquote{docker\sphinxhyphen{}compose.yml}} or Helm values:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nt}{services}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{ragflow}\PYG{p}{:}
\PYG{+w}{    }\PYG{n+nt}{environment}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{ENABLE\PYGZus{}SANDBOX=true}\PYG{+w}{   }\PYG{c+c1}{\PYGZsh{} default true in v0.10+}
\end{sphinxVerbatim}

\sphinxAtStartPar
To run without gVisor (less secure, for trusted environments only):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nt}{environment}\PYG{p}{:}
\PYG{+w}{  }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{ENABLE\PYGZus{}SANDBOX=false}
\end{sphinxVerbatim}


\section{Summary}
\label{\detokenize{gvisor:summary}}
\sphinxAtStartPar
By running untrusted code execution inside gVisor\sphinxhyphen{}powered containers, RAGFlow significantly reduces the risk of a compromised agent or malicious user\sphinxhyphen{}uploaded script affecting the host system or other services, while maintaining good performance and seamless integration with standard container orchestration tools.

\sphinxstepscope


\chapter{RAGFlow GPU vs CPU: Full Explanation (2025 Edition)}
\label{\detokenize{ragflow-gpu-cpu:ragflow-gpu-vs-cpu-full-explanation-2025-edition}}\label{\detokenize{ragflow-gpu-cpu::doc}}

\section{Why Does RAGFlow Still Need a GPU Even When Using Ollama?}
\label{\detokenize{ragflow-gpu-cpu:why-does-ragflow-still-need-a-gpu-even-when-using-ollama}}
\sphinxAtStartPar
You are absolutely right to ask this question.

\sphinxAtStartPar
Most people configure RAGFlow to use \sphinxstylestrong{external services} (Ollama, Xinference, vLLM, OpenAI, etc.) for:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Embedding model

\item {} 
\sphinxAtStartPar
Re\sphinxhyphen{}ranking model

\item {} 
\sphinxAtStartPar
Inference LLM (the actual answer generator)

\end{itemize}

\sphinxAtStartPar
Ollama runs these models \sphinxstylestrong{entirely on your GPU} — RAGFlow only sends HTTP requests.
So why does the official documentation and community still strongly recommend the \sphinxstylestrong{ragflow\sphinxhyphen{}gpu} image (or setting \sphinxcode{\sphinxupquote{DEVICE=gpu}})?

\sphinxAtStartPar
The answer is simple: \sphinxstylestrong{Deep Document Understanding (DeepDoc)} — the part that happens \sphinxstyleemphasis{before} any embedding or LLM call.


\section{Complete RAGFlow Pipeline (with GPU usage marked)}
\label{\detokenize{ragflow-gpu-cpu:complete-ragflow-pipeline-with-gpu-usage-marked}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabular}[t]{*{4}{\X{1}{4}}}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Step
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Component
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Runs inside RAGFlow?
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Uses GPU when?
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Document Upload

\end{enumerate}
&
\sphinxAtStartPar
DeepDoc parser
&
\sphinxAtStartPar
Yes (core of RAGFlow)
&
\sphinxAtStartPar
\sphinxstylestrong{YES} — heavily
\\
\sphinxhline\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{1}
\item {} 
\sphinxAtStartPar
Chunking

\end{enumerate}
&
\sphinxAtStartPar
Text splitting
&
\sphinxAtStartPar
Yes
&
\sphinxAtStartPar
No (pure CPU)
\\
\sphinxhline\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{2}
\item {} 
\sphinxAtStartPar
Embedding

\end{enumerate}
&
\sphinxAtStartPar
Sentence\sphinxhyphen{}Transformers, BGE…
&
\sphinxAtStartPar
External (Ollama, etc.)
&
\sphinxAtStartPar
GPU via Ollama/vLLM
\\
\sphinxhline\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{3}
\item {} 
\sphinxAtStartPar
Vector storage

\end{enumerate}
&
\sphinxAtStartPar
Elasticsearch / InfiniFlow DB
&
\sphinxAtStartPar
Yes
&
\sphinxAtStartPar
No
\\
\sphinxhline\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{4}
\item {} 
\sphinxAtStartPar
Retrieval

\end{enumerate}
&
\sphinxAtStartPar
Vector + keyword search
&
\sphinxAtStartPar
Yes
&
\sphinxAtStartPar
No
\\
\sphinxhline\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{5}
\item {} 
\sphinxAtStartPar
Re\sphinxhyphen{}ranking

\end{enumerate}
&
\sphinxAtStartPar
Cross\sphinxhyphen{}encoder (optional)
&
\sphinxAtStartPar
External or local
&
\sphinxAtStartPar
GPU via external service
\\
\sphinxhline\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{6}
\item {} 
\sphinxAtStartPar
Answer generation

\end{enumerate}
&
\sphinxAtStartPar
LLM (Llama 3, Qwen2, etc.)
&
\sphinxAtStartPar
External (Ollama, etc.)
&
\sphinxAtStartPar
GPU via Ollama/vLLM
\\
\sphinxbottomrule
\end{tabular}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\(\rightarrow\) The \sphinxstylestrong{only part that RAGFlow itself accelerates with GPU} is Step 1 — but it is by far the most compute\sphinxhyphen{}intensive for real\sphinxhyphen{}world documents.


\section{What DeepDoc Actually Does (and Why GPU Makes It 5\textendash{}20× Faster)}
\label{\detokenize{ragflow-gpu-cpu:what-deepdoc-actually-does-and-why-gpu-makes-it-520-faster}}
\sphinxAtStartPar
When you upload a PDF, scanned image, or complex report, DeepDoc performs these AI\sphinxhyphen{}heavy tasks:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Layout Detection}
Detects columns, headers, footers, reading order using CNN\sphinxhyphen{}based models (LayoutLM\sphinxhyphen{}style).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Table Structure Recognition (TSR)}
Identifies table boundaries, row/column spans, merged cells — extremely important for accurate retrieval.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Formula \& Math Recognition}
Converts LaTeX/math images into readable text.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Enhanced OCR}
For scanned PDFs: runs deep\sphinxhyphen{}learning OCR models (not just Tesseract CPU).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Visual Language Model Tasks} (charts, diagrams, screenshots)
Optionally calls lightweight VLMs (Qwen2\sphinxhyphen{}VL, LLaVA, etc.) to describe images inside the document.

\end{enumerate}

\sphinxAtStartPar
All of these run \sphinxstylestrong{inside RAGFlow’s deepdoc module} using PyTorch + CUDA when \sphinxcode{\sphinxupquote{DEVICE=gpu}} is enabled.


\section{Real\sphinxhyphen{}World Performance Numbers}
\label{\detokenize{ragflow-gpu-cpu:real-world-performance-numbers}}

\section{When Do You Actually Need ragflow\sphinxhyphen{}gpu?}
\label{\detokenize{ragflow-gpu-cpu:when-do-you-actually-need-ragflow-gpu}}
\sphinxAtStartPar
\sphinxstylestrong{YES \textendash{} You need it if your documents contain any of the following:}
\sphinxhyphen{} Scanned pages (images instead of selectable text)
\sphinxhyphen{} Complex tables or financial reports
\sphinxhyphen{} Charts, graphs, screenshots
\sphinxhyphen{} Mixed layouts (multi\sphinxhyphen{}column, sidebars, footnotes)
\sphinxhyphen{} Handwritten notes or formulas

\sphinxAtStartPar
\sphinxstylestrong{NO \textendash{} You can stay on ragflow\sphinxhyphen{}cpu if:}
\sphinxhyphen{} All documents are clean, born\sphinxhyphen{}digital text (Word \(\rightarrow\) PDF, Markdown, etc.)
\sphinxhyphen{} You only do quick prototypes with a few simple files
\sphinxhyphen{} You have no NVIDIA GPU available


\section{Recommended Setup in 2025 (Best of Both Worlds)}
\label{\detokenize{ragflow-gpu-cpu:recommended-setup-in-2025-best-of-both-worlds}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} .env file}
\PYG{n+nv}{DEVICE}\PYG{o}{=}gpu\PYG{+w}{                     }\PYG{c+c1}{\PYGZsh{} ← Enables DeepDoc GPU acceleration}
\PYG{n+nv}{SVR\PYGZus{}HTTP\PYGZus{}PORT}\PYG{o}{=}\PYG{l+m}{80}

\PYG{c+c1}{\PYGZsh{} Use Ollama (or vLLM) for embeddings + LLM}
\PYG{n+nv}{EMBEDDING\PYGZus{}MODEL}\PYG{o}{=}ollama/bge\PYGZhy{}large
\PYG{n+nv}{RERANK\PYGZus{}MODEL}\PYG{o}{=}reranker\PYG{+w}{  }\PYG{o}{(}this\PYG{+w}{ }cannnot\PYG{+w}{ }be\PYG{+w}{ }served\PYG{+w}{ }by\PYG{+w}{ }ollama,\PYG{+w}{ }vLLM\PYG{+w}{ }or\PYG{+w}{ }llama.cpp\PYG{+w}{ }is\PYG{+w}{ }needed\PYG{o}{)}
\PYG{n+nv}{LLM\PYGZus{}MODEL}\PYG{o}{=}ollama/llama3.1:70b
\end{sphinxVerbatim}

\sphinxAtStartPar
Result:
\sphinxhyphen{} DeepDoc parsing \(\rightarrow\) blazing fast on your NVIDIA GPU (inside RAGFlow container)
\sphinxhyphen{} Embeddings + LLM \(\rightarrow\) also blazing fast on the same GPU (inside Ollama container)
\sphinxhyphen{} No bottlenecks anywhere


\section{Monitoring \& Verification}
\label{\detokenize{ragflow-gpu-cpu:monitoring-verification}}
\sphinxAtStartPar
During document upload:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
watch\PYG{+w}{ }\PYGZhy{}n\PYG{+w}{ }\PYG{l+m}{1}\PYG{+w}{ }nvidia\PYGZhy{}smi
\end{sphinxVerbatim}

\sphinxAtStartPar
You will see:
\sphinxhyphen{} RAGFlow container using 4\textendash{}12 GB VRAM during parsing
\sphinxhyphen{} Ollama container using VRAM only when embedding or generatinging


\section{Conclusion}
\label{\detokenize{ragflow-gpu-cpu:conclusion}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{ragflow\sphinxhyphen{}cpu}  \(\rightarrow\) fine for clean text only

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{ragflow\sphinxhyphen{}gpu}  \(\rightarrow\) mandatory for real\sphinxhyphen{}world unstructured documents

\end{itemize}

\sphinxAtStartPar
Even if Ollama handles your LLM and embeddings perfectly, \sphinxstylestrong{without ragflow\sphinxhyphen{}gpu, the very first step (understanding the document) will be painfully slow or inaccurate}.

\sphinxAtStartPar
Use \sphinxcode{\sphinxupquote{DEVICE=gpu}} — it’s the difference between a toy and a true enterprise\sphinxhyphen{}grade RAG system.

\sphinxstepscope


\chapter{Upgrade to latest release :}
\label{\detokenize{upgrade:upgrade-to-latest-release}}\label{\detokenize{upgrade::doc}}
\sphinxAtStartPar
clone the github\sphinxhyphen{}repo : \sphinxurl{https://github.com/infiniflow/ragflow}
(git pull to get the latest)

\sphinxAtStartPar
in the docker directory :
\begin{quote}

\sphinxAtStartPar
sudo docker compose \sphinxhyphen{}f docker\sphinxhyphen{}compose.yml pull
sudo docker compose \sphinxhyphen{}f docker\sphinxhyphen{}compose.yml up \sphinxhyphen{}d
\end{quote}

\sphinxstepscope


\chapter{upload document}
\label{\detokenize{upload-document:upload-document}}\label{\detokenize{upload-document::doc}}
\sphinxAtStartPar
first get dataset\_id (0fbeb780b40d11f0bf690242ac170006)
\begin{quote}
\begin{description}
\sphinxlineitem{curl \textendash{}request POST }
\sphinxAtStartPar
\textendash{}url \sphinxurl{http://192.168.0.213/api/v1/datasets/0fbeb780b40d11f0bf690242ac170006/documents} \textendash{}header ‘Content\sphinxhyphen{}Type: multipart/form\sphinxhyphen{}data’ \textendash{}header ‘Authorization: Bearer ragflow\sphinxhyphen{}\_FdZQN68Q1NgLLeLmKe\sphinxhyphen{}3qbGHv15fqXg2AfJRENBIIw’ \textendash{}form ‘file=@./aai2228.pdf’

\end{description}
\end{quote}


\section{response}
\label{\detokenize{upload-document:response}}
\sphinxAtStartPar
\{“code”:0,”data”:{[}\{“chunk\_method”:”paper”,”created\_by”:”b61ff88eb40611f08d0f0242ac170006”,”dataset\_id”:”0fbeb780b40d11f0bf690242ac170006”,”id”:”67288092c92411f0953d0242ac170003”,”location”:”aai2228.pdf”,”name”:”aai2228.pdf”,”parser\_config”:\{“auto\_keywords”:0,”auto\_questions”:0,”chunk\_token\_num”:512,”delimiter”:”n”,”graphrag”:\{“entity\_types”:{[}“organization”,”person”,”geo”,”event”,”category”{]},”method”:”light”,”use\_graphrag”:true\},”html4excel”:false,”layout\_recognize”:”Docling”,”raptor”:\{“max\_cluster”:64,”max\_token”:256,”prompt”:”Please summarize the following paragraphs. Be careful with the numbers, do not make things up. Paragraphs as following:n      \{cluster\_content\}nThe above is the content you need to summarize.”,”random\_seed”:0,”scope”:”file”,”threshold”:0.1,”use\_raptor”:true\},”toc\_extraction”:false,”topn\_tags”:3\},”pipeline\_id”:””,”run”:”UNSTART”,”size”:2128247,”source\_type”:”local”,”suffix”:”pdf”,”thumbnail”:”thumbnail\_67288092c92411f0953d0242ac170003.png”,”type”:”pdf”\}{]}\}

\sphinxstepscope


\chapter{Graphrag}
\label{\detokenize{graphrag:graphrag}}\label{\detokenize{graphrag::doc}}\begin{description}
\sphinxlineitem{curl \textendash{}request POST }
\sphinxAtStartPar
\textendash{}url \sphinxurl{http:/}/\{address\}/api/v1/datasets/\{dataset\_id\}/run\_graphrag \textendash{}header ‘Authorization: Bearer \textless{}YOUR\_API\_KEY\textgreater{}’

\end{description}

\sphinxAtStartPar
\sphinxhyphen{}\textgreater{} get dataset ID
\begin{quote}

\sphinxAtStartPar
curl \sphinxhyphen{}H “Authorization: Bearer ragflow\sphinxhyphen{}\_FdZQN68Q1NgLLeLmKe\sphinxhyphen{}3qbGHv15fqXg2AfJRENBIIw”         \sphinxurl{http://192.168.0.213/api/v1/datasets}
\end{quote}


\section{reponse (json)}
\label{\detokenize{graphrag:reponse-json}}
\sphinxAtStartPar
\{“code”:0,”data”:{[}\{“avatar”:null,”chunk\_count”:698,”chunk\_method”:”paper”,”create\_date”:”Tue, 28 Oct 2025 22:47:44 GMT”,”create\_time”:1761662864024,”created\_by”:”b61ff88eb40611f08d0f0242ac170006”,”description”:”  “,”document\_count”:6,”embedding\_model”:”mxbai\sphinxhyphen{}embed\sphinxhyphen{}large:latest@Ollama”,”graphrag\_task\_finish\_at”:null,”graphrag\_task\_id”:null,”id”:”0fbeb780b40d11f0bf690242ac170006”,”language”:”English”,”mindmap\_task\_finish\_at”:null,”mindmap\_task\_id”:null,”name”:”testaankoop”,”pagerank”:15,”parser\_config”:\{“auto\_keywords”:5,”auto\_questions”:2,”chunk\_token\_num”:512,”delimiter”:”n”,”graphrag”:\{“community”:true,”entity\_types”:{[}“organization”,”person”,”geo”,”event”,”category”{]},”method”:”general”,”resolution”:true,”use\_graphrag”:true\},”html4excel”:false,”layout\_recognize”:”DeepDOC”,”raptor”:\{“max\_cluster”:64,”max\_token”:256,”prompt”:”Please summarize the following paragraphs. Be careful with the numbers, do not make things up. Paragraphs as following:n      \{cluster\_content\}nThe above is the content you need to summarize.”,”random\_seed”:0,”scope”:”file”,”threshold”:0.1,”use\_raptor”:true\},”tag\_kb\_ids”:{[}{]},”toc\_extraction”:false,”topn\_tags”:3\},”permission”:”me”,”pipeline\_id”:””,”raptor\_task\_finish\_at”:null,”raptor\_task\_id”:null,”similarity\_threshold”:0.2,”status”:”1”,”tenant\_id”:”b61ff88eb40611f08d0f0242ac170006”,”token\_num”:90496,”update\_date”:”Mon, 24 Nov 2025 23:09:25 GMT”,”update\_time”:1763996965365,”vector\_similarity\_weight”:0.3\}{]},”total\_datasets”:1\}

\sphinxAtStartPar
\textendash{}\textgreater{} we need: testaankoop
“id”:”0fbeb780b40d11f0bf690242ac170006”


\section{graphrag}
\label{\detokenize{graphrag:id1}}\begin{description}
\sphinxlineitem{curl \textendash{}request POST }
\sphinxAtStartPar
\textendash{}url \sphinxurl{http:/}/\{address\}/api/v1/datasets/\{dataset\_id\}/run\_graphrag \textendash{}header ‘Authorization: Bearer \textless{}YOUR\_API\_KEY\textgreater{}’

\sphinxlineitem{curl \textendash{}request POST }
\sphinxAtStartPar
\textendash{}url \sphinxurl{http://192.168.0.213/api/v1/datasets/0fbeb780b40d11f0bf690242ac170006/run\_graphrag} \textendash{}header “Authorization: Bearer ragflow\sphinxhyphen{}\_FdZQN68Q1NgLLeLmKe\sphinxhyphen{}3qbGHv15fqXg2AfJRENBIIw”

\end{description}

\sphinxAtStartPar
respons : \{“code”:0,”data”:\{“graphrag\_task\_id”:”9512c8e8c9e511f09b860242ac170003”\}\}

\sphinxAtStartPar
(very resource intensive!!!!!)

\sphinxAtStartPar
{[}GIN{]} 2025/11/25 \sphinxhyphen{} 10:12:59 | 200 |         8m10s |      172.17.0.1 | POST     “/api/chat”

\sphinxAtStartPar
{[}GIN{]} 2025/11/25 \sphinxhyphen{} 10:13:15 | 200 |         7m18s |      172.17.0.1 | POST     “/api/chat”

\sphinxAtStartPar
{[}GIN{]} 2025/11/25 \sphinxhyphen{} 10:13:57 | 200 |         6m56s |      172.17.0.1 | POST     “/api/chat”

\sphinxAtStartPar
{[}GIN{]} 2025/11/25 \sphinxhyphen{} 10:14:47 | 200 |          6m9s |      172.17.0.1 | POST     “/api/chat”


\subsection{{[}GIN{]} 2025/11/25 \sphinxhyphen{} 10:15:40 | 200 |         5m44s |      172.17.0.1 | POST     “/api/chat”}
\label{\detokenize{graphrag:gin-2025-11-25-10-15-40-200-5m44s-172-17-0-1-post-api-chat}}

\section{So slow, need to see progress}
\label{\detokenize{graphrag:so-slow-need-to-see-progress}}
\sphinxAtStartPar
check log docker container docker\sphinxhyphen{}ragflow\sphinxhyphen{}cpu\sphinxhyphen{}1:

\sphinxAtStartPar
2025\sphinxhyphen{}11\sphinxhyphen{}25 21:05:43,460 INFO     8082 task\_executor\_cedac6537903\_0 reported heartbeat: \{“ip\_address”: “172.23.0.3”, “pid”: 8082, “name”: “task\_executor\_cedac6537903\_0”, “now”: “2025\sphinxhyphen{}11\sphinxhyphen{}25T21:05:43.459+08:00”, “boot\_at”: “2025\sphinxhyphen{}11\sphinxhyphen{}25T16:43:07.370+08:00”, “pending”: 1, “lag”: 0, “done”: 0, “failed”: 0, “current”: \{“9512c8e8c9e511f09b860242ac170003”: \{“id”: “9512c8e8c9e511f09b860242ac170003”, “doc\_id”: “graph\_raptor\_x”, “from\_page”: 100000000, “to\_page”: 100000000, “retry\_count”: 0, “kb\_id”: “0fbeb780b40d11f0bf690242ac170006”, “parser\_id”: “paper”, “parser\_config”: \{“layout\_recognize”: “DeepDOC”, “chunk\_token\_num”: 512, “delimiter”: “n”, “auto\_keywords”: 0, “auto\_questions”: 0, “html4excel”: false, “topn\_tags”: 3, “raptor”: \{“use\_raptor”: true, “prompt”: “Please summarize the following paragraphs. Be careful with the numbers, do not make things up. Paragraphs as following:n      \{cluster\_content\}nThe above is the content you need to summarize.”, “max\_token”: 256, “threshold”: 0.1, “max\_cluster”: 64, “random\_seed”: 0\}, “graphrag”: \{“use\_graphrag”: true, “entity\_types”: {[}“organization”, “person”, “geo”, “event”, “category”{]}, “method”: “light”\}\}, “name”: “aai2225.pdf”, “type”: “pdf”, “location”: “aai2225.pdf”, “size”: 2893963, “tenant\_id”: “b61ff88eb40611f08d0f0242ac170006”, “language”: “English”, “embd\_id”: “mxbai\sphinxhyphen{}embed\sphinxhyphen{}large:latest@Ollama”, “pagerank”: 15, “kb\_parser\_config”: \{“layout\_recognize”: “DeepDOC”, “chunk\_token\_num”: 512, “delimiter”: “n”, “auto\_keywords”: 5, “auto\_questions”: 2, “html4excel”: false, “tag\_kb\_ids”: {[}{]}, “topn\_tags”: 3, “toc\_extraction”: false, “raptor”: \{“use\_raptor”: true, “prompt”: “Please summarize the following paragraphs. Be careful with the numbers, do not make things up. Paragraphs as following:n      \{cluster\_content\}nThe above is the content you need to summarize.”, “max\_token”: 256, “threshold”: 0.1, “max\_cluster”: 64, “random\_seed”: 0, “scope”: “file”\}, “graphrag”: \{“use\_graphrag”: true, “entity\_types”: {[}“organization”, “person”, “geo”, “event”, “category”{]}, “method”: “general”, “resolution”: true, “community”: true\}\}, “img2txt\_id”: “”, “asr\_id”: “”, “llm\_id”: “qwen2.5:14b@Ollama”, “update\_time”: 1764064833424, “doc\_ids”: {[}“2d7ce1c0b40d11f0aad70242ac170006”, “2dbc4db0b40d11f0aad70242ac170006”, “2df27a3eb40d11f0aad70242ac170006”, “2e46ad98b40d11f0aad70242ac170006”, “2ed60b1eb40d11f0aad70242ac170006”, “67288092c92411f0953d0242ac170003”{]}, “task\_type”: “graphrag”\}\}\}
2025\sphinxhyphen{}11\sphinxhyphen{}25 21:06:13,466 INFO     8082 task\_executor\_cedac6537903\_0 reported heartbeat: \{“ip\_address”: “172.23.0.3”, “pid”: 8082, “name”: “task\_executor\_cedac6537903\_0”, “now”: “2025\sphinxhyphen{}11\sphinxhyphen{}25T21:06:13.465+08:00”, “boot\_at”: “2025\sphinxhyphen{}11\sphinxhyphen{}25T16:43:07.370+08:00”, “pending”: 1, “lag”: 0, “done”: 0, “failed”: 0, “current”: \{“9512c8e8c9e511f09b860242ac170003”: \{“id”: “9512c8e8c9e511f09b860242ac170003”, “doc\_id”: “graph\_raptor\_x”, “from\_page”: 100000000, “to\_page”: 100000000, “retry\_count”: 0, “kb\_id”: “0fbeb780b40d11f0bf690242ac170006”, “parser\_id”: “paper”, “parser\_config”: \{“layout\_recognize”: “DeepDOC”, “chunk\_token\_num”: 512, “delimiter”: “n”, “auto\_keywords”: 0, “auto\_questions”: 0, “html4excel”: false, “topn\_tags”: 3, “raptor”: \{“use\_raptor”: true, “prompt”: “Please summarize the following paragraphs. Be careful with the numbers, do not make things up. Paragraphs as following:n      \{cluster\_content\}nThe above is the content you need to summarize.”, “max\_token”: 256, “threshold”: 0.1, “max\_cluster”: 64, “random\_seed”: 0\}, “graphrag”: \{“use\_graphrag”: true, “entity\_types”: {[}“organization”, “person”, “geo”, “event”, “category”{]}, “method”: “light”\}\}, “name”: “aai2225.pdf”, “type”: “pdf”, “location”: “aai2225.pdf”, “size”: 2893963, “tenant\_id”: “b61ff88eb40611f08d0f0242ac170006”, “language”: “English”, “embd\_id”: “mxbai\sphinxhyphen{}embed\sphinxhyphen{}large:latest@Ollama”, “pagerank”: 15, “kb\_parser\_config”: \{“layout\_recognize”: “DeepDOC”, “chunk\_token\_num”: 512, “delimiter”: “n”, “auto\_keywords”: 5, “auto\_questions”: 2, “html4excel”: false, “tag\_kb\_ids”: {[}{]}, “topn\_tags”: 3, “toc\_extraction”: false, “raptor”: \{“use\_raptor”: true, “prompt”: “Please summarize the following paragraphs. Be careful with the numbers, do not make things up. Paragraphs as following:n      \{cluster\_content\}nThe above is the content you need to summarize.”, “max\_token”: 256, “threshold”: 0.1, “max\_cluster”: 64, “random\_seed”: 0, “scope”: “file”\}, “graphrag”: \{“use\_graphrag”: true, “entity\_types”: {[}“organization”, “person”, “geo”, “event”, “category”{]}, “method”: “general”, “resolution”: true, “community”: true\}\}, “img2txt\_id”: “”, “asr\_id”: “”, “llm\_id”: “qwen2.5:14b@Ollama”, “update\_time”: 1764064833424, “doc\_ids”: {[}“2d7ce1c0b40d11f0aad70242ac170006”, “2dbc4db0b40d11f0aad70242ac170006”, “2df27a3eb40d11f0aad70242ac170006”, “2e46ad98b40d11f0aad70242ac170006”, “2ed60b1eb40d11f0aad70242ac170006”, “67288092c92411f0953d0242ac170003”{]}, “task\_type”: “graphrag”\}\}\}


\section{Stuck ???? what now????}
\label{\detokenize{graphrag:stuck-what-now}}
\sphinxAtStartPar
stopped the container ….

\sphinxAtStartPar
2025\sphinxhyphen{}11\sphinxhyphen{}25 21:18:25,668 INFO     36 task\_executor\_cedac6537903\_0 reported heartbeat: \{“ip\_address”: “172.23.0.3”, “pid”: 36, “name”: “task\_executor\_cedac6537903\_0”, “now”: “2025\sphinxhyphen{}11\sphinxhyphen{}25T21:18:25.667+08:00”, “boot\_at”: “2025\sphinxhyphen{}11\sphinxhyphen{}25T21:15:55.594+08:00”, “pending”: 0, “lag”: 0, “done”: 0, “failed”: 1, “current”: \{\}\}

\sphinxAtStartPar
(failed state task\_executor)

\sphinxstepscope


\chapter{Chat}
\label{\detokenize{chat:chat}}\label{\detokenize{chat::doc}}
\sphinxAtStartPar
do not forget the dataset!

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{chat}.png}
\end{figure}

\sphinxstepscope


\chapter{Why Infinity is a Good Alternative in RAGFlow}
\label{\detokenize{infinity:why-infinity-is-a-good-alternative-in-ragflow}}\label{\detokenize{infinity::doc}}

\section{Tailored for RAG/LLM Workloads}
\label{\detokenize{infinity:tailored-for-rag-llm-workloads}}
\sphinxAtStartPar
Infinity is an AI\sphinxhyphen{}native database built from the ground up for hybrid search (dense vectors, sparse vectors, multi\sphinxhyphen{}vectors/tensors, full\sphinxhyphen{}text, structured data) with fused ranking and reranking. It outperforms Elasticsearch in benchmarks for RAG\sphinxhyphen{}specific tasks (e.g., faster query latency \sphinxcode{\sphinxupquote{\textasciitilde{}0.1ms}} on million\sphinxhyphen{}scale vectors, higher QPS, better hybrid recall).


\section{Performance \& Efficiency}
\label{\detokenize{infinity:performance-efficiency}}
\sphinxAtStartPar
Claims several times faster than Elasticsearch for multi\sphinxhyphen{}recall scenarios, lower resource consumption, and advanced features like real\sphinxhyphen{}time search, better pruning for phrase queries, and native support for RAG needs (e.g., tensor\sphinxhyphen{}based reranking).


\section{Integration in RAGFlow}
\label{\detokenize{infinity:integration-in-ragflow}}
\sphinxAtStartPar
Introduced as an option in v0.14 (late 2024), with ongoing improvements. RAGFlow docs and blog state Infinity will become the preferred/default engine once fully mature, as it’s more powerful for their use case. Switching is simple (set \sphinxcode{\sphinxupquote{DOC\_ENGINE=infinity}} in \sphinxcode{\sphinxupquote{.env}}), and it replaces Elasticsearch entirely for storage/retrieval.


\section{Fully Open Source \& Vendor\sphinxhyphen{}Neutral}
\label{\detokenize{infinity:fully-open-source-vendor-neutral}}
\sphinxAtStartPar
Apache 2.0\sphinxhyphen{}licensed, no licensing drama, developed openly by InfiniFlow.


\section{Drawbacks}
\label{\detokenize{infinity:drawbacks}}
\sphinxAtStartPar
Newer and less battle\sphinxhyphen{}tested than Elasticsearch’s ecosystem (e.g., fewer plugins, tools like Kibana). Some early RAGFlow users reported minor integration bugs, but these have been fixed in recent releases.

\sphinxstepscope


\chapter{MinerU and Its Use in RAGFlow}
\label{\detokenize{minerU:mineru-and-its-use-in-ragflow}}\label{\detokenize{minerU::doc}}
\sphinxAtStartPar
test : \sphinxurl{https://huggingface.co/spaces/opendatalab/MinerU}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker\PYG{+w}{ }build\PYG{+w}{ }\PYGZhy{}t\PYG{+w}{ }mineru:latest\PYG{+w}{ }\PYGZhy{}f\PYG{+w}{ }Dockerfile\PYG{+w}{ }.\PYG{+w}{   }\PYG{o}{(}34GB\PYG{+w}{ }\PYG{k}{in}\PYG{+w}{ }size!!\PYG{o}{)}
docker\PYG{+w}{ }run\PYG{+w}{ }\PYGZhy{}\PYGZhy{}gpus\PYG{+w}{ }all\PYG{+w}{   }\PYGZhy{}\PYGZhy{}shm\PYGZhy{}size\PYG{+w}{ }32g\PYG{+w}{   }\PYGZhy{}p\PYG{+w}{ }\PYG{l+m}{30000}:30000\PYG{+w}{ }\PYGZhy{}p\PYG{+w}{ }\PYG{l+m}{7860}:7860\PYG{+w}{ }\PYGZhy{}p\PYG{+w}{ }\PYG{l+m}{8003}:8003\PYG{+w}{   }\PYGZhy{}\PYGZhy{}ipc\PYG{o}{=}host\PYG{+w}{   }\PYGZhy{}it\PYG{+w}{ }mineru:latest\PYG{+w}{ }/bin/bash\PYG{+w}{ }\PYG{o}{(}normally\PYG{+w}{ }with\PYG{+w}{ }port\PYG{+w}{ }\PYG{l+m}{8000},\PYG{+w}{ }but\PYG{+w}{ }I\PYG{+w}{ }used\PYG{+w}{ }\PYG{l+m}{8003}\PYG{o}{)}
\end{sphinxVerbatim}


\section{Introduction to MinerU}
\label{\detokenize{minerU:introduction-to-mineru}}
\sphinxAtStartPar
MinerU is an open\sphinxhyphen{}source tool developed by OpenDataLab (Shanghai AI
Laboratory) for converting complex PDF documents into machine\sphinxhyphen{}readable
formats, such as Markdown or JSON. It excels at extracting text, tables
(in HTML or LaTeX), mathematical formulas (in LaTeX), images (with
captions), and preserving document structure, including headings,
paragraphs, lists, and reading order for multi\sphinxhyphen{}column layouts.

\sphinxAtStartPar
Key features include:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Removal of noise elements like headers, footers, footnotes, and page
numbers.

\item {} 
\sphinxAtStartPar
Support for scanned PDFs via OCR (PaddleOCR, multilingual with over
80 languages).

\item {} 
\sphinxAtStartPar
Handling of complex layouts, including scientific literature with
symbols and equations.

\item {} 
\sphinxAtStartPar
Multiple backends: pipeline (rule\sphinxhyphen{}based, CPU\sphinxhyphen{}friendly), VLM\sphinxhyphen{}based
(vision\sphinxhyphen{}language models for higher accuracy, often GPU\sphinxhyphen{}accelerated),
and hybrid modes.

\item {} 
\sphinxAtStartPar
Built on PDF\sphinxhyphen{}Extract\sphinxhyphen{}Kit models for layout detection, table
recognition, and formula parsing.

\item {} 
\sphinxAtStartPar
AGPL\sphinxhyphen{}3.0 license.

\end{itemize}

\sphinxAtStartPar
MinerU is particularly suited for preparing documents for LLM workflows,
such as Retrieval\sphinxhyphen{}Augmented Generation (RAG), due to its structured,
clean output that minimizes hallucinations in downstream tasks. Recent
versions (e.g., MinerU 2.5) achieve state\sphinxhyphen{}of\sphinxhyphen{}the\sphinxhyphen{}art performance on
benchmarks like OmniDocBench.


\section{MinerU in RAGFlow}
\label{\detokenize{minerU:mineru-in-ragflow}}
\sphinxAtStartPar
RAGFlow is an open\sphinxhyphen{}source RAG engine focused on deep document
understanding, supporting complex data ingestion for accurate
question\sphinxhyphen{}answering with citations.

\sphinxAtStartPar
MinerU integration was introduced in RAGFlow v0.22.0 (released October
2025), supporting MinerU \textgreater{}= 2.6.3. RAGFlow acts solely as a \sphinxstylestrong{client}
to MinerU:
\begin{itemize}
\item {} 
\sphinxAtStartPar
RAGFlow calls MinerU to parse uploaded PDFs.

\item {} 
\sphinxAtStartPar
MinerU processes the file and outputs structured data (e.g.,
JSON/Markdown with images and tables).

\item {} 
\sphinxAtStartPar
RAGFlow reads the output and proceeds with chunking, embedding, and
indexing.

\end{itemize}

\sphinxAtStartPar
Configuration options:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Enable via \sphinxcode{\sphinxupquote{USE\_MINERU=true}} in Docker/.env or manual environment
variables.

\item {} 
\sphinxAtStartPar
Select MinerU in the dataset configuration UI under “PDF parser” (for
built\sphinxhyphen{}in pipelines) or in the Parser component (for custom
pipelines).

\item {} 
\sphinxAtStartPar
Supports remote MinerU API deployment (e.g., via vLLM backend for GPU
offloading, decoupling from RAGFlow’s CPU\sphinxhyphen{}only server).

\item {} 
\sphinxAtStartPar
Alongside other parsers like DeepDoc (RAGFlow’s default VLM), Naive
(text\sphinxhyphen{}only), and Docling.

\end{itemize}

\sphinxAtStartPar
This integration leverages MinerU’s superior handling of complex PDFs
(e.g., tables, formulas in academic/technical documents) to improve
retrieval quality in RAGFlow\sphinxhyphen{}based applications.


\section{Comparison with Existing PDF Ingestion Tools}
\label{\detokenize{minerU:comparison-with-existing-pdf-ingestion-tools}}
\sphinxAtStartPar
Common PDF ingestion tools for RAG include Unstructured.io, LlamaParse
(LlamaIndex), Docling, Marker, and traditional libraries like PyMuPDF.
As of early 2026, MinerU frequently ranks among the top open\sphinxhyphen{}source
options in benchmarks for complex PDFs, especially scientific/technical
ones with tables and formulas.


\section{Summary of MinerU Advantages}
\label{\detokenize{minerU:summary-of-mineru-advantages}}\begin{itemize}
\item {} 
\sphinxAtStartPar
High performance in 2025\textendash{}2026 benchmarks (e.g., top scores in table
recognition, formula parsing, and layout accuracy on complex docs).

\item {} 
\sphinxAtStartPar
Superior to Unstructured for structured scientific output; often
comparable to or better than LlamaParse in open\sphinxhyphen{}source/local setups.

\item {} 
\sphinxAtStartPar
In RAGFlow, it complements or outperforms the default DeepDoc parser
for challenging PDFs requiring top\sphinxhyphen{}tier layout/table handling.

\end{itemize}

\sphinxAtStartPar
For most RAG pipelines in 2026, MinerU is a leading open\sphinxhyphen{}source choice
for difficult PDFs, particularly when integrated into frameworks like
RAGFlow.


\section{Activating MinerU in RagFlow}
\label{\detokenize{minerU:activating-mineru-in-ragflow}}
\sphinxAtStartPar
in the dataset select in the configuration / ingestion pipeline / pdf parser \sphinxhyphen{}\textgreater{} mineru\sphinxhyphen{}from\sphinxhyphen{}env\sphinxhyphen{}1 Experimental

\sphinxAtStartPar
adapt the .env settings :
\# Enable Mineru
\# Uncommenting these lines will automatically add MinerU to the model provider whenever possible.
\# More details see \sphinxurl{https://ragflow.io/docs/faq\#how-to-use-mineru-to-parse-pdf-documents}.
MINERU\_APISERVER=http://host.docker.internal:8003
MINERU\_DELETE\_OUTPUT=0   \# keep output directory
MINERU\_BACKEND=pipeline  \# or another backend you prefer

\sphinxstepscope


\chapter{What is Agent context engine?}
\label{\detokenize{agent-context-engine:what-is-agent-context-engine}}\label{\detokenize{agent-context-engine::doc}}
\sphinxAtStartPar
From 2025, a silent revolution began beneath the dazzling surface of AI Agents. While the world marveled at agents that could write code, analyze data, and automate workflows, a fundamental bottleneck emerged: why do even the most advanced agents still stumble on simple questions, forget previous conversations, or misuse available tools?

\sphinxAtStartPar
The answer lies not in the intelligence of the Large Language Model (LLM) itself, but in the quality of the Context it receives. An LLM, no matter how powerful, is only as good as the information we feed it. Today’s cutting\sphinxhyphen{}edge agents are often crippled by a cumbersome, manual, and error\sphinxhyphen{}prone process of context assembly—a process known as Context Engineering.

\sphinxAtStartPar
This is where the Agent Context Engine comes in. It is not merely an incremental improvement but a foundational shift, representing the evolution of RAG from a singular technique into the core data and intelligence substrate for the entire Agent ecosystem.


\section{Beyond the hype: The reality of today’s “intelligent” Agents}
\label{\detokenize{agent-context-engine:beyond-the-hype-the-reality-of-today-s-intelligent-agents}}
\sphinxAtStartPar
Today, the “intelligence” behind most AI Agents hides a mountain of human labor. Developers must:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Hand\sphinxhyphen{}craft elaborate prompt templates

\item {} 
\sphinxAtStartPar
Hard\sphinxhyphen{}code document\sphinxhyphen{}retrieval logic for every task

\item {} 
\sphinxAtStartPar
Juggle tool descriptions, conversation history, and knowledge snippets inside a tiny context window

\item {} 
\sphinxAtStartPar
Repeat the whole process for each new scenario

\end{itemize}

\sphinxAtStartPar
This pattern is called Context Engineering. It is deeply tied to expert know\sphinxhyphen{}how, almost impossible to scale, and prohibitively expensive to maintain. When an enterprise needs to keep dozens of distinct agents alive, the artisanal workshop model collapses under its own weight.

\sphinxAtStartPar
The mission of an Agent Context Engine is to turn Context Engineering from an “art” into an industrial\sphinxhyphen{}grade science.


\section{Deconstructing the Agent Context Engine}
\label{\detokenize{agent-context-engine:deconstructing-the-agent-context-engine}}
\sphinxAtStartPar
So, what exactly is an Agent Context Engine? It is a unified, intelligent, and automated platform responsible for the end\sphinxhyphen{}to\sphinxhyphen{}end process of assembling the optimal context for an LLM or Agent at the moment of inference. It moves from artisanal crafting to industrialized production.

\sphinxAtStartPar
At its core, an Agent Context Engine is built on a triumvirate of next\sphinxhyphen{}generation retrieval capabilities, seamlessly integrated into a single service layer:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
The Knowledge Core (Advanced RAG): This is the evolution of traditional RAG. It moves beyond simple chunk\sphinxhyphen{}and\sphinxhyphen{}embed to intelligently process static, private enterprise knowledge. Techniques like TreeRAG (building LLM\sphinxhyphen{}generated document outlines for “locate\sphinxhyphen{}then\sphinxhyphen{}expand” retrieval) and GraphRAG (extracting entity networks to find semantically distant connections) work to close the “semantic gap.” The engine’s Ingestion Pipeline acts as the ETL for unstructured data, parsing multi\sphinxhyphen{}format documents and using LLMs to enrich content with summaries, metadata, and structure before indexing.

\item {} 
\sphinxAtStartPar
The Memory Layer: An Agent’s intelligence is defined by its ability to learn from interaction. The Memory Layer is a specialized retrieval system for dynamic, episodic data: conversation history, user preferences, and the agent’s own internal state (e.g., “waiting for human input”). It manages the lifecycle of this data—storing raw dialogue, triggering summarization into semantic memory, and retrieving relevant past interactions to provide continuity and personalization. Technologically, it is a close sibling to RAG, but focused on a temporal stream of data.

\item {} 
\sphinxAtStartPar
The Tool Orchestrator: As MCP (Model Context Protocol) enables the connection of hundreds of internal services as tools, a new problem arises: tool selection. The Context Engine solves this with Tool Retrieval. Instead of dumping all tool descriptions into the prompt, it maintains an index of tools and—critically—an index of Playbooks or Guidelines (best practices on when and how to use tools). For a given task, it retrieves only the most relevant tools and instructions, transforming the LLM’s job from “searching a haystack” to “following a recipe.”

\end{enumerate}


\section{Why we need a dedicated engine? The case for a unified substrate}
\label{\detokenize{agent-context-engine:why-we-need-a-dedicated-engine-the-case-for-a-unified-substrate}}
\sphinxAtStartPar
The necessity of an Agent Context Engine becomes clear when we examine the alternative: siloed, manually wired components.
\begin{itemize}
\item {} 
\sphinxAtStartPar
The Data Silo Problem: Knowledge, memory, and tools reside in separate systems, requiring complex integration for each new agent.

\item {} 
\sphinxAtStartPar
The Assembly Line Bottleneck: Developers spend more time on context plumbing than on agent logic, slowing innovation to a crawl.

\item {} 
\sphinxAtStartPar
The “Context Ownership” Dilemma: In manually engineered systems, context logic is buried in code, owned by developers, and opaque to business users. An Engine makes context a configurable, observable, and customer\sphinxhyphen{}owned asset.

\end{itemize}

\sphinxAtStartPar
The shift from Context Engineering to a Context Platform/Engine marks the maturation of enterprise AI, as summarized in the table below:


\section{RAGFlow: A resolute march toward the context engine of Agents}
\label{\detokenize{agent-context-engine:ragflow-a-resolute-march-toward-the-context-engine-of-agents}}
\sphinxAtStartPar
This is the future RAGFlow is forging.

\sphinxAtStartPar
We left behind the label of “yet another RAG system” long ago. From DeepDoc—our deeply\sphinxhyphen{}optimized, multimodal document parser—to the bleeding\sphinxhyphen{}edge architectures that bridge semantic chasms in complex RAG scenarios, all the way to a full\sphinxhyphen{}blown, enterprise\sphinxhyphen{}grade ingestion pipeline, every evolutionary step RAGFlow takes is a deliberate stride toward the ultimate form: an Agentic Context Engine.

\sphinxAtStartPar
We believe tomorrow’s enterprise AI advantage will hinge not on who owns the largest model, but on who can feed that model the highest\sphinxhyphen{}quality, most real\sphinxhyphen{}time, and most relevant context. An Agentic Context Engine is the critical infrastructure that turns this vision into reality.

\sphinxAtStartPar
In the paradigm shift from “hand\sphinxhyphen{}crafted prompts” to “intelligent context,” RAGFlow is determined to be the most steadfast propeller and enabler. We invite every developer, enterprise, and researcher who cares about the future of AI agents to follow RAGFlow’s journey—so together we can witness and build the cornerstone of the next\sphinxhyphen{}generation AI stack.

\sphinxstepscope


\chapter{Using SearXNG with RAGFlow}
\label{\detokenize{searxng:using-searxng-with-ragflow}}\label{\detokenize{searxng::doc}}

\section{Introduction to SearXNG in RAGFlow Agents}
\label{\detokenize{searxng:introduction-to-searxng-in-ragflow-agents}}
\sphinxAtStartPar
RAGFlow provides powerful \sphinxstylestrong{agent capabilities}, including built\sphinxhyphen{}in web
search tools for real\sphinxhyphen{}time information retrieval during reasoning or
chat sessions. The default web search often relies on cloud providers
like Tavily, which require API keys and may incur costs.

\sphinxAtStartPar
\sphinxstylestrong{SearXNG} serves as a privacy\sphinxhyphen{}focused, open\sphinxhyphen{}source metasearch engine
that aggregates results from multiple sources (e.g., Google, Bing,
DuckDuckGo) without tracking users. Deploying SearXNG locally offers a
fully self\sphinxhyphen{}hosted alternative, ideal for enterprises prioritizing data
privacy, avoiding external API dependencies, or operating in restricted
networks.

\sphinxAtStartPar
Although RAGFlow does not yet include native SearXNG support in its UI
(as of early 2026 releases), users can integrate it easily via \sphinxstylestrong{custom
agent tools} or by pointing the web search component to a local SearXNG
instance. This approach mirrors integrations in similar open\sphinxhyphen{}source
tools like Open WebUI, AnythingLLM, and Perplexica.


\section{Benefits of Using SearXNG with RAGFlow}
\label{\detokenize{searxng:benefits-of-using-searxng-with-ragflow}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Full Privacy and Control} — All searches stay within your
infrastructure. No data leaks to third\sphinxhyphen{}party APIs, making it perfect
for sensitive or regulated environments.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{No Costs or Rate Limits} — Unlike Tavily or SerpAPI, SearXNG runs
free with unlimited queries (limited only by your hardware).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Customizable Search Engines} — Configure SearXNG to use specific
engines, add restrictions, or focus on reliable sources.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hybrid Real\sphinxhyphen{}Time Retrieval} — Agents combine internal knowledge
bases (private documents) with fresh web results, reducing
hallucinations on current events or external facts.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Intranet Compatibility} — Deploy SearXNG in air\sphinxhyphen{}gapped or
intranet\sphinxhyphen{}only setups. For purely internal “browsing,” configure it to
search indexed intranet pages (via custom engines or plugins) or pair
it with RAGFlow’s built\sphinxhyphen{}in web page ingestion for static internal
sites.

\end{itemize}


\section{Benefits for Intranet Page Browsing}
\label{\detokenize{searxng:benefits-for-intranet-page-browsing}}
\sphinxAtStartPar
RAGFlow excels at ingesting and querying private/internal documents, but
dynamic intranet pages (e.g., wikis, dashboards) may require real\sphinxhyphen{}time
access.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{With Internet Access}: SearXNG provides standard web search while
keeping queries internal to your server.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Without Internet (Pure Intranet)}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Use RAGFlow’s native web crawling/ingestion to periodically index
intranet URLs into knowledge bases.

\item {} 
\sphinxAtStartPar
Configure SearXNG with custom “site:” restrictions or internal
search engines (e.g., via plugins for mediawiki or confluence) to
query only intranet domains.

\item {} 
\sphinxAtStartPar
This creates a “local web search” experience where agents retrieve
up\sphinxhyphen{}to\sphinxhyphen{}date content from internal sites without exposing data
externally.

\end{itemize}

\end{itemize}


\section{Deployment in Docker Containers}
\label{\detokenize{searxng:deployment-in-docker-containers}}
\sphinxAtStartPar
Both RAGFlow and SearXNG deploy easily with Docker, often on the same
network for seamless communication.


\subsection{1. Deploy SearXNG}
\label{\detokenize{searxng:deploy-searxng}}
\sphinxAtStartPar
Use the official \sphinxcode{\sphinxupquote{searxng/searxng}} image or the dedicated docker repo.

\sphinxAtStartPar
Example \sphinxcode{\sphinxupquote{docker\sphinxhyphen{}compose.yml}} for SearXNG:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
version: ‘3’ services: searxng: image: searxng/searxng:latest
container\_name: searxng ports: \sphinxhyphen{} “8080:8080” volumes: \sphinxhyphen{}
./searxng:/etc/searxng environment: \sphinxhyphen{} BASE\_URL=http://localhost:8080
restart: unless\sphinxhyphen{}stopped

\sphinxAtStartPar
Key configuration in \sphinxcode{\sphinxupquote{searxng/settings.yml}} (mounted volume):

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

\sphinxAtStartPar
search: formats: \sphinxhyphen{} html \sphinxhyphen{} json \# Required for API/tool calls server:
limiter: false \# Disable rate limiting for agent use secret\_key:
your\_strong\_secret

\sphinxAtStartPar
Run: \sphinxcode{\sphinxupquote{docker compose up \sphinxhyphen{}d}}


\subsection{2. Integrate with RAGFlow}
\label{\detokenize{searxng:integrate-with-ragflow}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Deploy RAGFlow normally (via its official docker\sphinxhyphen{}compose.yml).

\item {} 
\sphinxAtStartPar
Place both in the same Docker network or use \sphinxcode{\sphinxupquote{host}} networking.

\item {} 
\sphinxAtStartPar
In RAGFlow’s Agent builder:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Create a custom tool that calls your local SearXNG API
(\sphinxcode{\sphinxupquote{http://searxng:8080/search?q=\{query\}\&format=json}}).

\item {} 
\sphinxAtStartPar
Or, if using the built\sphinxhyphen{}in Websearch Agent, configure it (via env
vars or custom code) to route to your SearXNG endpoint instead of
Tavily.

\end{itemize}

\item {} 
\sphinxAtStartPar
Many users report success with similar custom integrations in agent
frameworks.

\end{itemize}


\subsection{Combined Example Network}
\label{\detokenize{searxng:combined-example-network}}
\sphinxAtStartPar
Extend RAGFlow’s compose file to include SearXNG:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nt}{services}\PYG{p}{:}\PYG{+w}{ }\PYG{c+c1}{\PYGZsh{} … existing RAGFlow services … searxng: image:}
\PYG{n+nt}{searxng/searxng:latest ports: \PYGZhy{} “8080:8080” volumes}\PYG{p}{:}\PYG{+w}{ }\PYG{err}{\PYGZhy{}}
\PYG{n+nt}{./searxng:/etc/searxng networks}\PYG{p}{:}\PYG{+w}{ }\PYG{err}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{ragflow\PYGZus{}network}
\end{sphinxVerbatim}

\sphinxAtStartPar
This setup keeps everything containerized, secure, and scalable.

\sphinxAtStartPar
example on homelab

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nt}{services}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{caddy}\PYG{p}{:}
\PYG{+w}{    }\PYG{n+nt}{container\PYGZus{}name}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{caddy}
\PYG{+w}{    }\PYG{n+nt}{image}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{docker.io/library/caddy:2\PYGZhy{}alpine}
\PYG{+w}{    }\PYG{n+nt}{network\PYGZus{}mode}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{host}
\PYG{+w}{    }\PYG{n+nt}{restart}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{unless\PYGZhy{}stopped}
\PYG{+w}{    }\PYG{n+nt}{volumes}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{./Caddyfile:/etc/caddy/Caddyfile:ro}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{caddy\PYGZhy{}data:/data:rw}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{caddy\PYGZhy{}config:/config:rw}
\PYG{+w}{    }\PYG{n+nt}{environment}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{SEARXNG\PYGZus{}HOSTNAME=\PYGZdl{}\PYGZob{}SEARXNG\PYGZus{}HOSTNAME:\PYGZhy{}http://192.168.0.213\PYGZcb{}}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{SEARXNG\PYGZus{}TLS=\PYGZdl{}\PYGZob{}LETSENCRYPT\PYGZus{}EMAIL:\PYGZhy{}internal\PYGZcb{}}
\PYG{+w}{    }\PYG{n+nt}{logging}\PYG{p}{:}
\PYG{+w}{      }\PYG{n+nt}{driver}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{json\PYGZhy{}file}\PYG{l+s}{\PYGZdq{}}
\PYG{+w}{      }\PYG{n+nt}{options}\PYG{p}{:}
\PYG{+w}{        }\PYG{n+nt}{max\PYGZhy{}size}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{1m}\PYG{l+s}{\PYGZdq{}}
\PYG{+w}{        }\PYG{n+nt}{max\PYGZhy{}file}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{1}\PYG{l+s}{\PYGZdq{}}

\PYG{+w}{  }\PYG{n+nt}{searxng}\PYG{p}{:}
\PYG{+w}{    }\PYG{n+nt}{container\PYGZus{}name}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{searxng}
\PYG{+w}{    }\PYG{n+nt}{image}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{docker.io/searxng/searxng:latest}
\PYG{+w}{    }\PYG{n+nt}{restart}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{unless\PYGZhy{}stopped}
\PYG{+w}{    }\PYG{n+nt}{networks}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{searxng}
\PYG{+w}{    }\PYG{n+nt}{ports}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{192.168.0.213:8080:8080}\PYG{l+s}{\PYGZdq{}}
\PYG{+w}{    }\PYG{n+nt}{volumes}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{./searxng:/etc/searxng:rw}
\PYG{+w}{    }\PYG{n+nt}{environment}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{SEARXNG\PYGZus{}BASE\PYGZus{}URL=https://\PYGZdl{}\PYGZob{}SEARXNG\PYGZus{}HOSTNAME:\PYGZhy{}192.168.0.213\PYGZcb{}/}
\PYG{+w}{    }\PYG{n+nt}{depends\PYGZus{}on}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{caddy}
\PYG{+w}{    }\PYG{n+nt}{logging}\PYG{p}{:}
\PYG{+w}{      }\PYG{n+nt}{driver}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{json\PYGZhy{}file}\PYG{l+s}{\PYGZdq{}}
\PYG{+w}{      }\PYG{n+nt}{options}\PYG{p}{:}
\PYG{+w}{        }\PYG{n+nt}{max\PYGZhy{}size}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{1m}\PYG{l+s}{\PYGZdq{}}
\PYG{+w}{        }\PYG{n+nt}{max\PYGZhy{}file}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{1}\PYG{l+s}{\PYGZdq{}}
\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{networks:}
\sphinxAtStartPar
searxng:

\sphinxlineitem{volumes:}
\sphinxAtStartPar
caddy\sphinxhyphen{}data:
caddy\sphinxhyphen{}config:

\end{description}


\section{Conclusion}
\label{\detokenize{searxng:conclusion}}
\sphinxAtStartPar
Integrating SearXNG with RAGFlow delivers a powerful, private
alternative to cloud\sphinxhyphen{}based web search in agents—especially valuable for
intranet deployments where data sovereignty matters. While awaiting
potential native support, the custom tool approach works reliably and
aligns with RAGFlow’s extensible design.

\sphinxAtStartPar
For the latest updates, monitor RAGFlow’s GitHub issues/releases or
community discussions. This combination empowers fully self\sphinxhyphen{}hosted,
real\sphinxhyphen{}time augmented agents without compromising privacy.

\sphinxstepscope


\chapter{homelab}
\label{\detokenize{homelab:homelab}}\label{\detokenize{homelab::doc}}

\section{problem older hardware :}
\label{\detokenize{homelab:problem-older-hardware}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Xeon E5\sphinxhyphen{}2690 v2

\item {} 
\sphinxAtStartPar
Tesla P100 16GB

\item {} 
\sphinxAtStartPar
Driver CUDA 11.8

\end{itemize}


\section{software :}
\label{\detokenize{homelab:software}}\begin{itemize}
\item {} 
\sphinxAtStartPar
CUDA 12.8

\item {} 
\sphinxAtStartPar
AVX512 or AV2 (\textgreater{} xeon v3

\end{itemize}


\section{problem reranking:}
\label{\detokenize{homelab:problem-reranking}}\begin{itemize}
\item {} 
\sphinxAtStartPar
compiled llama.cpp and gguf model which works OK

\item {} 
\sphinxAtStartPar
unfortunately not recognised in infiniflow/ragflow (not anymore)

\end{itemize}


\section{possible solution:}
\label{\detokenize{homelab:possible-solution}}
\sphinxAtStartPar
installing xinference (cpu version) (dockercontainer for GPU too big and CUDA 12.8)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker\PYG{+w}{ }run\PYG{+w}{ }\PYGZhy{}d\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}name\PYG{+w}{ }xinference\PYGZhy{}cpu\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}p\PYG{+w}{ }\PYG{l+m}{9997}:9997\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}shm\PYGZhy{}size\PYG{o}{=}8g\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}restart\PYG{+w}{ }unless\PYGZhy{}stopped\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}v\PYG{+w}{ }/home/jan/models:/models\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }xprobe/xinference:latest\PYGZhy{}cpu\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }xinference\PYGZhy{}local\PYG{+w}{ }\PYGZhy{}H\PYG{+w}{ }\PYG{l+m}{0}.0.0.0
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker\PYG{+w}{ }\PYG{n+nb}{exec}\PYG{+w}{ }xinference\PYGZhy{}cpu\PYG{+w}{ }xinference\PYG{+w}{ }launch\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}model\PYGZhy{}name\PYG{+w}{ }qwen3\PYGZhy{}reranker\PYGZhy{}4b\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}model\PYGZhy{}format\PYG{+w}{ }gguf\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}model\PYGZhy{}uri\PYG{+w}{ }/models/Qwen3\PYGZhy{}Reranker\PYGZhy{}4B\PYGZhy{}q5\PYGZus{}k\PYGZus{}m.gguf\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}model\PYGZhy{}type\PYG{+w}{ }rerank
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker\PYG{+w}{ }\PYG{n+nb}{exec}\PYG{+w}{ }\PYGZhy{}it\PYG{+w}{ }xinference\PYGZhy{}cpu\PYG{+w}{ }xinference\PYG{+w}{ }launch\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}model\PYGZhy{}name\PYG{+w}{ }Qwen3\PYGZhy{}Reranker\PYGZhy{}4B\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}model\PYGZhy{}type\PYG{+w}{ }rerank\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}model\PYGZhy{}format\PYG{+w}{ }gguf\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}model\PYGZhy{}uri\PYG{+w}{ }/models/Qwen3\PYGZhy{}Reranker\PYGZhy{}4B\PYGZhy{}q5\PYGZus{}k\PYGZus{}m.gguf


\PYG{k}{in}\PYG{+w}{ }the\PYG{+w}{ }container\PYG{+w}{ }:\PYG{+w}{  }\PYG{o}{(}this\PYG{+w}{ }seems\PYG{+w}{ }to\PYG{+w}{ }work\PYG{+w}{ }OK\PYG{o}{)}
xinference\PYG{+w}{ }launch\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}model\PYGZhy{}name\PYG{+w}{ }Qwen3\PYGZhy{}Reranker\PYGZhy{}4B\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}model\PYGZhy{}type\PYG{+w}{ }rerank\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}model\PYGZhy{}format\PYG{+w}{ }gguf\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}model\PYGZhy{}uri\PYG{+w}{ }/models/Qwen3\PYGZhy{}Reranker\PYGZhy{}4B\PYGZhy{}q5\PYGZus{}k\PYGZus{}m.gguf
\end{sphinxVerbatim}


\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{search}

\end{itemize}



\renewcommand{\indexname}{Index}
\printindex
\end{document}