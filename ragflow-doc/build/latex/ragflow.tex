%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}

\PassOptionsToPackage{booktabs}{sphinx}
\PassOptionsToPackage{colorrows}{sphinx}

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{tgtermes}
\usepackage{tgheros}
\renewcommand{\ttdefault}{txtt}



\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=auto}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}



\title{Ragflow}
\date{Nov 04, 2025}
\release{1}
\author{Jansen Jan}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\ifdefined\shorthandoff
  \ifnum\catcode`\=\string=\active\shorthandoff{=}\fi
  \ifnum\catcode`\"=\active\shorthandoff{"}\fi
\fi

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


\sphinxstepscope


\chapter{Why Infiniflow RAGFlow Uses a Reranker, an Embedding Model, and a Chat Model}
\label{\detokenize{rerank:why-infiniflow-ragflow-uses-a-reranker-an-embedding-model-and-a-chat-model}}\label{\detokenize{rerank:ragflow-models}}\label{\detokenize{rerank::doc}}
\sphinxAtStartPar
Infiniflow RAGFlow is a Retrieval\sphinxhyphen{}Augmented Generation (RAG) framework designed to build high\sphinxhyphen{}quality, traceable question\sphinxhyphen{}answering systems over complex data sources. To achieve accurate and contextually relevant responses, RAGFlow employs three distinct models that work in concert:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Embedding Model}
\sphinxhyphen{} \sphinxstylestrong{Purpose}: Converts both the user query and the chunks of retrieved documents into dense vector representations in the same semantic space.
\sphinxhyphen{} \sphinxstylestrong{Role in Pipeline}: Enables semantic similarity search during the retrieval phase. By computing cosine similarity (or other distance metrics) between the query embedding and document chunk embeddings, RAGFlow retrieves the most semantically relevant passages from a large corpus—far beyond keyword matching.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Reranker}
\sphinxhyphen{} \sphinxstylestrong{Purpose}: Refines the initial retrieval results by re\sphinxhyphen{}scoring the top\sphinxhyphen{}\sphinxstyleemphasis{k} candidate chunks using a cross\sphinxhyphen{}encoder architecture.
\sphinxhyphen{} \sphinxstylestrong{Role in Pipeline}: While the embedding model provides efficient approximate retrieval, the reranker applies a more computationally intensive but accurate relevance scoring. This step significantly improves precision by pushing the most contextually appropriate chunks to the top, reducing noise before generation.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{rerank}.png}
\caption{\sphinxstylestrong{Figure 1}: The reranker evaluates query\sphinxhyphen{}chunk pairs to produce fine\sphinxhyphen{}grained relevance scores.}\label{\detokenize{rerank:id1}}\end{figure}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Chat Model (LLM)}
\sphinxhyphen{} \sphinxstylestrong{Purpose}: Generates the final natural language response grounded in the refined retrieved context.
\sphinxhyphen{} \sphinxstylestrong{Role in Pipeline}: Takes the top reranked chunks as context and synthesizes a coherent, accurate, and fluent answer. The chat model (typically a large language model fine\sphinxhyphen{}tuned for instruction following) ensures the output is not only factually aligned with the source material but also conversational and user\sphinxhyphen{}friendly.

\end{enumerate}


\section{Synergy of the Three Models}
\label{\detokenize{rerank:synergy-of-the-three-models}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Embedding Model} \(\rightarrow\) Broad, fast, semantic retrieval

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Reranker} \(\rightarrow\) Precise, fine\sphinxhyphen{}grained reordering

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Chat Model} \(\rightarrow\) Coherent, grounded generation

\end{itemize}

\sphinxAtStartPar
This modular design allows RAGFlow to balance \sphinxstylestrong{speed}, \sphinxstylestrong{accuracy}, and \sphinxstylestrong{interpretability}, making it suitable for enterprise\sphinxhyphen{}grade RAG applications where both performance and trustworthiness are critical.

\sphinxstepscope


\chapter{Why vLLM is Used to Serve the Reranker Model}
\label{\detokenize{vllm:why-vllm-is-used-to-serve-the-reranker-model}}\label{\detokenize{vllm:vllm-reranker}}\label{\detokenize{vllm::doc}}
\sphinxAtStartPar
vLLM is a high\sphinxhyphen{}throughput, memory\sphinxhyphen{}efficient inference engine specifically designed for serving large language models (LLMs). In \sphinxstylestrong{Infiniflow RAGFlow}, the \sphinxstylestrong{reranker model}—responsible for fine\sphinxhyphen{}grained relevance scoring of retrieved document chunks—is served using \sphinxstylestrong{vLLM} to ensure low\sphinxhyphen{}latency, scalable, and production\sphinxhyphen{}ready performance.


\section{Key Reasons for Using vLLM to Serve the Reranker}
\label{\detokenize{vllm:key-reasons-for-using-vllm-to-serve-the-reranker}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{PagedAttention for Memory Efficiency}
\sphinxhyphen{} vLLM uses \sphinxstylestrong{PagedAttention}, a novel attention mechanism that manages KV cache in non\sphinxhyphen{}contiguous memory pages.
\sphinxhyphen{} This dramatically reduces memory fragmentation and enables \sphinxstylestrong{higher batch sizes} and \sphinxstylestrong{longer sequence lengths} (up to 8192 tokens in this case), critical for processing query\sphinxhyphen{}chunk pairs during reranking.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{High Throughput \& Low Latency}
\sphinxhyphen{} Supports \sphinxstylestrong{continuous batching}, allowing dynamic batch formation as requests arrive.
\sphinxhyphen{} Eliminates head\sphinxhyphen{}of\sphinxhyphen{}line blocking and maximizes GPU utilization—ideal for real\sphinxhyphen{}time reranking in interactive RAG pipelines.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{OpenAI\sphinxhyphen{}Compatible API}
\sphinxhyphen{} Exposes a clean, standardized REST API compatible with OpenAI’s format.
\sphinxhyphen{} Enables seamless integration with RAGFlow’s orchestration layer without custom inference code.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Support for Cross\sphinxhyphen{}Encoder Rerankers}
\sphinxhyphen{} Models like \sphinxstylestrong{Qwen3\sphinxhyphen{}Reranker\sphinxhyphen{}0.6B} are cross\sphinxhyphen{}encoders that take \sphinxcode{\sphinxupquote{{[}query, passage{]}}} pairs as input.
\sphinxhyphen{} vLLM efficiently handles the bidirectional attention required, delivering relevance scores via \sphinxcode{\sphinxupquote{logits{[}0{]}}} (typically for binary classification: relevant/irrelevant).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Ollama Does Not Support Reranker Models (Yet)}
\sphinxhyphen{} \sphinxstylestrong{Ollama} is excellent for local LLM inference and chat models, but \sphinxstylestrong{currently lacks native support for reranker (cross\sphinxhyphen{}encoder) models}.
\sphinxhyphen{} Rerankers require structured input formatting and logit extraction that Ollama’s current API and model loading system do not accommodate.
\sphinxhyphen{} vLLM, in contrast, supports any Hugging Face transformer model—including rerankers—with full access to outputs and fine\sphinxhyphen{}grained control.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Scalability Advantage Over Ollama}
\sphinxhyphen{} When scaling to \sphinxstylestrong{multiple concurrent users} or \sphinxstylestrong{high\sphinxhyphen{}throughput workloads}, vLLM is significantly more robust than Ollama.
\sphinxhyphen{} vLLM supports \sphinxstylestrong{distributed serving}, \sphinxstylestrong{tensor parallelism}, \sphinxstylestrong{GPU clustering}, and \sphinxstylestrong{dynamic batching at scale}.
\sphinxhyphen{} Ollama is primarily designed for \sphinxstylestrong{single\sphinxhyphen{}user, local development}, and does not scale efficiently in production environments.

\end{enumerate}


\section{Serving the Reranker Locally with vLLM}
\label{\detokenize{vllm:serving-the-reranker-locally-with-vllm}}
\sphinxAtStartPar
You can run the reranker model locally using vLLM with the following command:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
vllm\PYG{+w}{ }serve\PYG{+w}{ }/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}port\PYG{+w}{ }\PYG{l+m}{8123}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}max\PYGZhy{}model\PYGZhy{}len\PYG{+w}{ }\PYG{l+m}{8192}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}dtype\PYG{+w}{ }auto\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}trust\PYGZhy{}remote\PYGZhy{}code
\end{sphinxVerbatim}

\sphinxAtStartPar
Once running, the model is accessible via the OpenAI\sphinxhyphen{}compatible endpoint:

\sphinxAtStartPar
\sphinxstylestrong{GET} \sphinxcode{\sphinxupquote{http://localhost:8123/v1/models}}

\sphinxAtStartPar
\sphinxstylestrong{Example Response}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}object\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}list\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}data\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{[}
\PYG{+w}{    }\PYG{p}{\PYGZob{}}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}id\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}object\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}model\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}created\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{1762258164}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}owned\PYGZus{}by\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}vllm\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}root\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}parent\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{null}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}max\PYGZus{}model\PYGZus{}len\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{8192}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}permission\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{[}
\PYG{+w}{        }\PYG{p}{\PYGZob{}}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}id\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}modelperm\PYGZhy{}1a0d5938e30b4eeebb53d9e5c7d9599e\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}object\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}model\PYGZus{}permission\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}created\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{1762258164}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}allow\PYGZus{}create\PYGZus{}engine\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{false}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}allow\PYGZus{}sampling\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{true}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}allow\PYGZus{}logprobs\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{true}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}allow\PYGZus{}search\PYGZus{}indices\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{false}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}allow\PYGZus{}view\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{true}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}allow\PYGZus{}fine\PYGZus{}tuning\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{false}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}organization\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}*\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}group\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{null}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}is\PYGZus{}blocking\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{false}
\PYG{+w}{        }\PYG{p}{\PYGZcb{}}
\PYG{+w}{      }\PYG{p}{]}
\PYG{+w}{    }\PYG{p}{\PYGZcb{}}
\PYG{+w}{  }\PYG{p}{]}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\section{RAGFlow Integration}
\label{\detokenize{vllm:ragflow-integration}}
\sphinxAtStartPar
RAGFlow configures the reranker endpoint in its settings:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nt}{reranker}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{provider}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{vllm}
\PYG{+w}{  }\PYG{n+nt}{api\PYGZus{}base}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{http://localhost:8123/v1}
\PYG{+w}{  }\PYG{n+nt}{model}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b}
\end{sphinxVerbatim}

\sphinxAtStartPar
During inference, RAGFlow sends batched \sphinxcode{\sphinxupquote{{[}query, passage{]}}} pairs to the vLLM server, receives relevance scores, and reorders chunks before passing them to the chat model.

\sphinxAtStartPar
\sphinxstylestrong{Result}: Fast, accurate, and scalable reranking powered by optimized LLM inference—\sphinxstylestrong{where Ollama cannot currently follow, and where vLLM excels in both development and production.}

\sphinxstepscope


\chapter{Serving vLLM Reranker Using Docker (CPU\sphinxhyphen{}Only)}
\label{\detokenize{vllm-cpu:serving-vllm-reranker-using-docker-cpu-only}}\label{\detokenize{vllm-cpu:vllm-docker}}\label{\detokenize{vllm-cpu::doc}}
\sphinxAtStartPar
To ensure \sphinxstylestrong{reproducibility}, \sphinxstylestrong{portability}, and \sphinxstylestrong{isolation}, \sphinxstylestrong{vLLM} can be deployed using \sphinxstylestrong{Docker}. This is especially useful in environments with restricted internet access (e.g., corporate networks behind proxies or firewalls), where \sphinxstylestrong{Hugging Face Hub} may be blocked or rate\sphinxhyphen{}limited.

\sphinxAtStartPar
In this setup, \sphinxstylestrong{vLLM runs on CPU only} because:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Laptop has no GPU}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Home server has an old NVIDIA GPU} (not supported by vLLM’s CUDA requirements)

\end{itemize}

\sphinxAtStartPar
Thus, we use the \sphinxstylestrong{official CPU\sphinxhyphen{}optimized vLLM image} built from:
\sphinxurl{https://github.com/vllm-project/vllm/blob/main/docker/Dockerfile.cpu}

\sphinxAtStartPar
—


\section{Docker Compose Configuration (CPU Mode)}
\label{\detokenize{vllm-cpu:docker-compose-configuration-cpu-mode}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nt}{version}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{3.8}\PYG{l+s}{\PYGZsq{}}
\PYG{n+nt}{services}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{qwen\PYGZhy{}reranker}\PYG{p}{:}
\PYG{+w}{    }\PYG{n+nt}{image}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{vllm\PYGZhy{}cpu:latest}
\PYG{+w}{    }\PYG{n+nt}{ports}\PYG{p}{:}\PYG{+w}{ }\PYG{p+pIndicator}{[}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{8123:8000}\PYG{l+s}{\PYGZdq{}}\PYG{p+pIndicator}{]}
\PYG{+w}{    }\PYG{n+nt}{volumes}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{/home/naj/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b:/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b:ro}
\PYG{+w}{    }\PYG{n+nt}{environment}\PYG{p}{:}
\PYG{+w}{      }\PYG{n+nt}{VLLM\PYGZus{}HF\PYGZus{}OVERRIDES}\PYG{p}{:}\PYG{+w}{ }\PYG{p+pIndicator}{|}
\PYG{+w}{        }\PYG{n+no}{\PYGZob{}}
\PYG{+w}{          }\PYG{n+no}{\PYGZdq{}architectures\PYGZdq{}: [\PYGZdq{}Qwen3ForSequenceClassification\PYGZdq{}],}
\PYG{+w}{          }\PYG{n+no}{\PYGZdq{}classifier\PYGZus{}from\PYGZus{}token\PYGZdq{}: [\PYGZdq{}no\PYGZdq{}, \PYGZdq{}yes\PYGZdq{}],}
\PYG{+w}{          }\PYG{n+no}{\PYGZdq{}is\PYGZus{}original\PYGZus{}qwen3\PYGZus{}reranker\PYGZdq{}: true}
\PYG{+w}{        }\PYG{n+no}{\PYGZcb{}}
\PYG{+w}{    }\PYG{n+nt}{command}\PYG{p}{:}\PYG{+w}{ }\PYG{p+pIndicator}{\PYGZgt{}}
\PYG{+w}{      }\PYG{n+no}{/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}task score}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}dtype float32}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}port 8000}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}trust\PYGZhy{}remote\PYGZhy{}code}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}max\PYGZhy{}model\PYGZhy{}len 8192}
\PYG{+w}{    }\PYG{n+nt}{deploy}\PYG{p}{:}
\PYG{+w}{      }\PYG{n+nt}{resources}\PYG{p}{:}
\PYG{+w}{        }\PYG{n+nt}{limits}\PYG{p}{:}
\PYG{+w}{          }\PYG{n+nt}{cpus}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{10}\PYG{l+s}{\PYGZsq{}}
\PYG{+w}{          }\PYG{n+nt}{memory}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{16G}
\PYG{+w}{    }\PYG{n+nt}{shm\PYGZus{}size}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{4g}
\PYG{+w}{    }\PYG{n+nt}{restart}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{unless\PYGZhy{}stopped}
\end{sphinxVerbatim}

\sphinxAtStartPar
—


\section{Key Components Explained}
\label{\detokenize{vllm-cpu:key-components-explained}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{\textasciigrave{}\textasciigrave{}image: vllm\sphinxhyphen{}cpu:latest\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Official vLLM CPU image (no CUDA dependencies).
\sphinxhyphen{} Built from: \sphinxhref{https://github.com/vllm-project/vllm/blob/main/docker/Dockerfile.cpu}{vllm\sphinxhyphen{}project/vllm/docker/Dockerfile.cpu}
\sphinxhyphen{} Uses PyTorch CPU backend with optimized inference kernels.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{\textasciigrave{}\textasciigrave{}ports: {[}“8123:8000”{]}\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Host port \sphinxstylestrong{8123} \(\rightarrow\) container port \sphinxstylestrong{8000} (vLLM default).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{\textasciigrave{}\textasciigrave{}volumes\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Mounts \sphinxstylestrong{locally pre\sphinxhyphen{}downloaded model} in \sphinxstylestrong{read\sphinxhyphen{}only} mode.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{\textasciigrave{}\textasciigrave{}VLLM\_HF\_OVERRIDES\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Required for \sphinxstylestrong{Qwen3\sphinxhyphen{}Reranker} due to custom classification head and token handling.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{\textasciigrave{}\textasciigrave{}command\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} \sphinxcode{\sphinxupquote{\sphinxhyphen{}\sphinxhyphen{}task score}}: Enables reranker scoring (outputs relevance logits).
\sphinxhyphen{} \sphinxcode{\sphinxupquote{\sphinxhyphen{}\sphinxhyphen{}dtype float32}}: Mandatory on CPU (no half\sphinxhyphen{}precision support).
\sphinxhyphen{} \sphinxcode{\sphinxupquote{\sphinxhyphen{}\sphinxhyphen{}max\sphinxhyphen{}model\sphinxhyphen{}len 8192}}: Supports long query+passage pairs.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Resource Limits}
\sphinxhyphen{} \sphinxcode{\sphinxupquote{cpus: \textquotesingle{}10\textquotesingle{}}} and \sphinxcode{\sphinxupquote{memory: 16G}} prevent system overload.
\sphinxhyphen{} \sphinxcode{\sphinxupquote{shm\_size: 4g}} ensures sufficient shared memory for batched inference.

\end{itemize}

\sphinxAtStartPar
—


\section{Why the Model Must Be Pre\sphinxhyphen{}Downloaded Locally}
\label{\detokenize{vllm-cpu:why-the-model-must-be-pre-downloaded-locally}}
\sphinxAtStartPar
The container \sphinxstylestrong{cannot download the model at runtime} due to:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Corporate Proxy / Firewall}
\sphinxhyphen{} Outbound traffic to \sphinxcode{\sphinxupquote{huggingface.co}} is blocked or requires authentication.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hugging Face Hub Blocked}
\sphinxhyphen{} Git LFS and model downloads fail in restricted networks.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{vLLM Auto\sphinxhyphen{}Download Fails Offline}
\sphinxhyphen{} vLLM uses \sphinxcode{\sphinxupquote{transformers.AutoModel}} \(\rightarrow\) attempts online download if model not found.

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Solution: Download via mirror}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nv}{HF\PYGZus{}ENDPOINT}\PYG{o}{=}https://hf\PYGZhy{}mirror.com\PYG{+w}{ }huggingface\PYGZhy{}cli\PYG{+w}{ }download\PYG{+w}{ }Qwen/Qwen3\PYGZhy{}Reranker\PYGZhy{}0.6B\PYG{+w}{ }\PYGZhy{}\PYGZhy{}local\PYGZhy{}dir\PYG{+w}{ }./qwen3\PYGZhy{}reranker\PYGZhy{}0.6b
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Uses \sphinxstylestrong{accessible mirror} (\sphinxcode{\sphinxupquote{hf\sphinxhyphen{}mirror.com}}).

\item {} 
\sphinxAtStartPar
Saves model locally for volume mounting.

\end{itemize}

\sphinxAtStartPar
—


\section{Why CPU\sphinxhyphen{}Only (No GPU)?}
\label{\detokenize{vllm-cpu:why-cpu-only-no-gpu}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Laptop}: Integrated graphics only (no discrete GPU).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Home Server}: NVIDIA GPU too old (e.g., pre\sphinxhyphen{}Ampere) \(\rightarrow\) \sphinxstylestrong{not supported} by vLLM’s CUDA 11.8+ / FlashAttention requirements.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{vLLM CPU image} enables full functionality without GPU.

\end{itemize}

\sphinxAtStartPar
\textgreater{} \sphinxstylestrong{Performance Note}: CPU inference is slower (\textasciitilde{}1\textendash{}3 sec per batch), but sufficient for \sphinxstylestrong{development}, \sphinxstylestrong{prototyping}, or \sphinxstylestrong{low\sphinxhyphen{}throughput} use cases.

\sphinxAtStartPar
—


\section{Start the Service}
\label{\detokenize{vllm-cpu:start-the-service}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker\PYGZhy{}compose\PYG{+w}{ }up\PYG{+w}{ }\PYGZhy{}d
\end{sphinxVerbatim}


\section{Verify Availability}
\label{\detokenize{vllm-cpu:verify-availability}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
curl\PYG{+w}{ }http://localhost:8123/v1/models
\end{sphinxVerbatim}

\sphinxAtStartPar
Expected output confirms the model is loaded and ready.

\sphinxAtStartPar
—


\section{Integration with RAGFlow}
\label{\detokenize{vllm-cpu:integration-with-ragflow}}
\sphinxAtStartPar
Update RAGFlow config:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nt}{reranker}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{provider}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{vllm}
\PYG{+w}{  }\PYG{n+nt}{api\PYGZus{}base}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{http://localhost:8123/v1}
\PYG{+w}{  }\PYG{n+nt}{model}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b}
\end{sphinxVerbatim}

\sphinxAtStartPar
—


\section{Benefits of This CPU + Docker Setup}
\label{\detokenize{vllm-cpu:benefits-of-this-cpu-docker-setup}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Works on any machine} (laptop, old server, air\sphinxhyphen{}gapped systems)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{No GPU required}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Offline\sphinxhyphen{}first} with pre\sphinxhyphen{}downloaded model

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Consistent environment} via Docker

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Secure}: read\sphinxhyphen{}only model, isolated container

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Scalable later}: switch to GPU image when hardware upgrades

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Ideal for local RAGFlow development and constrained production environments.}

\sphinxstepscope


\chapter{Integrating vLLM with RAGFlow via Docker Network}
\label{\detokenize{vllm-network:integrating-vllm-with-ragflow-via-docker-network}}\label{\detokenize{vllm-network:ragflow-vllm-network}}\label{\detokenize{vllm-network::doc}}
\sphinxAtStartPar
To enable \sphinxstylestrong{Infiniflow RAGFlow} (running in Docker) to communicate with a \sphinxstylestrong{vLLM reranker container}, both services must be on the \sphinxstylestrong{same Docker network}. By default, containers are isolated and cannot resolve each other by service name unless explicitly networked.

\sphinxAtStartPar
In this setup, we ensure seamless internal communication between:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{RAGFlow} (web + backend containers)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{vLLM reranker} (serving \sphinxtitleref{Qwen3\sphinxhyphen{}Reranker\sphinxhyphen{}0.6B})

\end{itemize}

\sphinxAtStartPar
—


\section{Why Network Configuration is Required}
\label{\detokenize{vllm-network:why-network-configuration-is-required}}\begin{itemize}
\item {} 
\sphinxAtStartPar
RAGFlow runs inside Docker (typically via \sphinxtitleref{docker\sphinxhyphen{}compose}).

\item {} 
\sphinxAtStartPar
vLLM reranker runs in a \sphinxstylestrong{separate container} (e.g., CPU\sphinxhyphen{}only).

\item {} 
\sphinxAtStartPar
RAGFlow needs to call: \sphinxtitleref{http://\textless{}vllm\sphinxhyphen{}service\sphinxhyphen{}name\textgreater{}:8000/v1} internally.

\item {} 
\sphinxAtStartPar
Without shared network \(\rightarrow\) \sphinxtitleref{Connection refused} or DNS lookup failure.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Solution}: Attach both services to a \sphinxstylestrong{custom Docker bridge network} (e.g., \sphinxtitleref{docker\sphinxhyphen{}ragflow}).

\sphinxAtStartPar
—


\section{Step\sphinxhyphen{}by\sphinxhyphen{}Step: Configure Docker Network}
\label{\detokenize{vllm-network:step-by-step-configure-docker-network}}
\sphinxAtStartPar
\#\#\# 1. Create a Custom Network

\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker\PYG{+w}{ }network\PYG{+w}{ }create\PYG{+w}{ }docker\PYGZhy{}ragflow
\end{sphinxVerbatim}

\sphinxAtStartPar
—

\sphinxAtStartPar
\#\#\# 2. Update \sphinxtitleref{docker\sphinxhyphen{}compose.yaml} for vLLM Reranker

\sphinxAtStartPar
Ensure the vLLM service uses the network:
\sphinxSetupCaptionForVerbatim{docker\sphinxhyphen{}compose.yaml (vLLM)}
\def\sphinxLiteralBlockLabel{\label{\detokenize{vllm-network:id1}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nt}{version}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{3.8}\PYG{l+s}{\PYGZsq{}}
\PYG{n+nt}{services}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{qwen\PYGZhy{}reranker}\PYG{p}{:}
\PYG{+w}{    }\PYG{n+nt}{image}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{vllm\PYGZhy{}cpu:latest}
\PYG{+w}{    }\PYG{n+nt}{container\PYGZus{}name}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{ragflow\PYGZhy{}vllm\PYGZhy{}reranker}
\PYG{+w}{    }\PYG{n+nt}{ports}\PYG{p}{:}\PYG{+w}{ }\PYG{p+pIndicator}{[}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{8123:8000}\PYG{l+s}{\PYGZdq{}}\PYG{p+pIndicator}{]}
\PYG{+w}{    }\PYG{n+nt}{volumes}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{/home/naj/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b:/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b:ro}
\PYG{+w}{    }\PYG{n+nt}{environment}\PYG{p}{:}
\PYG{+w}{      }\PYG{n+nt}{VLLM\PYGZus{}HF\PYGZus{}OVERRIDES}\PYG{p}{:}\PYG{+w}{ }\PYG{p+pIndicator}{|}
\PYG{+w}{        }\PYG{n+no}{\PYGZob{}}
\PYG{+w}{          }\PYG{n+no}{\PYGZdq{}architectures\PYGZdq{}: [\PYGZdq{}Qwen3ForSequenceClassification\PYGZdq{}],}
\PYG{+w}{          }\PYG{n+no}{\PYGZdq{}classifier\PYGZus{}from\PYGZus{}token\PYGZdq{}: [\PYGZdq{}no\PYGZdq{}, \PYGZdq{}yes\PYGZdq{}],}
\PYG{+w}{          }\PYG{n+no}{\PYGZdq{}is\PYGZus{}original\PYGZus{}qwen3\PYGZus{}reranker\PYGZdq{}: true}
\PYG{+w}{        }\PYG{n+no}{\PYGZcb{}}
\PYG{+w}{    }\PYG{n+nt}{command}\PYG{p}{:}\PYG{+w}{ }\PYG{p+pIndicator}{\PYGZgt{}}
\PYG{+w}{      }\PYG{n+no}{/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}task score}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}dtype float32}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}port 8000}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}trust\PYGZhy{}remote\PYGZhy{}code}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}max\PYGZhy{}model\PYGZhy{}len 8192}
\PYG{+w}{    }\PYG{n+nt}{deploy}\PYG{p}{:}
\PYG{+w}{      }\PYG{n+nt}{resources}\PYG{p}{:}
\PYG{+w}{        }\PYG{n+nt}{limits}\PYG{p}{:}
\PYG{+w}{          }\PYG{n+nt}{cpus}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{10}\PYG{l+s}{\PYGZsq{}}
\PYG{+w}{          }\PYG{n+nt}{memory}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{16G}
\PYG{+w}{    }\PYG{n+nt}{shm\PYGZus{}size}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{4g}
\PYG{+w}{    }\PYG{n+nt}{restart}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{unless\PYGZhy{}stopped}
\PYG{+w}{    }\PYG{n+nt}{networks}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{docker\PYGZhy{}ragflow}

\PYG{n+nt}{networks}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{docker\PYGZhy{}ragflow}\PYG{p}{:}
\PYG{+w}{    }\PYG{n+nt}{external}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{true}
\end{sphinxVerbatim}

\sphinxAtStartPar
—

\sphinxAtStartPar
\#\#\# 3. Connect RAGFlow Containers to the Same Network

\sphinxAtStartPar
If RAGFlow is already running via its own \sphinxtitleref{docker\sphinxhyphen{}compose}, \sphinxstylestrong{attach} it:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker\PYG{+w}{ }network\PYG{+w}{ }connect\PYG{+w}{ }docker\PYGZhy{}ragflow\PYG{+w}{ }ragflow\PYGZhy{}web
docker\PYG{+w}{ }network\PYG{+w}{ }connect\PYG{+w}{ }docker\PYGZhy{}ragflow\PYG{+w}{ }ragflow\PYGZhy{}server
\end{sphinxVerbatim}

\sphinxAtStartPar
\textgreater{} Replace \sphinxtitleref{ragflow\sphinxhyphen{}web}, \sphinxtitleref{ragflow\sphinxhyphen{}server} with actual container names (check with \sphinxtitleref{docker ps}).

\sphinxAtStartPar
—

\sphinxAtStartPar
\#\#\# 4. Configure RAGFlow to Use Internal vLLM Endpoint

\sphinxAtStartPar
In RAGFlow settings (UI or config file), set:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nt}{reranker}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{provider}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{vllm}
\PYG{+w}{  }\PYG{n+nt}{api\PYGZus{}base}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{http://ragflow\PYGZhy{}vllm\PYGZhy{}reranker:8000/v1}
\PYG{+w}{  }\PYG{n+nt}{model}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Key}: Use \sphinxstylestrong{container name} (\sphinxtitleref{ragflow\sphinxhyphen{}vllm\sphinxhyphen{}reranker}) — Docker DNS resolves it automatically within the network.

\sphinxAtStartPar
—


\section{Architecture Diagram}
\label{\detokenize{vllm-network:architecture-diagram}}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=1.000\linewidth]{{ragflow-vllm-def}.png}
\caption{\sphinxstylestrong{Figure 1}: RAGFlow containers communicate with vLLM reranker via internal Docker network \sphinxtitleref{docker\sphinxhyphen{}ragflow}. External access (optional) via port \sphinxtitleref{8123}.}\label{\detokenize{vllm-network:id2}}\end{figure}

\sphinxAtStartPar
—


\section{Verification}
\label{\detokenize{vllm-network:verification}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{From RAGFlow container}, test connectivity:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker\PYG{+w}{ }\PYG{n+nb}{exec}\PYG{+w}{ }\PYGZhy{}it\PYG{+w}{ }ragflow\PYGZhy{}server\PYG{+w}{ }curl\PYG{+w}{ }http://ragflow\PYGZhy{}vllm\PYGZhy{}reranker:8000/v1/models
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Expected output}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}object\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}list\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}data\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{[}
\PYG{+w}{    }\PYG{p}{\PYGZob{}}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}id\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{      }\PYG{err}{.}\PYG{err}{.}\PYG{err}{.}
\PYG{+w}{    }\PYG{p}{\PYGZcb{}}
\PYG{+w}{  }\PYG{p}{]}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\end{enumerate}

\sphinxAtStartPar
—


\section{Benefits of This Setup}
\label{\detokenize{vllm-network:benefits-of-this-setup}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Zero external exposure} (optional): vLLM accessible \sphinxstylestrong{only} within \sphinxtitleref{docker\sphinxhyphen{}ragflow} network.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Secure \& fast} internal communication.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Scalable}: Add more rerankers, LLMs, or vector DBs on same network.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Portable}: Works across dev, staging, production with same config.

\end{itemize}

\sphinxAtStartPar
—


\section{Troubleshooting Tips}
\label{\detokenize{vllm-network:troubleshooting-tips}}
\begin{DUlineblock}{0em}
\item[] Issue | Solution |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}———\sphinxhyphen{}|
| \sphinxtitleref{Connection refused} | Check network: \sphinxtitleref{docker network inspect docker\sphinxhyphen{}ragflow} |
| \sphinxtitleref{Unknown host} | Use \sphinxstylestrong{container name}, not \sphinxtitleref{localhost} |
| Port conflict | Ensure no other service uses \sphinxtitleref{8000} inside network |
| Model not loading | Verify volume mount and \sphinxtitleref{trust\sphinxhyphen{}remote\sphinxhyphen{}code} |

\sphinxAtStartPar
—


\section{Summary}
\label{\detokenize{vllm-network:summary}}
\sphinxAtStartPar
To use \sphinxstylestrong{vLLM inside RAGFlow Docker environment}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Create network: \sphinxtitleref{docker network create docker\sphinxhyphen{}ragflow}

\item {} 
\sphinxAtStartPar
Connect both RAGFlow and vLLM containers

\item {} 
\sphinxAtStartPar
Use \sphinxstylestrong{container name} in \sphinxtitleref{api\_base}

\item {} 
\sphinxAtStartPar
Enjoy \sphinxstylestrong{fast, secure, internal reranking}

\end{enumerate}

\sphinxAtStartPar
\textgreater{} \sphinxstylestrong{No need for public IPs, reverse proxies, or complex routing} — Docker handles it all.

\sphinxstepscope


\chapter{Batch Processing and Metadata Management in Infiniflow RAGFlow}
\label{\detokenize{api:batch-processing-and-metadata-management-in-infiniflow-ragflow}}\label{\detokenize{api:ragflow-batch-api}}\label{\detokenize{api::doc}}
\sphinxAtStartPar
\sphinxstylestrong{Infiniflow RAGFlow} provides a \sphinxstylestrong{RESTful API} (\sphinxtitleref{/api/v1}) that enables \sphinxstylestrong{programmatic control} over datasets and documents, making it ideal for \sphinxstylestrong{batch processing large volumes of documents}, \sphinxstylestrong{automated ingestion pipelines}, and \sphinxstylestrong{metadata enrichment}.

\sphinxAtStartPar
This is essential in enterprise settings where thousands of PDFs, reports, or web pages need to be:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Ingested in bulk

\item {} 
\sphinxAtStartPar
Tagged with structured metadata (author, date, source, category, etc.)

\item {} 
\sphinxAtStartPar
Updated post\sphinxhyphen{}ingestion

\item {} 
\sphinxAtStartPar
Queried or filtered later via the RAG system

\end{itemize}

\sphinxAtStartPar
—


\section{API Base URL}
\label{\detokenize{api:api-base-url}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
http://\PYGZlt{}RAGFLOW\PYGZus{}HOST\PYGZgt{}/api/v1
\end{sphinxVerbatim}


\section{Authentication}
\label{\detokenize{api:authentication}}
\sphinxAtStartPar
All requests require a \sphinxstylestrong{Bearer token}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Authorization: Bearer ragflow\PYGZhy{}\PYGZlt{}your\PYGZhy{}token\PYGZgt{}
\end{sphinxVerbatim}

\sphinxAtStartPar
\textgreater{} \sphinxstylestrong{Tip}: Obtain token via login or API key management in the RAGFlow UI.

\sphinxAtStartPar
—


\section{Step 1: Retrieve Dataset and Document IDs}
\label{\detokenize{api:step-1-retrieve-dataset-and-document-ids}}
\sphinxAtStartPar
Before updating, you \sphinxstylestrong{must know} the target:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Dataset ID} (e.g., \sphinxtitleref{f388c05e9df711f0a0fe0242ac170003})

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Document ID} (e.g., \sphinxtitleref{4920227c9eb711f0bff40242ac170003})

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{List all datasets}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
curl\PYG{+w}{ }\PYGZhy{}H\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}Authorization: Bearer ragflow\PYGZhy{}...\PYGZdq{}}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{     }http://192.168.0.213/api/v1/datasets
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{List documents in a dataset}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
curl\PYG{+w}{ }\PYGZhy{}H\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}Authorization: Bearer ragflow\PYGZhy{}...\PYGZdq{}}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{     }http://192.168.0.213/api/v1/datasets/\PYGZlt{}dataset\PYGZus{}id\PYGZgt{}/documents
\end{sphinxVerbatim}

\sphinxAtStartPar
—


\section{Step 2: Add Metadata to a Document (via PUT)}
\label{\detokenize{api:step-2-add-metadata-to-a-document-via-put}}
\sphinxAtStartPar
Use the \sphinxstylestrong{PUT} endpoint to \sphinxstylestrong{update metadata} of an existing document:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
curl\PYG{+w}{ }\PYGZhy{}\PYGZhy{}request\PYG{+w}{ }PUT\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{     }\PYGZhy{}\PYGZhy{}url\PYG{+w}{ }http://192.168.0.213/api/v1/datasets/f388c05e9df711f0a0fe0242ac170003/documents/4920227c9eb711f0bff40242ac170003\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{     }\PYGZhy{}\PYGZhy{}header\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}Content\PYGZhy{}Type: multipart/form\PYGZhy{}data\PYGZsq{}}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{     }\PYGZhy{}\PYGZhy{}header\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}Authorization: Bearer ragflow\PYGZhy{}QxNWIzMGNlOWRmMzExZjBhZjljMDI0Mm\PYGZsq{}}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{     }\PYGZhy{}\PYGZhy{}data\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}\PYGZob{}}
\PYG{l+s+s1}{       \PYGZdq{}meta\PYGZus{}fields\PYGZdq{}: \PYGZob{}}
\PYG{l+s+s1}{         \PYGZdq{}author\PYGZdq{}: \PYGZdq{}Example Author\PYGZdq{},}
\PYG{l+s+s1}{         \PYGZdq{}publish\PYGZus{}date\PYGZdq{}: \PYGZdq{}2025\PYGZhy{}01\PYGZhy{}01\PYGZdq{},}
\PYG{l+s+s1}{         \PYGZdq{}category\PYGZdq{}: \PYGZdq{}AI Business Report\PYGZdq{},}
\PYG{l+s+s1}{         \PYGZdq{}url\PYGZdq{}: \PYGZdq{}https://example.com/report.pdf\PYGZdq{}}
\PYG{l+s+s1}{       \PYGZcb{}}
\PYG{l+s+s1}{     \PYGZcb{}\PYGZsq{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Request Breakdown}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Method}: \sphinxtitleref{PUT}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Path}: \sphinxtitleref{/api/v1/datasets/\textless{}dataset\_id\textgreater{}/documents/\textless{}document\_id\textgreater{}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Content\sphinxhyphen{}Type}: \sphinxtitleref{multipart/form\sphinxhyphen{}data} (required even for JSON payload)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Body}: JSON string with \sphinxtitleref{“meta\_fields”} object

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Response (on success)}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}code\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}message\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}Success\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}data\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{\PYGZob{}}\PYG{+w}{ }\PYG{n+nt}{\PYGZdq{}document\PYGZus{}id\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}4920227c9eb711f0bff40242ac170003\PYGZdq{}}\PYG{+w}{ }\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
—


\section{Use Case: Batch Metadata Enrichment}
\label{\detokenize{api:use-case-batch-metadata-enrichment}}
\sphinxAtStartPar
You can \sphinxstylestrong{automate metadata tagging} for \sphinxstylestrong{1000s of documents} using a script:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{requests}
\PYG{k+kn}{import} \PYG{n+nn}{json}

\PYG{n}{BASE\PYGZus{}URL} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{http://192.168.0.213/api/v1}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{TOKEN} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ragflow\PYGZhy{}QxNWIzMGNlOWRmMzExZjBhZjljMDI0Mm}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{HEADERS} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Authorization}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Bearer }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{TOKEN}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Content\PYGZhy{}Type}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{multipart/form\PYGZhy{}data}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{p}{\PYGZcb{}}

\PYG{c+c1}{\PYGZsh{} Example: Load CSV with doc\PYGZus{}id, author, date, url...}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{documents\PYGZus{}metadata.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{\PYGZus{}}\PYG{p}{,} \PYG{n}{row} \PYG{o+ow}{in} \PYG{n}{df}\PYG{o}{.}\PYG{n}{iterrows}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{dataset\PYGZus{}id} \PYG{o}{=} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dataset\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
    \PYG{n}{doc\PYGZus{}id} \PYG{o}{=} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{document\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
    \PYG{n}{payload} \PYG{o}{=} \PYG{p}{\PYGZob{}}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{meta\PYGZus{}fields}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{author}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{author}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{publish\PYGZus{}date}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{publish\PYGZus{}date}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{category}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{category}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{url}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{source\PYGZus{}url}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
        \PYG{p}{\PYGZcb{}}
    \PYG{p}{\PYGZcb{}}

    \PYG{n}{files} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{json}\PYG{o}{.}\PYG{n}{dumps}\PYG{p}{(}\PYG{n}{payload}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{application/json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{\PYGZcb{}}
    \PYG{n}{resp} \PYG{o}{=} \PYG{n}{requests}\PYG{o}{.}\PYG{n}{put}\PYG{p}{(}
        \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{BASE\PYGZus{}URL}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{/datasets/}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{dataset\PYGZus{}id}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{/documents/}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{doc\PYGZus{}id}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{headers}\PYG{o}{=}\PYG{n}{HEADERS}\PYG{p}{,}
        \PYG{n}{files}\PYG{o}{=}\PYG{n}{files}
    \PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{doc\PYGZus{}id}\PYG{p}{,} \PYG{n}{resp}\PYG{o}{.}\PYG{n}{json}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{message}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Benefits}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Enrich RAG context with \sphinxstylestrong{structured, queryable metadata}

\item {} 
\sphinxAtStartPar
Enable \sphinxstylestrong{filtering} in UI or API (e.g., “Show reports from 2025 by Author X”)

\item {} 
\sphinxAtStartPar
Improve \sphinxstylestrong{traceability} and \sphinxstylestrong{auditability}

\end{itemize}

\sphinxAtStartPar
—


\section{Other Batch\sphinxhyphen{}Capable Endpoints}
\label{\detokenize{api:other-batch-capable-endpoints}}
\begin{DUlineblock}{0em}
\item[] Endpoint | Purpose |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}——\textendash{}|
| \sphinxtitleref{POST /api/v1/datasets} | Create new dataset |
| \sphinxtitleref{POST /api/v1/datasets/\{id\}/documents} | Upload new documents (with metadata) |
| \sphinxtitleref{DELETE /api/v1/datasets/\{id\}/documents/\{doc\_id\}} | Remove document |
| \sphinxtitleref{GET /api/v1/datasets/\{id\}/documents} | List + filter by metadata |

\sphinxAtStartPar
—


\section{Best Practices}
\label{\detokenize{api:best-practices}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Always use IDs} — never rely on filenames

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Batch in chunks} (e.g., 100 docs/sec) to avoid rate limits

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Validate metadata schema} in RAGFlow settings first

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Log responses} for retry logic

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Use dataset\sphinxhyphen{}level permissions} for access control

\end{enumerate}

\sphinxAtStartPar
—


\section{Summary}
\label{\detokenize{api:summary}}
\sphinxAtStartPar
RAGFlow’s \sphinxstylestrong{API\sphinxhyphen{}first design} enables:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Scalable batch ingestion}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Rich metadata attachment}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Full automation} of document lifecycle

\end{itemize}

\sphinxAtStartPar
\textgreater{} \sphinxstylestrong{Perfect for ETL pipelines, CMS integration, or enterprise knowledge base automation.}

\sphinxAtStartPar
With this API, you can manage \sphinxstylestrong{tens of thousands of documents} with full metadata — all programmatically.


\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{search}

\end{itemize}



\renewcommand{\indexname}{Index}
\printindex
\end{document}