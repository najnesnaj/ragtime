%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}

\PassOptionsToPackage{booktabs}{sphinx}
\PassOptionsToPackage{colorrows}{sphinx}

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{tgtermes}
\usepackage{tgheros}
\renewcommand{\ttdefault}{txtt}



\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=auto}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}



\title{Ragflow}
\date{Nov 10, 2025}
\release{1}
\author{Jansen Jan}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\ifdefined\shorthandoff
  \ifnum\catcode`\=\string=\active\shorthandoff{=}\fi
  \ifnum\catcode`\"=\active\shorthandoff{"}\fi
\fi

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


\sphinxstepscope


\chapter{About RAGFlow: Named Among GitHub’s Fastest\sphinxhyphen{}Growing Open Source Projects}
\label{\detokenize{about:about-ragflow-named-among-githubs-fastest-growing-open-source-projects}}\label{\detokenize{about:about-ragflow-octoverse}}\label{\detokenize{about::doc}}
\sphinxAtStartPar
\sphinxstylestrong{October 28, 2025 · 3 min read}

\sphinxAtStartPar
The release of \sphinxstylestrong{GitHub’s 2025 Octoverse report} marks a pivotal moment for the open source ecosystem—and for projects like \sphinxstylestrong{RAGFlow}, which has emerged as \sphinxstylestrong{one of the fastest\sphinxhyphen{}growing open source projects by contributors this year}.

\sphinxAtStartPar
With a \sphinxstylestrong{remarkable 2,596\% year\sphinxhyphen{}over\sphinxhyphen{}year growth in contributor engagement}, RAGFlow isn’t just gaining traction—\sphinxstylestrong{it’s defining the next wave of AI\sphinxhyphen{}powered development}.

\sphinxAtStartPar
—


\section{The Rise of Retrieval\sphinxhyphen{}Augmented Generation in Production}
\label{\detokenize{about:the-rise-of-retrieval-augmented-generation-in-production}}
\sphinxAtStartPar
As the Octoverse report highlights, \sphinxstylestrong{AI is no longer experimental—it’s foundational}.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{4.3 million+ AI\sphinxhyphen{}related repositories} on GitHub

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{1.1 million+ public repos} import LLM SDKs — a \sphinxstylestrong{178\% YoY increase}

\end{itemize}

\sphinxAtStartPar
In this context, \sphinxstylestrong{RAGFlow’s rapid adoption signals a clear shift}: developers are moving \sphinxstylestrong{beyond prototyping} and into \sphinxstylestrong{production\sphinxhyphen{}grade AI workflows}.

\sphinxAtStartPar
\sphinxstylestrong{RAGFlow}—an \sphinxstylestrong{end\sphinxhyphen{}to\sphinxhyphen{}end retrieval\sphinxhyphen{}augmented generation engine with built\sphinxhyphen{}in agent capabilities}—is perfectly positioned to meet this demand. It enables developers to build \sphinxstylestrong{scalable, context\sphinxhyphen{}aware AI applications} that are both \sphinxstylestrong{powerful and practical}.

\sphinxAtStartPar
\textgreater{} As the report notes:
\textgreater{} \sphinxstyleemphasis{“AI infrastructure is emerging as a major magnet” for open source contributions.}
\textgreater{} — \sphinxstylestrong{RAGFlow sits squarely at the intersection of AI infrastructure and real\sphinxhyphen{}world usability.}

\sphinxAtStartPar
—


\section{Why RAGFlow Resonates in the AI Era}
\label{\detokenize{about:why-ragflow-resonates-in-the-ai-era}}
\sphinxAtStartPar
Several trends highlighted in the Octoverse report \sphinxstylestrong{align closely} with RAGFlow’s design and mission:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{From Notebooks to Production}
\sphinxhyphen{} Jupyter Notebooks: \sphinxstylestrong{+75\% YoY}
\sphinxhyphen{} Python codebases: \sphinxstylestrong{surging}
\sphinxhyphen{} \sphinxstylestrong{RAGFlow supports this transition} with a \sphinxstylestrong{structured, reproducible framework} for deploying RAG systems in production.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Agentic Workflows Are Going Mainstream}
\sphinxhyphen{} GitHub Copilot coding agent launch
\sphinxhyphen{} Rise of AI\sphinxhyphen{}assisted development
\sphinxhyphen{} \sphinxstylestrong{RAGFlow’s built\sphinxhyphen{}in agent capabilities} automate \sphinxstylestrong{retrieval, reasoning, and response generation}—key components of modern AI apps.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Security and Scalability Are Top of Mind}
\sphinxhyphen{} \sphinxstylestrong{172\% YoY increase} in Broken Access Control vulnerabilities
\sphinxhyphen{} \sphinxstylestrong{RAGFlow’s enterprise\sphinxhyphen{}ready deployment} helps teams address these challenges \sphinxstylestrong{secure\sphinxhyphen{}by\sphinxhyphen{}design}

\end{enumerate}

\sphinxAtStartPar
—


\section{A Project in Active Development}
\label{\detokenize{about:a-project-in-active-development}}
\sphinxAtStartPar
RAGFlow’s evolution mirrors a \sphinxstylestrong{deliberate journey}—from solving foundational RAG challenges to \sphinxstylestrong{shaping the next generation of enterprise AI infrastructure}.

\sphinxAtStartPar
\#\#\# Phase 1: Solving Core RAG Limitations
RAGFlow first made its mark by \sphinxstylestrong{systematically addressing core RAG limitations} through integrated technological innovation:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Deep document understanding} for parsing complex formats (PDFs, tables, forms)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hybrid retrieval} blending multiple search strategies (vector, keyword, graph)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Built\sphinxhyphen{}in advanced tools}: \sphinxstylestrong{GraphRAG}, \sphinxstylestrong{RAPTOR}, and more

\item {} 
\sphinxAtStartPar
Result: \sphinxstylestrong{dramatically enhanced retrieval accuracy and reasoning performance}

\end{itemize}

\sphinxAtStartPar
\#\#\# Phase 2: The Superior Context Engine for Enterprise Agents
Now, building on this robust technical foundation, \sphinxstylestrong{RAGFlow is steering toward a bolder vision}:

\sphinxAtStartPar
\textgreater{} \sphinxstylestrong{To become the superior context engine for enterprise\sphinxhyphen{}grade Agents.}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Evolving from a \sphinxstylestrong{specialized RAG engine} into a \sphinxstylestrong{unified, resilient context layer}

\item {} 
\sphinxAtStartPar
Positioning itself as the \sphinxstylestrong{essential data foundation for LLMs in the enterprise}

\item {} 
\sphinxAtStartPar
Enabling \sphinxstylestrong{Agents of any kind} to access \sphinxstylestrong{rich, precise, and secure context}

\item {} 
\sphinxAtStartPar
Ensuring \sphinxstylestrong{reliable and effective operation across all tasks}

\end{itemize}

\sphinxAtStartPar
—


\section{Conclusion}
\label{\detokenize{about:conclusion}}
\sphinxAtStartPar
RAGFlow’s \sphinxstylestrong{explosive growth} in the 2025 Octoverse is not a coincidence.

\sphinxAtStartPar
It reflects a \sphinxstylestrong{global developer movement} toward \sphinxstylestrong{production\sphinxhyphen{}ready, agentic, secure AI systems}—and RAGFlow is \sphinxstylestrong{leading the charge}.

\sphinxAtStartPar
From \sphinxstylestrong{deep document parsing} to \sphinxstylestrong{scalable agent workflows}, RAGFlow delivers the \sphinxstylestrong{infrastructure} and \sphinxstylestrong{usability} that modern AI demands.

\sphinxAtStartPar
\sphinxstylestrong{The future of enterprise AI is context\sphinxhyphen{}aware, agent\sphinxhyphen{}driven, and open source—and RAGFlow is building it.}

\sphinxstepscope


\chapter{Why Infiniflow RAGFlow Uses a Reranker, an Embedding Model, and a Chat Model}
\label{\detokenize{rerank:why-infiniflow-ragflow-uses-a-reranker-an-embedding-model-and-a-chat-model}}\label{\detokenize{rerank:ragflow-models}}\label{\detokenize{rerank::doc}}
\sphinxAtStartPar
Infiniflow RAGFlow is a Retrieval\sphinxhyphen{}Augmented Generation (RAG) framework designed to build high\sphinxhyphen{}quality, traceable question\sphinxhyphen{}answering systems over complex data sources. To achieve accurate and contextually relevant responses, RAGFlow employs three distinct models that work in concert:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Embedding Model}
\sphinxhyphen{} \sphinxstylestrong{Purpose}: Converts both the user query and the chunks of retrieved documents into dense vector representations in the same semantic space.
\sphinxhyphen{} \sphinxstylestrong{Role in Pipeline}: Enables semantic similarity search during the retrieval phase. By computing cosine similarity (or other distance metrics) between the query embedding and document chunk embeddings, RAGFlow retrieves the most semantically relevant passages from a large corpus—far beyond keyword matching.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Reranker}
\sphinxhyphen{} \sphinxstylestrong{Purpose}: Refines the initial retrieval results by re\sphinxhyphen{}scoring the top\sphinxhyphen{}\sphinxstyleemphasis{k} candidate chunks using a cross\sphinxhyphen{}encoder architecture.
\sphinxhyphen{} \sphinxstylestrong{Role in Pipeline}: While the embedding model provides efficient approximate retrieval, the reranker applies a more computationally intensive but accurate relevance scoring. This step significantly improves precision by pushing the most contextually appropriate chunks to the top, reducing noise before generation.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{rerank}.png}
\caption{\sphinxstylestrong{Figure 1}: The reranker evaluates query\sphinxhyphen{}chunk pairs to produce fine\sphinxhyphen{}grained relevance scores.}\label{\detokenize{rerank:id1}}\end{figure}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Chat Model (LLM)}
\sphinxhyphen{} \sphinxstylestrong{Purpose}: Generates the final natural language response grounded in the refined retrieved context.
\sphinxhyphen{} \sphinxstylestrong{Role in Pipeline}: Takes the top reranked chunks as context and synthesizes a coherent, accurate, and fluent answer. The chat model (typically a large language model fine\sphinxhyphen{}tuned for instruction following) ensures the output is not only factually aligned with the source material but also conversational and user\sphinxhyphen{}friendly.

\end{enumerate}


\section{Synergy of the Three Models}
\label{\detokenize{rerank:synergy-of-the-three-models}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Embedding Model} \(\rightarrow\) Broad, fast, semantic retrieval

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Reranker} \(\rightarrow\) Precise, fine\sphinxhyphen{}grained reordering

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Chat Model} \(\rightarrow\) Coherent, grounded generation

\end{itemize}

\sphinxAtStartPar
This modular design allows RAGFlow to balance \sphinxstylestrong{speed}, \sphinxstylestrong{accuracy}, and \sphinxstylestrong{interpretability}, making it suitable for enterprise\sphinxhyphen{}grade RAG applications where both performance and trustworthiness are critical.

\sphinxstepscope


\chapter{Why vLLM is Used to Serve the Reranker Model}
\label{\detokenize{vllm:why-vllm-is-used-to-serve-the-reranker-model}}\label{\detokenize{vllm:vllm-reranker}}\label{\detokenize{vllm::doc}}
\sphinxAtStartPar
vLLM is a high\sphinxhyphen{}throughput, memory\sphinxhyphen{}efficient inference engine specifically designed for serving large language models (LLMs). In \sphinxstylestrong{Infiniflow RAGFlow}, the \sphinxstylestrong{reranker model}—responsible for fine\sphinxhyphen{}grained relevance scoring of retrieved document chunks—is served using \sphinxstylestrong{vLLM} to ensure low\sphinxhyphen{}latency, scalable, and production\sphinxhyphen{}ready performance.


\section{Key Reasons for Using vLLM to Serve the Reranker}
\label{\detokenize{vllm:key-reasons-for-using-vllm-to-serve-the-reranker}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{PagedAttention for Memory Efficiency}
\sphinxhyphen{} vLLM uses \sphinxstylestrong{PagedAttention}, a novel attention mechanism that manages KV cache in non\sphinxhyphen{}contiguous memory pages.
\sphinxhyphen{} This dramatically reduces memory fragmentation and enables \sphinxstylestrong{higher batch sizes} and \sphinxstylestrong{longer sequence lengths} (up to 8192 tokens in this case), critical for processing query\sphinxhyphen{}chunk pairs during reranking.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{High Throughput \& Low Latency}
\sphinxhyphen{} Supports \sphinxstylestrong{continuous batching}, allowing dynamic batch formation as requests arrive.
\sphinxhyphen{} Eliminates head\sphinxhyphen{}of\sphinxhyphen{}line blocking and maximizes GPU utilization—ideal for real\sphinxhyphen{}time reranking in interactive RAG pipelines.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{OpenAI\sphinxhyphen{}Compatible API}
\sphinxhyphen{} Exposes a clean, standardized REST API compatible with OpenAI’s format.
\sphinxhyphen{} Enables seamless integration with RAGFlow’s orchestration layer without custom inference code.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Support for Cross\sphinxhyphen{}Encoder Rerankers}
\sphinxhyphen{} Models like \sphinxstylestrong{Qwen3\sphinxhyphen{}Reranker\sphinxhyphen{}0.6B} are cross\sphinxhyphen{}encoders that take \sphinxcode{\sphinxupquote{{[}query, passage{]}}} pairs as input.
\sphinxhyphen{} vLLM efficiently handles the bidirectional attention required, delivering relevance scores via \sphinxcode{\sphinxupquote{logits{[}0{]}}} (typically for binary classification: relevant/irrelevant).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Ollama Does Not Support Reranker Models (Yet)}
\sphinxhyphen{} \sphinxstylestrong{Ollama} is excellent for local LLM inference and chat models, but \sphinxstylestrong{currently lacks native support for reranker (cross\sphinxhyphen{}encoder) models}.
\sphinxhyphen{} Rerankers require structured input formatting and logit extraction that Ollama’s current API and model loading system do not accommodate.
\sphinxhyphen{} vLLM, in contrast, supports any Hugging Face transformer model—including rerankers—with full access to outputs and fine\sphinxhyphen{}grained control.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Scalability Advantage Over Ollama}
\sphinxhyphen{} When scaling to \sphinxstylestrong{multiple concurrent users} or \sphinxstylestrong{high\sphinxhyphen{}throughput workloads}, vLLM is significantly more robust than Ollama.
\sphinxhyphen{} vLLM supports \sphinxstylestrong{distributed serving}, \sphinxstylestrong{tensor parallelism}, \sphinxstylestrong{GPU clustering}, and \sphinxstylestrong{dynamic batching at scale}.
\sphinxhyphen{} Ollama is primarily designed for \sphinxstylestrong{single\sphinxhyphen{}user, local development}, and does not scale efficiently in production environments.

\end{enumerate}


\section{Serving the Reranker Locally with vLLM}
\label{\detokenize{vllm:serving-the-reranker-locally-with-vllm}}
\sphinxAtStartPar
You can run the reranker model locally using vLLM with the following command:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
vllm\PYG{+w}{ }serve\PYG{+w}{ }/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}port\PYG{+w}{ }\PYG{l+m}{8123}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}max\PYGZhy{}model\PYGZhy{}len\PYG{+w}{ }\PYG{l+m}{8192}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}dtype\PYG{+w}{ }auto\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{    }\PYGZhy{}\PYGZhy{}trust\PYGZhy{}remote\PYGZhy{}code
\end{sphinxVerbatim}

\sphinxAtStartPar
Once running, the model is accessible via the OpenAI\sphinxhyphen{}compatible endpoint:

\sphinxAtStartPar
\sphinxstylestrong{GET} \sphinxcode{\sphinxupquote{http://localhost:8123/v1/models}}

\sphinxAtStartPar
\sphinxstylestrong{Example Response}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}object\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}list\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}data\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{[}
\PYG{+w}{    }\PYG{p}{\PYGZob{}}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}id\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}object\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}model\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}created\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{1762258164}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}owned\PYGZus{}by\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}vllm\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}root\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}parent\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{null}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}max\PYGZus{}model\PYGZus{}len\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{8192}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}permission\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{[}
\PYG{+w}{        }\PYG{p}{\PYGZob{}}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}id\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}modelperm\PYGZhy{}1a0d5938e30b4eeebb53d9e5c7d9599e\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}object\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}model\PYGZus{}permission\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}created\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{1762258164}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}allow\PYGZus{}create\PYGZus{}engine\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{false}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}allow\PYGZus{}sampling\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{true}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}allow\PYGZus{}logprobs\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{true}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}allow\PYGZus{}search\PYGZus{}indices\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{false}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}allow\PYGZus{}view\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{true}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}allow\PYGZus{}fine\PYGZus{}tuning\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{false}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}organization\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}*\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}group\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{null}\PYG{p}{,}
\PYG{+w}{          }\PYG{n+nt}{\PYGZdq{}is\PYGZus{}blocking\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{false}
\PYG{+w}{        }\PYG{p}{\PYGZcb{}}
\PYG{+w}{      }\PYG{p}{]}
\PYG{+w}{    }\PYG{p}{\PYGZcb{}}
\PYG{+w}{  }\PYG{p}{]}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\section{RAGFlow Integration}
\label{\detokenize{vllm:ragflow-integration}}
\sphinxAtStartPar
RAGFlow configures the reranker endpoint in its settings:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nt}{reranker}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{provider}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{vllm}
\PYG{+w}{  }\PYG{n+nt}{api\PYGZus{}base}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{http://localhost:8123/v1}
\PYG{+w}{  }\PYG{n+nt}{model}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b}
\end{sphinxVerbatim}

\sphinxAtStartPar
During inference, RAGFlow sends batched \sphinxcode{\sphinxupquote{{[}query, passage{]}}} pairs to the vLLM server, receives relevance scores, and reorders chunks before passing them to the chat model.

\sphinxAtStartPar
\sphinxstylestrong{Result}: Fast, accurate, and scalable reranking powered by optimized LLM inference—\sphinxstylestrong{where Ollama cannot currently follow, and where vLLM excels in both development and production.}

\sphinxstepscope


\chapter{Serving vLLM Reranker Using Docker (CPU\sphinxhyphen{}Only)}
\label{\detokenize{vllm-cpu:serving-vllm-reranker-using-docker-cpu-only}}\label{\detokenize{vllm-cpu:vllm-docker}}\label{\detokenize{vllm-cpu::doc}}
\sphinxAtStartPar
To ensure \sphinxstylestrong{reproducibility}, \sphinxstylestrong{portability}, and \sphinxstylestrong{isolation}, \sphinxstylestrong{vLLM} can be deployed using \sphinxstylestrong{Docker}. This is especially useful in environments with restricted internet access (e.g., corporate networks behind proxies or firewalls), where \sphinxstylestrong{Hugging Face Hub} may be blocked or rate\sphinxhyphen{}limited.

\sphinxAtStartPar
In this setup, \sphinxstylestrong{vLLM runs on CPU only} because:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Laptop has no GPU}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Home server has an old NVIDIA GPU} (not supported by vLLM’s CUDA requirements)

\end{itemize}

\sphinxAtStartPar
Thus, we use the \sphinxstylestrong{official CPU\sphinxhyphen{}optimized vLLM image} built from:
\sphinxurl{https://github.com/vllm-project/vllm/blob/main/docker/Dockerfile.cpu}

\sphinxAtStartPar
—


\section{Docker Compose Configuration (CPU Mode)}
\label{\detokenize{vllm-cpu:docker-compose-configuration-cpu-mode}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nt}{version}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{3.8}\PYG{l+s}{\PYGZsq{}}
\PYG{n+nt}{services}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{qwen\PYGZhy{}reranker}\PYG{p}{:}
\PYG{+w}{    }\PYG{n+nt}{image}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{vllm\PYGZhy{}cpu:latest}
\PYG{+w}{    }\PYG{n+nt}{ports}\PYG{p}{:}\PYG{+w}{ }\PYG{p+pIndicator}{[}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{8123:8000}\PYG{l+s}{\PYGZdq{}}\PYG{p+pIndicator}{]}
\PYG{+w}{    }\PYG{n+nt}{volumes}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{/home/naj/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b:/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b:ro}
\PYG{+w}{    }\PYG{n+nt}{environment}\PYG{p}{:}
\PYG{+w}{      }\PYG{n+nt}{VLLM\PYGZus{}HF\PYGZus{}OVERRIDES}\PYG{p}{:}\PYG{+w}{ }\PYG{p+pIndicator}{|}
\PYG{+w}{        }\PYG{n+no}{\PYGZob{}}
\PYG{+w}{          }\PYG{n+no}{\PYGZdq{}architectures\PYGZdq{}: [\PYGZdq{}Qwen3ForSequenceClassification\PYGZdq{}],}
\PYG{+w}{          }\PYG{n+no}{\PYGZdq{}classifier\PYGZus{}from\PYGZus{}token\PYGZdq{}: [\PYGZdq{}no\PYGZdq{}, \PYGZdq{}yes\PYGZdq{}],}
\PYG{+w}{          }\PYG{n+no}{\PYGZdq{}is\PYGZus{}original\PYGZus{}qwen3\PYGZus{}reranker\PYGZdq{}: true}
\PYG{+w}{        }\PYG{n+no}{\PYGZcb{}}
\PYG{+w}{    }\PYG{n+nt}{command}\PYG{p}{:}\PYG{+w}{ }\PYG{p+pIndicator}{\PYGZgt{}}
\PYG{+w}{      }\PYG{n+no}{/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}task score}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}dtype float32}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}port 8000}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}trust\PYGZhy{}remote\PYGZhy{}code}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}max\PYGZhy{}model\PYGZhy{}len 8192}
\PYG{+w}{    }\PYG{n+nt}{deploy}\PYG{p}{:}
\PYG{+w}{      }\PYG{n+nt}{resources}\PYG{p}{:}
\PYG{+w}{        }\PYG{n+nt}{limits}\PYG{p}{:}
\PYG{+w}{          }\PYG{n+nt}{cpus}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{10}\PYG{l+s}{\PYGZsq{}}
\PYG{+w}{          }\PYG{n+nt}{memory}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{16G}
\PYG{+w}{    }\PYG{n+nt}{shm\PYGZus{}size}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{4g}
\PYG{+w}{    }\PYG{n+nt}{restart}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{unless\PYGZhy{}stopped}
\end{sphinxVerbatim}

\sphinxAtStartPar
—


\section{Key Components Explained}
\label{\detokenize{vllm-cpu:key-components-explained}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{\textasciigrave{}\textasciigrave{}image: vllm\sphinxhyphen{}cpu:latest\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Official vLLM CPU image (no CUDA dependencies).
\sphinxhyphen{} Built from: \sphinxhref{https://github.com/vllm-project/vllm/blob/main/docker/Dockerfile.cpu}{vllm\sphinxhyphen{}project/vllm/docker/Dockerfile.cpu}
\sphinxhyphen{} Uses PyTorch CPU backend with optimized inference kernels.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{\textasciigrave{}\textasciigrave{}ports: {[}“8123:8000”{]}\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Host port \sphinxstylestrong{8123} \(\rightarrow\) container port \sphinxstylestrong{8000} (vLLM default).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{\textasciigrave{}\textasciigrave{}volumes\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Mounts \sphinxstylestrong{locally pre\sphinxhyphen{}downloaded model} in \sphinxstylestrong{read\sphinxhyphen{}only} mode.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{\textasciigrave{}\textasciigrave{}VLLM\_HF\_OVERRIDES\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} Required for \sphinxstylestrong{Qwen3\sphinxhyphen{}Reranker} due to custom classification head and token handling.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{\textasciigrave{}\textasciigrave{}command\textasciigrave{}\textasciigrave{}}
\sphinxhyphen{} \sphinxcode{\sphinxupquote{\sphinxhyphen{}\sphinxhyphen{}task score}}: Enables reranker scoring (outputs relevance logits).
\sphinxhyphen{} \sphinxcode{\sphinxupquote{\sphinxhyphen{}\sphinxhyphen{}dtype float32}}: Mandatory on CPU (no half\sphinxhyphen{}precision support).
\sphinxhyphen{} \sphinxcode{\sphinxupquote{\sphinxhyphen{}\sphinxhyphen{}max\sphinxhyphen{}model\sphinxhyphen{}len 8192}}: Supports long query+passage pairs.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Resource Limits}
\sphinxhyphen{} \sphinxcode{\sphinxupquote{cpus: \textquotesingle{}10\textquotesingle{}}} and \sphinxcode{\sphinxupquote{memory: 16G}} prevent system overload.
\sphinxhyphen{} \sphinxcode{\sphinxupquote{shm\_size: 4g}} ensures sufficient shared memory for batched inference.

\end{itemize}

\sphinxAtStartPar
—


\section{Why the Model Must Be Pre\sphinxhyphen{}Downloaded Locally}
\label{\detokenize{vllm-cpu:why-the-model-must-be-pre-downloaded-locally}}
\sphinxAtStartPar
The container \sphinxstylestrong{cannot download the model at runtime} due to:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Corporate Proxy / Firewall}
\sphinxhyphen{} Outbound traffic to \sphinxcode{\sphinxupquote{huggingface.co}} is blocked or requires authentication.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hugging Face Hub Blocked}
\sphinxhyphen{} Git LFS and model downloads fail in restricted networks.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{vLLM Auto\sphinxhyphen{}Download Fails Offline}
\sphinxhyphen{} vLLM uses \sphinxcode{\sphinxupquote{transformers.AutoModel}} \(\rightarrow\) attempts online download if model not found.

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Solution: Download via mirror}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nv}{HF\PYGZus{}ENDPOINT}\PYG{o}{=}https://hf\PYGZhy{}mirror.com\PYG{+w}{ }huggingface\PYGZhy{}cli\PYG{+w}{ }download\PYG{+w}{ }Qwen/Qwen3\PYGZhy{}Reranker\PYGZhy{}0.6B\PYG{+w}{ }\PYGZhy{}\PYGZhy{}local\PYGZhy{}dir\PYG{+w}{ }./qwen3\PYGZhy{}reranker\PYGZhy{}0.6b
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Remark : for some models you need a token HF\_TOKEN=xxxxxxxx (you have to specify the model in the token definition!)

\item {} 
\sphinxAtStartPar
Remark2 : use “sudo” if non\sphinxhyphen{}root!!!

\item {} 
\sphinxAtStartPar
Uses \sphinxstylestrong{accessible mirror} (\sphinxcode{\sphinxupquote{hf\sphinxhyphen{}mirror.com}}).

\item {} 
\sphinxAtStartPar
Saves model locally for volume mounting.

\end{itemize}

\sphinxAtStartPar
—


\section{Why CPU\sphinxhyphen{}Only (No GPU)?}
\label{\detokenize{vllm-cpu:why-cpu-only-no-gpu}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Laptop}: Integrated graphics only (no discrete GPU).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Home Server}: NVIDIA GPU too old (e.g., pre\sphinxhyphen{}Ampere) \(\rightarrow\) \sphinxstylestrong{not supported} by vLLM’s CUDA 11.8+ / FlashAttention requirements.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{vLLM CPU image} enables full functionality without GPU.

\end{itemize}

\sphinxAtStartPar
\textgreater{} \sphinxstylestrong{Performance Note}: CPU inference is slower (\textasciitilde{}1\textendash{}3 sec per batch), but sufficient for \sphinxstylestrong{development}, \sphinxstylestrong{prototyping}, or \sphinxstylestrong{low\sphinxhyphen{}throughput} use cases.

\sphinxAtStartPar
—


\section{Start the Service}
\label{\detokenize{vllm-cpu:start-the-service}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker\PYGZhy{}compose\PYG{+w}{ }up\PYG{+w}{ }\PYGZhy{}d
\end{sphinxVerbatim}


\section{Verify Availability}
\label{\detokenize{vllm-cpu:verify-availability}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
curl\PYG{+w}{ }http://localhost:8123/v1/models
\end{sphinxVerbatim}

\sphinxAtStartPar
Expected output confirms the model is loaded and ready.

\sphinxAtStartPar
—


\section{Integration with RAGFlow}
\label{\detokenize{vllm-cpu:integration-with-ragflow}}
\sphinxAtStartPar
Update RAGFlow config:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nt}{reranker}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{provider}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{vllm}
\PYG{+w}{  }\PYG{n+nt}{api\PYGZus{}base}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{http://localhost:8123/v1}
\PYG{+w}{  }\PYG{n+nt}{model}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b}
\end{sphinxVerbatim}

\sphinxAtStartPar
—


\section{Benefits of This CPU + Docker Setup}
\label{\detokenize{vllm-cpu:benefits-of-this-cpu-docker-setup}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Works on any machine} (laptop, old server, air\sphinxhyphen{}gapped systems)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{No GPU required}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Offline\sphinxhyphen{}first} with pre\sphinxhyphen{}downloaded model

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Consistent environment} via Docker

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Secure}: read\sphinxhyphen{}only model, isolated container

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Scalable later}: switch to GPU image when hardware upgrades

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Ideal for local RAGFlow development and constrained production environments.}

\sphinxstepscope


\chapter{Integrating vLLM with RAGFlow via Docker Network}
\label{\detokenize{vllm-network:integrating-vllm-with-ragflow-via-docker-network}}\label{\detokenize{vllm-network:ragflow-vllm-network}}\label{\detokenize{vllm-network::doc}}
\sphinxAtStartPar
To enable \sphinxstylestrong{Infiniflow RAGFlow} (running in Docker) to communicate with a \sphinxstylestrong{vLLM reranker container}, both services must be on the \sphinxstylestrong{same Docker network}. By default, containers are isolated and cannot resolve each other by service name unless explicitly networked.

\sphinxAtStartPar
In this setup, we ensure seamless internal communication between:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{RAGFlow} (web + backend containers)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{vLLM reranker} (serving \sphinxtitleref{Qwen3\sphinxhyphen{}Reranker\sphinxhyphen{}0.6B})

\end{itemize}

\sphinxAtStartPar
—


\section{Why Network Configuration is Required}
\label{\detokenize{vllm-network:why-network-configuration-is-required}}\begin{itemize}
\item {} 
\sphinxAtStartPar
RAGFlow runs inside Docker (typically via \sphinxtitleref{docker\sphinxhyphen{}compose}).

\item {} 
\sphinxAtStartPar
vLLM reranker runs in a \sphinxstylestrong{separate container} (e.g., CPU\sphinxhyphen{}only).

\item {} 
\sphinxAtStartPar
RAGFlow needs to call: \sphinxtitleref{http://\textless{}vllm\sphinxhyphen{}service\sphinxhyphen{}name\textgreater{}:8000/v1} internally.

\item {} 
\sphinxAtStartPar
Without shared network \(\rightarrow\) \sphinxtitleref{Connection refused} or DNS lookup failure.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Solution}: Attach both services to a \sphinxstylestrong{custom Docker bridge network} (e.g., \sphinxtitleref{docker\sphinxhyphen{}ragflow}).

\sphinxAtStartPar
—


\section{Step\sphinxhyphen{}by\sphinxhyphen{}Step: Configure Docker Network}
\label{\detokenize{vllm-network:step-by-step-configure-docker-network}}
\sphinxAtStartPar
\#\#\# 1. Create a Custom Network

\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker\PYG{+w}{ }network\PYG{+w}{ }create\PYG{+w}{ }docker\PYGZhy{}ragflow
\end{sphinxVerbatim}

\sphinxAtStartPar
—

\sphinxAtStartPar
\#\#\# 2. Update \sphinxtitleref{docker\sphinxhyphen{}compose.yaml} for vLLM Reranker

\sphinxAtStartPar
Ensure the vLLM service uses the network:
\sphinxSetupCaptionForVerbatim{docker\sphinxhyphen{}compose.yaml (vLLM)}
\def\sphinxLiteralBlockLabel{\label{\detokenize{vllm-network:id1}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nt}{version}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{3.8}\PYG{l+s}{\PYGZsq{}}
\PYG{n+nt}{services}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{qwen\PYGZhy{}reranker}\PYG{p}{:}
\PYG{+w}{    }\PYG{n+nt}{image}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{vllm\PYGZhy{}cpu:latest}
\PYG{+w}{    }\PYG{n+nt}{container\PYGZus{}name}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{ragflow\PYGZhy{}vllm\PYGZhy{}reranker}
\PYG{+w}{    }\PYG{n+nt}{ports}\PYG{p}{:}\PYG{+w}{ }\PYG{p+pIndicator}{[}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{8123:8000}\PYG{l+s}{\PYGZdq{}}\PYG{p+pIndicator}{]}
\PYG{+w}{    }\PYG{n+nt}{volumes}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{/home/naj/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b:/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b:ro}
\PYG{+w}{    }\PYG{n+nt}{environment}\PYG{p}{:}
\PYG{+w}{      }\PYG{n+nt}{VLLM\PYGZus{}HF\PYGZus{}OVERRIDES}\PYG{p}{:}\PYG{+w}{ }\PYG{p+pIndicator}{|}
\PYG{+w}{        }\PYG{n+no}{\PYGZob{}}
\PYG{+w}{          }\PYG{n+no}{\PYGZdq{}architectures\PYGZdq{}: [\PYGZdq{}Qwen3ForSequenceClassification\PYGZdq{}],}
\PYG{+w}{          }\PYG{n+no}{\PYGZdq{}classifier\PYGZus{}from\PYGZus{}token\PYGZdq{}: [\PYGZdq{}no\PYGZdq{}, \PYGZdq{}yes\PYGZdq{}],}
\PYG{+w}{          }\PYG{n+no}{\PYGZdq{}is\PYGZus{}original\PYGZus{}qwen3\PYGZus{}reranker\PYGZdq{}: true}
\PYG{+w}{        }\PYG{n+no}{\PYGZcb{}}
\PYG{+w}{    }\PYG{n+nt}{command}\PYG{p}{:}\PYG{+w}{ }\PYG{p+pIndicator}{\PYGZgt{}}
\PYG{+w}{      }\PYG{n+no}{/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}task score}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}dtype float32}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}port 8000}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}trust\PYGZhy{}remote\PYGZhy{}code}
\PYG{+w}{      }\PYG{n+no}{\PYGZhy{}\PYGZhy{}max\PYGZhy{}model\PYGZhy{}len 8192}
\PYG{+w}{    }\PYG{n+nt}{deploy}\PYG{p}{:}
\PYG{+w}{      }\PYG{n+nt}{resources}\PYG{p}{:}
\PYG{+w}{        }\PYG{n+nt}{limits}\PYG{p}{:}
\PYG{+w}{          }\PYG{n+nt}{cpus}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{10}\PYG{l+s}{\PYGZsq{}}
\PYG{+w}{          }\PYG{n+nt}{memory}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{16G}
\PYG{+w}{    }\PYG{n+nt}{shm\PYGZus{}size}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{4g}
\PYG{+w}{    }\PYG{n+nt}{restart}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{unless\PYGZhy{}stopped}
\PYG{+w}{    }\PYG{n+nt}{networks}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{docker\PYGZhy{}ragflow}

\PYG{n+nt}{networks}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{docker\PYGZhy{}ragflow}\PYG{p}{:}
\PYG{+w}{    }\PYG{n+nt}{external}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{true}
\end{sphinxVerbatim}

\sphinxAtStartPar
—

\sphinxAtStartPar
\#\#\# 3. Connect RAGFlow Containers to the Same Network

\sphinxAtStartPar
If RAGFlow is already running via its own \sphinxtitleref{docker\sphinxhyphen{}compose}, \sphinxstylestrong{attach} it:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker\PYG{+w}{ }network\PYG{+w}{ }connect\PYG{+w}{ }docker\PYGZhy{}ragflow\PYG{+w}{ }ragflow\PYGZhy{}web
docker\PYG{+w}{ }network\PYG{+w}{ }connect\PYG{+w}{ }docker\PYGZhy{}ragflow\PYG{+w}{ }ragflow\PYGZhy{}server
\end{sphinxVerbatim}

\sphinxAtStartPar
\textgreater{} Replace \sphinxtitleref{ragflow\sphinxhyphen{}web}, \sphinxtitleref{ragflow\sphinxhyphen{}server} with actual container names (check with \sphinxtitleref{docker ps}).

\sphinxAtStartPar
—

\sphinxAtStartPar
\#\#\# 4. Configure RAGFlow to Use Internal vLLM Endpoint

\sphinxAtStartPar
In RAGFlow settings (UI or config file), set:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nt}{reranker}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{provider}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{vllm}
\PYG{+w}{  }\PYG{n+nt}{api\PYGZus{}base}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{http://ragflow\PYGZhy{}vllm\PYGZhy{}reranker:8000/v1}
\PYG{+w}{  }\PYG{n+nt}{model}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Key}: Use \sphinxstylestrong{container name} (\sphinxtitleref{ragflow\sphinxhyphen{}vllm\sphinxhyphen{}reranker}) — Docker DNS resolves it automatically within the network.

\sphinxAtStartPar
—


\section{Architecture Diagram}
\label{\detokenize{vllm-network:architecture-diagram}}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=1.000\linewidth]{{ragflow-vllm-def}.png}
\caption{\sphinxstylestrong{Figure 1}: RAGFlow containers communicate with vLLM reranker via internal Docker network \sphinxtitleref{docker\sphinxhyphen{}ragflow}. External access (optional) via port \sphinxtitleref{8123}.}\label{\detokenize{vllm-network:id2}}\end{figure}

\sphinxAtStartPar
—


\section{Verification}
\label{\detokenize{vllm-network:verification}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{From RAGFlow container}, test connectivity:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker\PYG{+w}{ }\PYG{n+nb}{exec}\PYG{+w}{ }\PYGZhy{}it\PYG{+w}{ }ragflow\PYGZhy{}server\PYG{+w}{ }curl\PYG{+w}{ }http://ragflow\PYGZhy{}vllm\PYGZhy{}reranker:8000/v1/models
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Expected output}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}object\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}list\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}data\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{[}
\PYG{+w}{    }\PYG{p}{\PYGZob{}}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}id\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}/models/qwen3\PYGZhy{}reranker\PYGZhy{}0.6b\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{      }\PYG{err}{.}\PYG{err}{.}\PYG{err}{.}
\PYG{+w}{    }\PYG{p}{\PYGZcb{}}
\PYG{+w}{  }\PYG{p}{]}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\end{enumerate}

\sphinxAtStartPar
—


\section{Benefits of This Setup}
\label{\detokenize{vllm-network:benefits-of-this-setup}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Zero external exposure} (optional): vLLM accessible \sphinxstylestrong{only} within \sphinxtitleref{docker\sphinxhyphen{}ragflow} network.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Secure \& fast} internal communication.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Scalable}: Add more rerankers, LLMs, or vector DBs on same network.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Portable}: Works across dev, staging, production with same config.

\end{itemize}

\sphinxAtStartPar
—


\section{Troubleshooting Tips}
\label{\detokenize{vllm-network:troubleshooting-tips}}
\begin{DUlineblock}{0em}
\item[] Issue | Solution |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}———\sphinxhyphen{}|
| \sphinxtitleref{Connection refused} | Check network: \sphinxtitleref{docker network inspect docker\sphinxhyphen{}ragflow} |
| \sphinxtitleref{Unknown host} | Use \sphinxstylestrong{container name}, not \sphinxtitleref{localhost} |
| Port conflict | Ensure no other service uses \sphinxtitleref{8000} inside network |
| Model not loading | Verify volume mount and \sphinxtitleref{trust\sphinxhyphen{}remote\sphinxhyphen{}code} |

\sphinxAtStartPar
—


\section{Summary}
\label{\detokenize{vllm-network:summary}}
\sphinxAtStartPar
To use \sphinxstylestrong{vLLM inside RAGFlow Docker environment}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Create network: \sphinxtitleref{docker network create docker\sphinxhyphen{}ragflow}

\item {} 
\sphinxAtStartPar
Connect both RAGFlow and vLLM containers

\item {} 
\sphinxAtStartPar
Use \sphinxstylestrong{container name} in \sphinxtitleref{api\_base}

\item {} 
\sphinxAtStartPar
Enjoy \sphinxstylestrong{fast, secure, internal reranking}

\end{enumerate}

\sphinxAtStartPar
\textgreater{} \sphinxstylestrong{No need for public IPs, reverse proxies, or complex routing} — Docker handles it all.

\sphinxstepscope


\chapter{Batch Processing and Metadata Management in Infiniflow RAGFlow}
\label{\detokenize{api:batch-processing-and-metadata-management-in-infiniflow-ragflow}}\label{\detokenize{api:ragflow-batch-api}}\label{\detokenize{api::doc}}
\sphinxAtStartPar
\sphinxstylestrong{Infiniflow RAGFlow} provides a \sphinxstylestrong{RESTful API} (\sphinxtitleref{/api/v1}) that enables \sphinxstylestrong{programmatic control} over datasets and documents, making it ideal for \sphinxstylestrong{batch processing large volumes of documents}, \sphinxstylestrong{automated ingestion pipelines}, and \sphinxstylestrong{metadata enrichment}.

\sphinxAtStartPar
This is essential in enterprise settings where thousands of PDFs, reports, or web pages need to be:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Ingested in bulk

\item {} 
\sphinxAtStartPar
Tagged with structured metadata (author, date, source, category, etc.)

\item {} 
\sphinxAtStartPar
Updated post\sphinxhyphen{}ingestion

\item {} 
\sphinxAtStartPar
Queried or filtered later via the RAG system

\end{itemize}

\sphinxAtStartPar
—


\section{API Base URL}
\label{\detokenize{api:api-base-url}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
http://\PYGZlt{}RAGFLOW\PYGZus{}HOST\PYGZgt{}/api/v1
\end{sphinxVerbatim}


\section{Authentication}
\label{\detokenize{api:authentication}}
\sphinxAtStartPar
All requests require a \sphinxstylestrong{Bearer token}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Authorization: Bearer ragflow\PYGZhy{}\PYGZlt{}your\PYGZhy{}token\PYGZgt{}
\end{sphinxVerbatim}

\sphinxAtStartPar
\textgreater{} \sphinxstylestrong{Tip}: Obtain token via login or API key management in the RAGFlow UI.

\sphinxAtStartPar
—


\section{Step 1: Retrieve Dataset and Document IDs}
\label{\detokenize{api:step-1-retrieve-dataset-and-document-ids}}
\sphinxAtStartPar
Before updating, you \sphinxstylestrong{must know} the target:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Dataset ID} (e.g., \sphinxtitleref{f388c05e9df711f0a0fe0242ac170003})

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Document ID} (e.g., \sphinxtitleref{4920227c9eb711f0bff40242ac170003})

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{List all datasets}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
curl\PYG{+w}{ }\PYGZhy{}H\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}Authorization: Bearer ragflow\PYGZhy{}...\PYGZdq{}}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{     }http://192.168.0.213/api/v1/datasets
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{List documents in a dataset}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
curl\PYG{+w}{ }\PYGZhy{}H\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}Authorization: Bearer ragflow\PYGZhy{}...\PYGZdq{}}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{     }http://192.168.0.213/api/v1/datasets/\PYGZlt{}dataset\PYGZus{}id\PYGZgt{}/documents
\end{sphinxVerbatim}

\sphinxAtStartPar
—


\section{Step 2: Add Metadata to a Document (via PUT)}
\label{\detokenize{api:step-2-add-metadata-to-a-document-via-put}}
\sphinxAtStartPar
Use the \sphinxstylestrong{PUT} endpoint to \sphinxstylestrong{update metadata} of an existing document:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
curl\PYG{+w}{ }\PYGZhy{}\PYGZhy{}request\PYG{+w}{ }PUT\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{     }\PYGZhy{}\PYGZhy{}url\PYG{+w}{ }http://192.168.0.213/api/v1/datasets/f388c05e9df711f0a0fe0242ac170003/documents/4920227c9eb711f0bff40242ac170003\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{     }\PYGZhy{}\PYGZhy{}header\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}Content\PYGZhy{}Type: multipart/form\PYGZhy{}data\PYGZsq{}}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{     }\PYGZhy{}\PYGZhy{}header\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}Authorization: Bearer ragflow\PYGZhy{}QxNWIzMGNlOWRmMzExZjBhZjljMDI0Mm\PYGZsq{}}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{     }\PYGZhy{}\PYGZhy{}data\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}\PYGZob{}}
\PYG{l+s+s1}{       \PYGZdq{}meta\PYGZus{}fields\PYGZdq{}: \PYGZob{}}
\PYG{l+s+s1}{         \PYGZdq{}author\PYGZdq{}: \PYGZdq{}Example Author\PYGZdq{},}
\PYG{l+s+s1}{         \PYGZdq{}publish\PYGZus{}date\PYGZdq{}: \PYGZdq{}2025\PYGZhy{}01\PYGZhy{}01\PYGZdq{},}
\PYG{l+s+s1}{         \PYGZdq{}category\PYGZdq{}: \PYGZdq{}AI Business Report\PYGZdq{},}
\PYG{l+s+s1}{         \PYGZdq{}url\PYGZdq{}: \PYGZdq{}https://example.com/report.pdf\PYGZdq{}}
\PYG{l+s+s1}{       \PYGZcb{}}
\PYG{l+s+s1}{     \PYGZcb{}\PYGZsq{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Request Breakdown}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Method}: \sphinxtitleref{PUT}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Path}: \sphinxtitleref{/api/v1/datasets/\textless{}dataset\_id\textgreater{}/documents/\textless{}document\_id\textgreater{}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Content\sphinxhyphen{}Type}: \sphinxtitleref{multipart/form\sphinxhyphen{}data} (required even for JSON payload)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Body}: JSON string with \sphinxtitleref{“meta\_fields”} object

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Response (on success)}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}code\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}message\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}Success\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}data\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{\PYGZob{}}\PYG{+w}{ }\PYG{n+nt}{\PYGZdq{}document\PYGZus{}id\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}4920227c9eb711f0bff40242ac170003\PYGZdq{}}\PYG{+w}{ }\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
—


\section{Use Case: Batch Metadata Enrichment}
\label{\detokenize{api:use-case-batch-metadata-enrichment}}
\sphinxAtStartPar
You can \sphinxstylestrong{automate metadata tagging} for \sphinxstylestrong{1000s of documents} using a script:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{requests}
\PYG{k+kn}{import} \PYG{n+nn}{json}

\PYG{n}{BASE\PYGZus{}URL} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{http://192.168.0.213/api/v1}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{TOKEN} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ragflow\PYGZhy{}QxNWIzMGNlOWRmMzExZjBhZjljMDI0Mm}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{HEADERS} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Authorization}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Bearer }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{TOKEN}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Content\PYGZhy{}Type}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{multipart/form\PYGZhy{}data}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{p}{\PYGZcb{}}

\PYG{c+c1}{\PYGZsh{} Example: Load CSV with doc\PYGZus{}id, author, date, url...}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{documents\PYGZus{}metadata.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{\PYGZus{}}\PYG{p}{,} \PYG{n}{row} \PYG{o+ow}{in} \PYG{n}{df}\PYG{o}{.}\PYG{n}{iterrows}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{dataset\PYGZus{}id} \PYG{o}{=} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dataset\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
    \PYG{n}{doc\PYGZus{}id} \PYG{o}{=} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{document\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
    \PYG{n}{payload} \PYG{o}{=} \PYG{p}{\PYGZob{}}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{meta\PYGZus{}fields}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{author}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{author}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{publish\PYGZus{}date}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{publish\PYGZus{}date}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{category}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{category}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{url}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{source\PYGZus{}url}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
        \PYG{p}{\PYGZcb{}}
    \PYG{p}{\PYGZcb{}}

    \PYG{n}{files} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{json}\PYG{o}{.}\PYG{n}{dumps}\PYG{p}{(}\PYG{n}{payload}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{application/json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{\PYGZcb{}}
    \PYG{n}{resp} \PYG{o}{=} \PYG{n}{requests}\PYG{o}{.}\PYG{n}{put}\PYG{p}{(}
        \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{BASE\PYGZus{}URL}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{/datasets/}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{dataset\PYGZus{}id}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{/documents/}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{doc\PYGZus{}id}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{headers}\PYG{o}{=}\PYG{n}{HEADERS}\PYG{p}{,}
        \PYG{n}{files}\PYG{o}{=}\PYG{n}{files}
    \PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{doc\PYGZus{}id}\PYG{p}{,} \PYG{n}{resp}\PYG{o}{.}\PYG{n}{json}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{message}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Benefits}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Enrich RAG context with \sphinxstylestrong{structured, queryable metadata}

\item {} 
\sphinxAtStartPar
Enable \sphinxstylestrong{filtering} in UI or API (e.g., “Show reports from 2025 by Author X”)

\item {} 
\sphinxAtStartPar
Improve \sphinxstylestrong{traceability} and \sphinxstylestrong{auditability}

\end{itemize}

\sphinxAtStartPar
—


\section{Other Batch\sphinxhyphen{}Capable Endpoints}
\label{\detokenize{api:other-batch-capable-endpoints}}
\begin{DUlineblock}{0em}
\item[] Endpoint | Purpose |
\end{DUlineblock}

\sphinxAtStartPar
{\color{red}\bfseries{}|\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}|}——\textendash{}|
| \sphinxtitleref{POST /api/v1/datasets} | Create new dataset |
| \sphinxtitleref{POST /api/v1/datasets/\{id\}/documents} | Upload new documents (with metadata) |
| \sphinxtitleref{DELETE /api/v1/datasets/\{id\}/documents/\{doc\_id\}} | Remove document |
| \sphinxtitleref{GET /api/v1/datasets/\{id\}/documents} | List + filter by metadata |

\sphinxAtStartPar
—


\section{Best Practices}
\label{\detokenize{api:best-practices}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Always use IDs} — never rely on filenames

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Batch in chunks} (e.g., 100 docs/sec) to avoid rate limits

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Validate metadata schema} in RAGFlow settings first

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Log responses} for retry logic

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Use dataset\sphinxhyphen{}level permissions} for access control

\end{enumerate}

\sphinxAtStartPar
—


\section{Summary}
\label{\detokenize{api:summary}}
\sphinxAtStartPar
RAGFlow’s \sphinxstylestrong{API\sphinxhyphen{}first design} enables:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Scalable batch ingestion}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Rich metadata attachment}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Full automation} of document lifecycle

\end{itemize}

\sphinxAtStartPar
\textgreater{} \sphinxstylestrong{Perfect for ETL pipelines, CMS integration, or enterprise knowledge base automation.}

\sphinxAtStartPar
With this API, you can manage \sphinxstylestrong{tens of thousands of documents} with full metadata — all programmatically.

\sphinxstepscope


\chapter{How the Knowledge Graph in Infiniflow/RAGFlow Works}
\label{\detokenize{graph:how-the-knowledge-graph-in-infiniflow-ragflow-works}}\label{\detokenize{graph::doc}}
\sphinxAtStartPar
RAGFlow is an open\sphinxhyphen{}source Retrieval\sphinxhyphen{}Augmented Generation (RAG) engine developed by Infiniflow, designed to enhance LLM\sphinxhyphen{}based question\sphinxhyphen{}answering by integrating deep document understanding with structured data processing. The knowledge graph (KG) component plays a pivotal role in handling complex queries, particularly multi\sphinxhyphen{}hop question\sphinxhyphen{}answering, by extracting and organizing entities and relationships from documents into a graph structure. This enables more accurate and interconnected retrieval beyond simple vector\sphinxhyphen{}based searches.


\section{Overview}
\label{\detokenize{graph:overview}}
\sphinxAtStartPar
The KG is constructed as an intermediate step in RAGFlow’s data pipeline, bridging raw document extraction and final indexing. It transforms unstructured text into a relational graph, allowing for entity\sphinxhyphen{}based reasoning and traversal during retrieval. Key benefits include:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Multi\sphinxhyphen{}Hop Query Support}: Facilitates queries requiring inference across multiple documents or concepts (e.g., “What caused the event that affected company X?”).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Dynamic Updates}: From version 0.16.0 onward, the KG is built across an entire knowledge base (multiple files) and automatically updates when new documents are uploaded and parsed.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Integration with RAG 2.0}: Part of preprocessing stages like document clustering and domain\sphinxhyphen{}specific embedding, ensuring retrieval results are contextually rich and grounded.

\end{itemize}

\sphinxAtStartPar
The KG is stored as chunks in RAGFlow’s document engine (Elasticsearch by default or Infinity for advanced vector/graph capabilities), making it queryable alongside embeddings.


\section{Construction Process}
\label{\detokenize{graph:construction-process}}
\sphinxAtStartPar
The KG construction occurs after initial document parsing but before indexing. Here’s the step\sphinxhyphen{}by\sphinxhyphen{}step workflow:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Document Ingestion and Extraction}:
\sphinxhyphen{} Users upload files (e.g., PDF, Word, Excel, TXT) to a knowledge base.
\sphinxhyphen{} RAGFlow’s Deep Document Understanding (DDU) module parses the content, extracting structured elements like text blocks, tables, and layouts using OCR and layout models.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Entity and Relation Extraction}:
\sphinxhyphen{} Using NLP models (integrated via configurable LLMs or embedding services), RAGFlow identifies entities (e.g., people, organizations, events) and relations (e.g., “causes”, “affiliated with”) from extracted chunks.
\sphinxhyphen{} This is model\sphinxhyphen{}driven: Preprocessing applies entity recognition and relation extraction to raw text, often leveraging domain\sphinxhyphen{}specific prompts for accuracy.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Graph Building}:
\sphinxhyphen{} Entities become nodes, and relations form directed/undirected edges.
\sphinxhyphen{} The graph is unified across the entire dataset (not per\sphinxhyphen{}file since v0.16.0), enabling cross\sphinxhyphen{}document connections.
\sphinxhyphen{} Acceleration features (introduced in later releases) optimize extraction speed, such as batch processing or efficient model inference.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Storage}:
\sphinxhyphen{} Graph chunks (nodes, edges, metadata) are serialized and stored in the document engine.
\sphinxhyphen{} No separate graph database is required; it’s embedded within the vector/full\sphinxhyphen{}text index for hybrid queries.

\end{enumerate}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Construction can be toggled per knowledge base and is optional, but recommended for complex domains like finance or healthcare.
\end{sphinxadmonition}


\section{Query and Retrieval Process}
\label{\detokenize{graph:query-and-retrieval-process}}
\sphinxAtStartPar
During inference, the KG enhances retrieval in the following manner:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Query Parsing}:
\sphinxhyphen{} Incoming user queries are analyzed to detect multi\sphinxhyphen{}hop intent (e.g., via LLM routing).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hybrid Retrieval}:
\sphinxhyphen{} Combine vector similarity search (for semantic relevance) with graph traversal:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Start from query entities as seed nodes.

\item {} 
\sphinxAtStartPar
Traverse edges to fetch connected nodes (e.g., 1\sphinxhyphen{}2 hops).

\item {} 
\sphinxAtStartPar
Rank results by relevance scores, incorporating graph proximity.

\end{itemize}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Infinity engine (optional) supports efficient graph\sphinxhyphen{}range filtering alongside vectors.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Augmentation and Generation}:
\sphinxhyphen{} Retrieved graph\sphinxhyphen{}derived contexts (e.g., subgraphs or paths) are fused with text chunks.
\sphinxhyphen{} Fed to the LLM for grounded generation, with citations traceable to source documents.

\end{enumerate}

\sphinxAtStartPar
This process addresses limitations of pure vector RAG, such as hallucination in interconnected scenarios, by providing explicit relational paths.


\section{Key Features and Limitations}
\label{\detokenize{graph:key-features-and-limitations}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Features}:
\sphinxhyphen{} \sphinxstylestrong{Scalability}: Handles enterprise\sphinxhyphen{}scale knowledge bases with dynamic rebuilding.
\sphinxhyphen{} \sphinxstylestrong{Customizability}: Configurable extraction models and hop limits.
\sphinxhyphen{} \sphinxstylestrong{Agent Integration}: Supports agentic workflows for iterative graph exploration.
\sphinxhyphen{} \sphinxstylestrong{Performance}: Accelerated extraction in v0.21+ releases.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Limitations}:
\sphinxhyphen{} Relies on quality of upstream extraction; noisy documents may yield incomplete graphs.
\sphinxhyphen{} Graph depth is configurable but can increase latency for deep traversals.
\sphinxhyphen{} Arm64 Linux support is limited when using Infinity.

\end{itemize}

\sphinxAtStartPar
For implementation details, refer to the official guide at \sphinxurl{https://github.com/infiniflow/ragflow/blob/main/docs/guides/dataset/construct\_knowledge\_graph.md}. To experiment, deploy RAGFlow via Docker and enable KG in your knowledge base settings.

\sphinxstepscope


\chapter{Running Llama 3.1 with llama.cpp}
\label{\detokenize{llama-cpp:running-llama-3-1-with-llama-cpp}}\label{\detokenize{llama-cpp:llama-cpp-model-format-gguf}}\label{\detokenize{llama-cpp::doc}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{Table of Contents}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{llama-cpp:id1}}{\hyperref[\detokenize{llama-cpp:model-format-gguf}]{\sphinxcrossref{1. Model Format: GGUF}}}

\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{llama-cpp:id2}}{\hyperref[\detokenize{llama-cpp:compile-llama-cpp-for-intel-i7-cpu-only}]{\sphinxcrossref{2. Compile llama.cpp for Intel i7 (CPU\sphinxhyphen{}only)}}}

\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{llama-cpp:id3}}{\hyperref[\detokenize{llama-cpp:run-the-model-with-web-interface}]{\sphinxcrossref{3. Run the Model with Web Interface}}}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{llama-cpp:id4}}{\hyperref[\detokenize{llama-cpp:features}]{\sphinxcrossref{Features}}}

\end{itemize}

\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{llama-cpp:id5}}{\hyperref[\detokenize{llama-cpp:compile-llama-cpp-with-nvidia-gpu-support-cuda}]{\sphinxcrossref{4. Compile llama.cpp with NVIDIA GPU Support (CUDA)}}}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{llama-cpp:id6}}{\hyperref[\detokenize{llama-cpp:prerequisites}]{\sphinxcrossref{Prerequisites}}}

\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{llama-cpp:id7}}{\hyperref[\detokenize{llama-cpp:build-with-cuda}]{\sphinxcrossref{Build with CUDA}}}

\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{llama-cpp:id8}}{\hyperref[\detokenize{llama-cpp:run-with-gpu-offloading}]{\sphinxcrossref{Run with GPU offloading}}}

\end{itemize}

\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{llama-cpp:id9}}{\hyperref[\detokenize{llama-cpp:summary}]{\sphinxcrossref{Summary}}}

\end{itemize}
\end{sphinxShadowBox}


\bigskip\hrule\bigskip



\section{1. Model Format: GGUF}
\label{\detokenize{llama-cpp:model-format-gguf}}\label{\detokenize{llama-cpp:llama-cpp-compile-llama-cpp-for-intel-i7-cpu-only}}
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{llama.cpp}} uses the \sphinxstylestrong{GGUF} (GPT\sphinxhyphen{}Generated Unified Format) model format.

\sphinxAtStartPar
You can download a pre\sphinxhyphen{}quantized \sphinxstylestrong{Llama 3.1 8B} model in GGUF format directly from Hugging Face:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
https://huggingface.co/QuantFactory/Meta\PYGZhy{}Llama\PYGZhy{}3\PYGZhy{}8B\PYGZhy{}GGUF
\end{sphinxVerbatim}

\sphinxAtStartPar
Example file: \sphinxcode{\sphinxupquote{Meta\sphinxhyphen{}Llama\sphinxhyphen{}3\sphinxhyphen{}8B.Q4\_K\_S.gguf}} (\textasciitilde{}4.7 GB, 4\sphinxhyphen{}bit quantization, excellent quality/size tradeoff).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{Q4\_K\_S}} variant uses \textasciitilde{}4.7 GB RAM and runs efficiently on Intel i7 CPUs.
\end{sphinxadmonition}


\bigskip\hrule\bigskip



\section{2. Compile llama.cpp for Intel i7 (CPU\sphinxhyphen{}only)}
\label{\detokenize{llama-cpp:compile-llama-cpp-for-intel-i7-cpu-only}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Step 1: Clone the repository}
git\PYG{+w}{ }clone\PYG{+w}{ }https://github.com/ggerganov/llama.cpp
\PYG{n+nb}{cd}\PYG{+w}{ }llama.cpp

\PYG{c+c1}{\PYGZsh{} Step 2: Build for CPU (Intel i7, AVX2 enabled by default)}
make\PYG{+w}{ }clean
make\PYG{+w}{ }\PYGZhy{}j\PYG{k}{\PYGZdl{}(}nproc\PYG{k}{)}\PYG{+w}{ }\PYG{n+nv}{LLAMA\PYGZus{}CPU}\PYG{o}{=}\PYG{l+m}{1}

\PYG{c+c1}{\PYGZsh{} Optional: Force AVX2 (most i7 CPUs support it)}
make\PYG{+w}{ }clean
make\PYG{+w}{ }\PYGZhy{}j\PYG{k}{\PYGZdl{}(}nproc\PYG{k}{)}\PYG{+w}{ }\PYG{n+nv}{LLAMA\PYGZus{}CPU}\PYG{o}{=}\PYG{l+m}{1}\PYG{+w}{ }\PYG{n+nv}{LLAMA\PYGZus{}AVX2}\PYG{o}{=}\PYG{l+m}{1}
\end{sphinxVerbatim}

\sphinxAtStartPar
The binaries will be in the root directory:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{./llama\sphinxhyphen{}cli}} \(\rightarrow\) interactive CLI

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{./server}}   \(\rightarrow\) web server (OpenAI\sphinxhyphen{}compatible API + full web UI)

\end{itemize}

\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
\sphinxstylestrong{Do not use vLLM} on older Xeon v2 CPUs — they \sphinxstylestrong{lack AVX\sphinxhyphen{}512}, which vLLM requires.
\sphinxstylestrong{llama.cpp is a better choice} — it runs efficiently with just AVX2 or even SSE.
\end{sphinxadmonition}


\bigskip\hrule\bigskip



\section{3. Run the Model with Web Interface}
\label{\detokenize{llama-cpp:run-the-model-with-web-interface}}\label{\detokenize{llama-cpp:llama-cpp-run-the-model-with-web-interface}}
\sphinxAtStartPar
Place the downloaded GGUF file in a \sphinxcode{\sphinxupquote{models/}} folder:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
mkdir\PYG{+w}{ }\PYGZhy{}p\PYG{+w}{ }models
\PYG{c+c1}{\PYGZsh{} Copy or symlink the model}
ln\PYG{+w}{ }\PYGZhy{}s\PYG{+w}{ }/path/to/Meta\PYGZhy{}Llama\PYGZhy{}3\PYGZhy{}8B.Q4\PYGZus{}K\PYGZus{}S.gguf\PYG{+w}{ }models/
\end{sphinxVerbatim}

\sphinxAtStartPar
Start the server on port \sphinxstylestrong{8087} using \sphinxstylestrong{12 threads}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
./server\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}model\PYG{+w}{ }models/Meta\PYGZhy{}Llama\PYGZhy{}3\PYGZhy{}8B.Q4\PYGZus{}K\PYGZus{}S.gguf\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}port\PYG{+w}{ }\PYG{l+m}{8087}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}threads\PYG{+w}{ }\PYG{l+m}{12}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}host\PYG{+w}{ }\PYG{l+m}{0}.0.0.0
\end{sphinxVerbatim}


\subsection{Features}
\label{\detokenize{llama-cpp:features}}\label{\detokenize{llama-cpp:llama-cpp-features}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Full web UI} at: \sphinxurl{http://localhost:8087}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{OpenAI\sphinxhyphen{}compatible API} at: \sphinxurl{http://localhost:8087/v1}

\item {} 
\sphinxAtStartPar
List models:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
curl\PYG{+w}{ }http://localhost:8087/v1/models
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Response}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}object\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}list\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}data\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{[}
\PYG{+w}{    }\PYG{p}{\PYGZob{}}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}id\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}models/Meta\PYGZhy{}Llama\PYGZhy{}3\PYGZhy{}8B.Q4\PYGZus{}K\PYGZus{}S.gguf\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}object\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}model\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}created\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{1762783003}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}owned\PYGZus{}by\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}llamacpp\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{      }\PYG{n+nt}{\PYGZdq{}meta\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{        }\PYG{n+nt}{\PYGZdq{}vocab\PYGZus{}type\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{2}\PYG{p}{,}
\PYG{+w}{        }\PYG{n+nt}{\PYGZdq{}n\PYGZus{}vocab\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{128256}\PYG{p}{,}
\PYG{+w}{        }\PYG{n+nt}{\PYGZdq{}n\PYGZus{}ctx\PYGZus{}train\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{8192}\PYG{p}{,}
\PYG{+w}{        }\PYG{n+nt}{\PYGZdq{}n\PYGZus{}embd\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{4096}\PYG{p}{,}
\PYG{+w}{        }\PYG{n+nt}{\PYGZdq{}n\PYGZus{}params\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{8030261248}\PYG{p}{,}
\PYG{+w}{        }\PYG{n+nt}{\PYGZdq{}size\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{4684832768}
\PYG{+w}{      }\PYG{p}{\PYGZcb{}}
\PYG{+w}{    }\PYG{p}{\PYGZcb{}}
\PYG{+w}{  }\PYG{p}{]}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\bigskip\hrule\bigskip



\section{4. Compile llama.cpp with NVIDIA GPU Support (CUDA)}
\label{\detokenize{llama-cpp:compile-llama-cpp-with-nvidia-gpu-support-cuda}}\label{\detokenize{llama-cpp:llama-cpp-compile-llama-cpp-with-nvidia-gpu-support-cuda}}
\sphinxAtStartPar
If you have an \sphinxstylestrong{NVIDIA GPU} (e.g., RTX 3060, 4070, A100, etc.), enable \sphinxstylestrong{CUDA acceleration}:


\subsection{Prerequisites}
\label{\detokenize{llama-cpp:prerequisites}}\label{\detokenize{llama-cpp:llama-cpp-prerequisites}}\begin{itemize}
\item {} 
\sphinxAtStartPar
NVIDIA driver (\textbackslash{}geq\{\} 525)

\item {} 
\sphinxAtStartPar
CUDA Toolkit (\textbackslash{}geq\{\} 11.8, preferably 12.x)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{nvcc}} in \sphinxcode{\sphinxupquote{\$PATH}}

\end{itemize}


\subsection{Build with CUDA}
\label{\detokenize{llama-cpp:build-with-cuda}}\label{\detokenize{llama-cpp:llama-cpp-build-with-cuda}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Clean previous build}
make\PYG{+w}{ }clean

\PYG{c+c1}{\PYGZsh{} Build with full CUDA support}
make\PYG{+w}{ }\PYGZhy{}j\PYG{k}{\PYGZdl{}(}nproc\PYG{k}{)}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYG{n+nv}{LLAMA\PYGZus{}CUDA}\PYG{o}{=}\PYG{l+m}{1}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYG{n+nv}{LLAMA\PYGZus{}CUDA\PYGZus{}DMMV}\PYG{o}{=}\PYG{l+m}{1}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYG{n+nv}{LLAMA\PYGZus{}CUDA\PYGZus{}F16}\PYG{o}{=}\PYG{l+m}{1}

\PYG{c+c1}{\PYGZsh{} Optional: Specify compute capability (e.g., for RTX 40xx)}
\PYG{c+c1}{\PYGZsh{} make LLAMA\PYGZus{}CUDA=1 CUDA\PYGZus{}ARCH=\PYGZdq{}\PYGZhy{}gencode arch=compute\PYGZus{}89,code=sm\PYGZus{}89\PYGZdq{}}
\end{sphinxVerbatim}


\subsection{Run with GPU offloading}
\label{\detokenize{llama-cpp:run-with-gpu-offloading}}\label{\detokenize{llama-cpp:llama-cpp-run-with-gpu-offloading}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
./server\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}model\PYG{+w}{ }models/Meta\PYGZhy{}Llama\PYGZhy{}3\PYGZhy{}8B.Q4\PYGZus{}K\PYGZus{}S.gguf\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}port\PYG{+w}{ }\PYG{l+m}{8087}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}threads\PYG{+w}{ }\PYG{l+m}{8}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}n\PYGZhy{}gpu\PYGZhy{}layers\PYG{+w}{ }\PYG{l+m}{999}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{} }\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} offload ALL layers to GPU}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}host\PYG{+w}{ }\PYG{l+m}{0}.0.0.0
\end{sphinxVerbatim}

\begin{sphinxadmonition}{tip}{Tip:}
\sphinxAtStartPar
Use \sphinxcode{\sphinxupquote{nvidia\sphinxhyphen{}smi}} to monitor VRAM usage.
For 8B Q4 (\textasciitilde{}4.7 GB), even a \sphinxstylestrong{6 GB GPU} can run it fully offloaded.
\end{sphinxadmonition}


\bigskip\hrule\bigskip



\section{Summary}
\label{\detokenize{llama-cpp:summary}}\label{\detokenize{llama-cpp:llama-cpp-summary}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Feature
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Command / Note
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
Model Format
&
\sphinxAtStartPar
\sphinxstylestrong{GGUF}
\\
\sphinxhline
\sphinxAtStartPar
Download
&
\sphinxAtStartPar
\sphinxhref{https://huggingface.co/QuantFactory/...GGUF}{https://huggingface.co/QuantFactory/…GGUF}
\\
\sphinxhline
\sphinxAtStartPar
CPU Build (i7)
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{make LLAMA\_CPU=1}}
\\
\sphinxhline
\sphinxAtStartPar
GPU Build (CUDA)
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{make LLAMA\_CUDA=1}}
\\
\sphinxhline
\sphinxAtStartPar
Run Server
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{./server \sphinxhyphen{}\sphinxhyphen{}model ... \sphinxhyphen{}\sphinxhyphen{}port 8087}}
\\
\sphinxhline
\sphinxAtStartPar
Web UI
&
\sphinxAtStartPar
\sphinxurl{http://localhost:8087}
\\
\sphinxhline
\sphinxAtStartPar
API
&
\sphinxAtStartPar
\sphinxurl{http://localhost:8087/v1}
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\sphinxstylestrong{llama.cpp = lightweight, CPU/GPU flexible, no AVX\sphinxhyphen{}512 needed \(\rightarrow\) ideal replacement for vLLM on older hardware.}

\sphinxAtStartPar
—

\sphinxstepscope

\sphinxAtStartPar
{\color{red}\bfseries{}\textasciigrave{}\textasciigrave{}}{\color{red}\bfseries{}\textasciigrave{}}rst
.. \_deployment\sphinxhyphen{}considerations:


\chapter{Deploying LLMs in Hybrid Cloud: Why llama.cpp Wins for Us}
\label{\detokenize{hybrid:deploying-llms-in-hybrid-cloud-why-llama-cpp-wins-for-us}}\label{\detokenize{hybrid::doc}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{Table of Contents}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{hybrid:id5}}{\hyperref[\detokenize{hybrid:current-setup-ollama-in-testing}]{\sphinxcrossref{1. Current Setup: Ollama in Testing}}}

\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{hybrid:id6}}{\hyperref[\detokenize{hybrid:production-requirements-hybrid-cloud-multi-user-access}]{\sphinxcrossref{2. Production Requirements: Hybrid Cloud \& Multi\sphinxhyphen{}User Access}}}

\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{hybrid:id7}}{\hyperref[\detokenize{hybrid:evaluation-vllm-vs-llama-cpp}]{\sphinxcrossref{3. Evaluation: vLLM vs llama.cpp}}}

\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{hybrid:id8}}{\hyperref[\detokenize{hybrid:why-llama-cpp-is-our-production-choice}]{\sphinxcrossref{4. Why llama.cpp Is Our Production Choice}}}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{hybrid:id9}}{\hyperref[\detokenize{hybrid:example-production-ready-server}]{\sphinxcrossref{Example: Production\sphinxhyphen{}Ready Server}}}

\end{itemize}

\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{hybrid:id10}}{\hyperref[\detokenize{hybrid:migration-path-from-ollama-llama-cpp}]{\sphinxcrossref{5. Migration Path: From Ollama \(\rightarrow\) llama.cpp}}}

\item {} 
\sphinxAtStartPar
\phantomsection\label{\detokenize{hybrid:id11}}{\hyperref[\detokenize{hybrid:summary}]{\sphinxcrossref{Summary}}}

\end{itemize}
\end{sphinxShadowBox}


\bigskip\hrule\bigskip



\section{1. Current Setup: Ollama in Testing}
\label{\detokenize{hybrid:current-setup-ollama-in-testing}}
\sphinxAtStartPar
We have been using \sphinxstylestrong{Ollama} in a \sphinxstylestrong{test environment} with excellent results:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Easy to use} — \sphinxtitleref{ollama run llama3.1} just works

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Docker support} is first\sphinxhyphen{}class:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{FROM}\PYG{+w}{ }\PYG{l+s}{ollama/ollama}
\PYG{k}{COPY}\PYG{+w}{ }Modelfile\PYG{+w}{ }/root/.ollama/
\PYG{k}{RUN}\PYG{+w}{ }ollama\PYG{+w}{ }create\PYG{+w}{ }my\PYGZhy{}llama3.1\PYG{+w}{ }\PYGZhy{}f\PYG{+w}{ }Modelfile
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
Models are pulled, versioned, and cached automatically

\item {} 
\sphinxAtStartPar
Web UI and OpenAI\sphinxhyphen{}compatible API available out of the box

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Verdict}: Perfect for \sphinxstylestrong{prototyping}, \sphinxstylestrong{local dev}, and \sphinxstylestrong{small\sphinxhyphen{}scale testing}.


\bigskip\hrule\bigskip



\section{2. Production Requirements: Hybrid Cloud \& Multi\sphinxhyphen{}User Access}
\label{\detokenize{hybrid:production-requirements-hybrid-cloud-multi-user-access}}
\sphinxAtStartPar
When moving to \sphinxstylestrong{production in a hybrid cloud}, new constraints emerge:

\sphinxAtStartPar
We need a \sphinxstylestrong{lightweight, portable, hardware\sphinxhyphen{}agnostic} inference engine.


\bigskip\hrule\bigskip



\section{3. Evaluation: vLLM vs llama.cpp}
\label{\detokenize{hybrid:evaluation-vllm-vs-llama-cpp}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabular}[t]{\X{30}{100}\X{35}{100}\X{35}{100}}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Criteria
&\sphinxstyletheadfamily 
\sphinxAtStartPar
vLLM
&\sphinxstyletheadfamily 
\sphinxAtStartPar
llama.cpp
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
\sphinxstylestrong{Hardware Requirements}
&
\sphinxAtStartPar
Requires \sphinxstylestrong{AVX\sphinxhyphen{}512} (fails on Xeon v2, older i7)
&
\sphinxAtStartPar
Runs on \sphinxstylestrong{SSE2+}, AVX2 optional
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{GPU Support}
&
\sphinxAtStartPar
Excellent (PagedAttention, high throughput)
&
\sphinxAtStartPar
CUDA, Metal, Vulkan — full offloading
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{CPU Performance}
&
\sphinxAtStartPar
Poor without AVX\sphinxhyphen{}512
&
\sphinxAtStartPar
\sphinxstylestrong{Best\sphinxhyphen{}in\sphinxhyphen{}class} quantized inference
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{Binary Size}
&
\sphinxAtStartPar
\textasciitilde{}200 MB + Python deps
&
\sphinxAtStartPar
\sphinxstylestrong{\textless{} 10 MB} (statically linked)
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{Deployment}
&
\sphinxAtStartPar
Python server, complex deps
&
\sphinxAtStartPar
Single binary, \sphinxtitleref{scp} and run
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{Multi\sphinxhyphen{}user / API}
&
\sphinxAtStartPar
Built\sphinxhyphen{}in OpenAI API
&
\sphinxAtStartPar
\sphinxtitleref{server} binary with full OpenAI compat + web UI
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{Quantization Support}
&
\sphinxAtStartPar
FP16/BF16 only
&
\sphinxAtStartPar
Q4\_K, Q5\_K, Q8\_0, etc. — \sphinxstylestrong{4\textendash{}8 GB models fit in RAM}
\\
\sphinxbottomrule
\end{tabular}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\sphinxstylestrong{Key Finding}:

\sphinxAtStartPar
\textgreater{} \sphinxstylestrong{We cannot use vLLM} on our legacy Xeon v2 fleet due to missing \sphinxstylestrong{AVX\sphinxhyphen{}512}.
\textgreater{} \sphinxstylestrong{llama.cpp runs efficiently} on the \sphinxstylestrong{same hardware} with \sphinxstylestrong{Q4\_K\_M} models.


\bigskip\hrule\bigskip



\section{4. Why llama.cpp Is Our Production Choice}
\label{\detokenize{hybrid:why-llama-cpp-is-our-production-choice}}
\begin{sphinxadmonition}{note}{Decision}

\sphinxAtStartPar
\sphinxstylestrong{llama.cpp} is selected for \sphinxstylestrong{hybrid cloud LLM deployment} because:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Runs everywhere}: Old CPUs, new GPUs, laptops, edge

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Single static binary}: No Python, no CUDA runtime hell

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{GGUF format}: Share models with Ollama, local files, S3

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Built\sphinxhyphen{}in server}: OpenAI API + full web UI

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Thread \& context control}: \sphinxtitleref{\textendash{}threads}, \sphinxtitleref{\textendash{}ctx\sphinxhyphen{}size}, \sphinxtitleref{\textendash{}n\sphinxhyphen{}gpu\sphinxhyphen{}layers}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Kubernetes\sphinxhyphen{}ready}: Tiny image, fast startup

\end{itemize}
\end{sphinxadmonition}


\subsection{Example: Production\sphinxhyphen{}Ready Server}
\label{\detokenize{hybrid:example-production-ready-server}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
./llama.cpp/server\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}model\PYG{+w}{ }/models/llama3.1\PYGZhy{}8b\PYGZhy{}instruct.Q4\PYGZus{}K\PYGZus{}M.gguf\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}port\PYG{+w}{ }\PYG{l+m}{8080}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}host\PYG{+w}{ }\PYG{l+m}{0}.0.0.0\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}threads\PYG{+w}{ }\PYG{l+m}{16}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}ctx\PYGZhy{}size\PYG{+w}{ }\PYG{l+m}{8192}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}n\PYGZhy{}gpu\PYGZhy{}layers\PYG{+w}{ }\PYG{l+m}{0}\PYG{+w}{    }\PYG{c+c1}{\PYGZsh{} CPU\PYGZhy{}only on older nodes}
\PYG{+w}{  }\PYGZhy{}\PYGZhy{}log\PYGZhy{}disable
\end{sphinxVerbatim}

\sphinxAtStartPar
Deploy via Docker:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{FROM}\PYG{+w}{ }\PYG{l+s}{alpine:latest}
\PYG{k}{COPY}\PYG{+w}{ }llama.cpp/server\PYG{+w}{ }/usr/bin/
\PYG{k}{COPY}\PYG{+w}{ }models/*.gguf\PYG{+w}{ }/models/
\PYG{k}{EXPOSE}\PYG{+w}{ }\PYG{l+s}{8080}
\PYG{k}{CMD}\PYG{+w}{ }\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}server\PYGZdq{}}\PYG{p}{,}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}\PYGZhy{}\PYGZhy{}model\PYGZdq{}}\PYG{p}{,}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}/models/llama3.1\PYGZhy{}8b\PYGZhy{}instruct.Q4\PYGZus{}K\PYGZus{}M.gguf\PYGZdq{}}\PYG{p}{,}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}\PYGZhy{}\PYGZhy{}port\PYGZdq{}}\PYG{p}{,}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}8080\PYGZdq{}}\PYG{p}{]}
\end{sphinxVerbatim}


\bigskip\hrule\bigskip



\section{5. Migration Path: From Ollama \(\rightarrow\) llama.cpp}
\label{\detokenize{hybrid:migration-path-from-ollama-llama-cpp}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} 1. Reuse Ollama\PYGZsq{}s GGUF}
cp\PYG{+w}{ }\PYGZti{}/.ollama/models/blobs/sha256\PYGZhy{}*\PYG{+w}{ }/production/models/

\PYG{c+c1}{\PYGZsh{} 2. Deploy llama.cpp server}
kubectl\PYG{+w}{ }apply\PYG{+w}{ }\PYGZhy{}f\PYG{+w}{ }llama\PYGZhy{}cpp\PYGZhy{}deployment.yaml

\PYG{c+c1}{\PYGZsh{} 3. Point clients to new endpoint}
\PYG{n+nb}{export}\PYG{+w}{ }\PYG{n+nv}{OPENAI\PYGZus{}API\PYGZus{}BASE}\PYG{o}{=}http://llama\PYGZhy{}cpp\PYGZhy{}prod:8080/v1
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Zero model reconversion. Zero downtime.}


\bigskip\hrule\bigskip



\section{Summary}
\label{\detokenize{hybrid:summary}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Use Case
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Recommended Tool
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
Local dev / prototyping
&
\sphinxAtStartPar
\sphinxstylestrong{Ollama}
\\
\sphinxhline
\sphinxAtStartPar
Hybrid cloud, old hardware, scale
&
\sphinxAtStartPar
\sphinxstylestrong{llama.cpp}
\\
\sphinxhline
\sphinxAtStartPar
High\sphinxhyphen{}throughput GPU cluster
&
\sphinxAtStartPar
vLLM (if AVX\sphinxhyphen{}512 available)
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\textgreater{} \sphinxstylestrong{llama.cpp = the Swiss Army knife of LLM inference.}

\sphinxAtStartPar
—


\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{search}

\end{itemize}



\renewcommand{\indexname}{Index}
\printindex
\end{document}