\babel@toc {english}{}\relax 
\contentsline {chapter}{\numberline {1}About RAGFlow: Named Among GitHub’s Fastest\sphinxhyphen {}Growing Open Source Projects}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}The Rise of Retrieval\sphinxhyphen {}Augmented Generation in Production}{1}{section.1.1}%
\contentsline {section}{\numberline {1.2}Why RAGFlow Resonates in the AI Era}{1}{section.1.2}%
\contentsline {section}{\numberline {1.3}A Project in Active Development}{2}{section.1.3}%
\contentsline {section}{\numberline {1.4}Conclusion}{2}{section.1.4}%
\contentsline {chapter}{\numberline {2}RAGFlow System Architecture}{3}{chapter.2}%
\contentsline {section}{\numberline {2.1}Overview}{3}{section.2.1}%
\contentsline {section}{\numberline {2.2}Core Infrastructure Containers}{3}{section.2.2}%
\contentsline {section}{\numberline {2.3}RAGFlow Application Containers}{3}{section.2.3}%
\contentsline {section}{\numberline {2.4}High\sphinxhyphen {}Level Architecture Diagram}{4}{section.2.4}%
\contentsline {section}{\numberline {2.5}Data \& Execution Flow}{4}{section.2.5}%
\contentsline {chapter}{\numberline {3}Why Infiniflow RAGFlow Uses a Reranker, an Embedding Model, and a Chat Model}{5}{chapter.3}%
\contentsline {section}{\numberline {3.1}Synergy of the Three Models}{6}{section.3.1}%
\contentsline {chapter}{\numberline {4}Why vLLM is Used to Serve the Reranker Model}{7}{chapter.4}%
\contentsline {section}{\numberline {4.1}Key Reasons for Using vLLM to Serve the Reranker}{7}{section.4.1}%
\contentsline {section}{\numberline {4.2}Serving the Reranker Locally with vLLM}{8}{section.4.2}%
\contentsline {section}{\numberline {4.3}RAGFlow Integration}{9}{section.4.3}%
\contentsline {chapter}{\numberline {5}Serving vLLM Reranker Using Docker (CPU\sphinxhyphen {}Only)}{11}{chapter.5}%
\contentsline {section}{\numberline {5.1}Docker Compose Configuration (CPU Mode)}{11}{section.5.1}%
\contentsline {section}{\numberline {5.2}Key Components Explained}{12}{section.5.2}%
\contentsline {section}{\numberline {5.3}Why the Model Must Be Pre\sphinxhyphen {}Downloaded Locally}{12}{section.5.3}%
\contentsline {section}{\numberline {5.4}Why CPU\sphinxhyphen {}Only (No GPU)?}{13}{section.5.4}%
\contentsline {section}{\numberline {5.5}Start the Service}{13}{section.5.5}%
\contentsline {section}{\numberline {5.6}Verify Availability}{13}{section.5.6}%
\contentsline {section}{\numberline {5.7}Integration with RAGFlow}{13}{section.5.7}%
\contentsline {section}{\numberline {5.8}Benefits of This CPU + Docker Setup}{13}{section.5.8}%
\contentsline {chapter}{\numberline {6}Integrating vLLM with RAGFlow via Docker Network}{15}{chapter.6}%
\contentsline {section}{\numberline {6.1}Why Network Configuration is Required}{15}{section.6.1}%
\contentsline {section}{\numberline {6.2}Step\sphinxhyphen {}by\sphinxhyphen {}Step: Configure Docker Network}{15}{section.6.2}%
\contentsline {section}{\numberline {6.3}Architecture Diagram}{17}{section.6.3}%
\contentsline {section}{\numberline {6.4}Verification}{17}{section.6.4}%
\contentsline {section}{\numberline {6.5}Benefits of This Setup}{17}{section.6.5}%
\contentsline {section}{\numberline {6.6}Troubleshooting Tips}{17}{section.6.6}%
\contentsline {section}{\numberline {6.7}Summary}{19}{section.6.7}%
\contentsline {chapter}{\numberline {7}Batch Processing and Metadata Management in Infiniflow RAGFlow}{21}{chapter.7}%
\contentsline {section}{\numberline {7.1}API Base URL}{21}{section.7.1}%
\contentsline {section}{\numberline {7.2}Authentication}{21}{section.7.2}%
\contentsline {section}{\numberline {7.3}Step 1: Retrieve Dataset and Document IDs}{22}{section.7.3}%
\contentsline {section}{\numberline {7.4}Step 2: Add Metadata to a Document (via PUT)}{22}{section.7.4}%
\contentsline {section}{\numberline {7.5}Use Case: Batch Metadata Enrichment}{23}{section.7.5}%
\contentsline {section}{\numberline {7.6}Other Batch\sphinxhyphen {}Capable Endpoints}{24}{section.7.6}%
\contentsline {section}{\numberline {7.7}Best Practices}{24}{section.7.7}%
\contentsline {section}{\numberline {7.8}See also:}{24}{section.7.8}%
\contentsline {section}{\numberline {7.9}Summary}{24}{section.7.9}%
\contentsline {chapter}{\numberline {8}How the Knowledge Graph in Infiniflow/RAGFlow Works}{25}{chapter.8}%
\contentsline {section}{\numberline {8.1}Overview}{25}{section.8.1}%
\contentsline {section}{\numberline {8.2}Construction Process}{25}{section.8.2}%
\contentsline {section}{\numberline {8.3}Query and Retrieval Process}{26}{section.8.3}%
\contentsline {section}{\numberline {8.4}Key Features and Limitations}{26}{section.8.4}%
\contentsline {chapter}{\numberline {9}Running Llama 3.1 with llama.cpp}{27}{chapter.9}%
\contentsline {section}{\numberline {9.1}1. Model Format: GGUF}{27}{section.9.1}%
\contentsline {section}{\numberline {9.2}2. Compile llama.cpp for Intel i7 (CPU\sphinxhyphen {}only)}{28}{section.9.2}%
\contentsline {section}{\numberline {9.3}3. Run the Model with Web Interface}{28}{section.9.3}%
\contentsline {subsection}{\numberline {9.3.1}Features}{29}{subsection.9.3.1}%
\contentsline {section}{\numberline {9.4}4. Compile llama.cpp with NVIDIA GPU Support (CUDA)}{29}{section.9.4}%
\contentsline {subsection}{\numberline {9.4.1}Prerequisites}{29}{subsection.9.4.1}%
\contentsline {subsection}{\numberline {9.4.2}Build with CUDA}{30}{subsection.9.4.2}%
\contentsline {subsection}{\numberline {9.4.3}Run with GPU offloading}{30}{subsection.9.4.3}%
\contentsline {section}{\numberline {9.5}Summary}{30}{section.9.5}%
\contentsline {chapter}{\numberline {10}Running Multiple Models on llama.cpp Using Docker}{31}{chapter.10}%
\contentsline {section}{\numberline {10.1}Example \sphinxstyleliteralintitle {\sphinxupquote {docker\sphinxhyphen {}compose.yml}}}{31}{section.10.1}%
\contentsline {section}{\numberline {10.2}Key Configuration Notes}{32}{section.10.2}%
\contentsline {section}{\numberline {10.3}Usage}{32}{section.10.3}%
\contentsline {chapter}{\numberline {11}Deploying LLMs in Hybrid Cloud: Why llama.cpp Wins for Us}{33}{chapter.11}%
\contentsline {section}{\numberline {11.1}1. Current Setup: Ollama in Testing}{33}{section.11.1}%
\contentsline {section}{\numberline {11.2}2. Production Requirements: Hybrid Cloud \& Multi\sphinxhyphen {}User Access}{34}{section.11.2}%
\contentsline {section}{\numberline {11.3}3. Evaluation: vLLM vs llama.cpp}{34}{section.11.3}%
\contentsline {section}{\numberline {11.4}4. Why llama.cpp Is Our Production Choice}{34}{section.11.4}%
\contentsline {subsection}{\numberline {11.4.1}Example: Production\sphinxhyphen {}Ready Server}{35}{subsection.11.4.1}%
\contentsline {section}{\numberline {11.5}5. Migration Path: From Ollama \(\rightarrow \) llama.cpp}{35}{section.11.5}%
\contentsline {section}{\numberline {11.6}Summary}{35}{section.11.6}%
\contentsline {chapter}{\numberline {12}How InfiniFlow RAGFlow Uses gVisor}{37}{chapter.12}%
\contentsline {section}{\numberline {12.1}gVisor Integration}{37}{section.12.1}%
\contentsline {subsection}{\numberline {12.1.1}What is gVisor?}{37}{subsection.12.1.1}%
\contentsline {section}{\numberline {12.2}How RAGFlow Uses gVisor Sandboxes}{37}{section.12.2}%
\contentsline {section}{\numberline {12.3}Security Advantages in Practice}{38}{section.12.3}%
\contentsline {section}{\numberline {12.4}Enabling/Disabling the Sandbox}{38}{section.12.4}%
\contentsline {section}{\numberline {12.5}Summary}{38}{section.12.5}%
\contentsline {chapter}{\numberline {13}RAGFlow GPU vs CPU: Full Explanation (2025 Edition)}{39}{chapter.13}%
\contentsline {section}{\numberline {13.1}Why Does RAGFlow Still Need a GPU Even When Using Ollama?}{39}{section.13.1}%
\contentsline {section}{\numberline {13.2}Complete RAGFlow Pipeline (with GPU usage marked)}{39}{section.13.2}%
\contentsline {section}{\numberline {13.3}What DeepDoc Actually Does (and Why GPU Makes It 5\textendash {}20× Faster)}{40}{section.13.3}%
\contentsline {section}{\numberline {13.4}Real\sphinxhyphen {}World Performance Numbers}{40}{section.13.4}%
\contentsline {section}{\numberline {13.5}When Do You Actually Need ragflow\sphinxhyphen {}gpu?}{40}{section.13.5}%
\contentsline {section}{\numberline {13.6}Recommended Setup in 2025 (Best of Both Worlds)}{40}{section.13.6}%
\contentsline {section}{\numberline {13.7}Monitoring \& Verification}{41}{section.13.7}%
\contentsline {section}{\numberline {13.8}Conclusion}{41}{section.13.8}%
\contentsline {chapter}{\numberline {14}Indices and tables}{43}{chapter.14}%
